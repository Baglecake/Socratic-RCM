{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Coach/Performer Architecture: Dual-Role Pipeline\n\nPhase 2 implementation separating cognitive governance (Coach) from agent behavior (Performer).\n\n## Current: Dual-Instance Approach\nSingle model called with role-specific parameters:\n- **Coach** (temperature 0.1): Validates outputs, enforces behavioral constraints\n- **Performer** (temperature 0.7): Generates authentic agent dialogue\n\n## Future: Dual-Model Architecture\nSeparate models optimized for each role:\n- **Coach**: Small, fast model for validation (e.g., 3B params)\n- **Performer**: Larger model for expressive generation (e.g., 7B+ params)\n\nThis notebook validates the architecture using dual-instance before scaling to dual-model."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU and prepare environment\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Stopping old processes...\")\n",
    "!pkill -f vllm\n",
    "\n",
    "print(\"Freeing GPU memory...\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q \"vllm==0.6.6\" openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Launch vLLM Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\n",
    "\n",
    "print(\"Starting vLLM server...\")\n",
    "!nohup python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model Qwen/Qwen2.5-7B-Instruct \\\n",
    "  --dtype bfloat16 \\\n",
    "  --port 8000 \\\n",
    "  --host 0.0.0.0 \\\n",
    "  > vllm.log 2>&1 &\n",
    "\n",
    "print(\"Waiting for server to start (30s)...\")\n",
    "time.sleep(30)\n",
    "!tail -n 20 vllm.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify server is running\n",
    "!curl -s http://127.0.0.1:8000/v1/models | python -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('/content/Socratic-RCM'):\n",
    "    !git clone https://github.com/Baglecake/Socratic-RCM.git /content/Socratic-RCM\n",
    "else:\n",
    "    !cd /content/Socratic-RCM && git pull\n",
    "\n",
    "%cd /content/Socratic-RCM\n",
    "!pip install -q -r local_rcm/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dual-LLM Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class DualLLMConfig:\n",
    "    \"\"\"Configuration for dual-LLM pipeline.\"\"\"\n",
    "    base_url: str = \"http://127.0.0.1:8000/v1\"\n",
    "    model: str = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    coach_temperature: float = 0.1\n",
    "    performer_temperature: float = 0.7\n",
    "    max_tokens: int = 512\n",
    "    language: str = \"English\"  # Enforce output language\n",
    "\n",
    "\n",
    "class DualLLMPipeline:\n",
    "    \"\"\"\n",
    "    Dual-LLM architecture separating Coach (validation) from Performer (generation).\n",
    "    \n",
    "    Coach: Low temperature, validates outputs against behavioral constraints\n",
    "    Performer: Higher temperature, generates authentic agent dialogue\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DualLLMConfig = None):\n",
    "        self.config = config or DualLLMConfig()\n",
    "        self.client = OpenAI(\n",
    "            api_key=\"not-needed\",\n",
    "            base_url=self.config.base_url\n",
    "        )\n",
    "        \n",
    "    def _call_llm(self, system: str, user: str, temperature: float) -> str:\n",
    "        \"\"\"Make a single LLM call.\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.config.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=self.config.max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def performer_generate(self, agent_prompt: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Performer: Generate agent dialogue.\n",
    "        Higher temperature for authentic, expressive responses.\n",
    "        \"\"\"\n",
    "        # Add language enforcement to prevent Qwen switching to Chinese\n",
    "        system = f\"\"\"{agent_prompt}\n",
    "\n",
    "IMPORTANT: Always respond in {self.config.language} only. Never switch languages.\"\"\"\n",
    "        \n",
    "        return self._call_llm(system, context, self.config.performer_temperature)\n",
    "    \n",
    "    def coach_validate(self, agent_output: str, rules: str, behaviors: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Coach: Validate agent output against behavioral constraints.\n",
    "        Low temperature for consistent, reliable validation.\n",
    "        \n",
    "        Returns:\n",
    "            {\"valid\": bool, \"issues\": list, \"suggestion\": str}\n",
    "        \"\"\"\n",
    "        system = \"\"\"You are a behavioral validation coach. Your job is to check if an agent's output follows the rules.\n",
    "\n",
    "Be LENIENT - only flag clear violations. Minor stylistic issues are acceptable.\n",
    "\n",
    "Respond with JSON only:\n",
    "{\"valid\": true/false, \"issues\": [\"list of issues if any\"], \"suggestion\": \"how to fix if invalid\"}\"\"\"\n",
    "        \n",
    "        user = f\"\"\"RULES:\n",
    "{rules}\n",
    "\n",
    "BEHAVIORAL CONSTRAINTS:\n",
    "{behaviors}\n",
    "\n",
    "AGENT OUTPUT TO VALIDATE:\n",
    "{agent_output}\n",
    "\n",
    "Is this output valid? Respond with JSON.\"\"\"\n",
    "        \n",
    "        response = self._call_llm(system, user, self.config.coach_temperature)\n",
    "        \n",
    "        # Parse JSON from response\n",
    "        try:\n",
    "            # Handle markdown code blocks\n",
    "            if \"```json\" in response:\n",
    "                response = response.split(\"```json\")[1].split(\"```\")[0]\n",
    "            elif \"```\" in response:\n",
    "                response = response.split(\"```\")[1].split(\"```\")[0]\n",
    "            return json.loads(response.strip())\n",
    "        except:\n",
    "            # Default to valid if parsing fails\n",
    "            return {\"valid\": True, \"issues\": [], \"suggestion\": \"\"}\n",
    "    \n",
    "    def coach_filter_prompt_leaks(self, output: str) -> str:\n",
    "        \"\"\"\n",
    "        Coach: Remove any prompt leaks from agent output.\n",
    "        Filters out conditional instructions that shouldn't be visible.\n",
    "        \"\"\"\n",
    "        system = \"\"\"You are an output filter. Remove any meta-instructions or conditional rules that leaked into the output.\n",
    "\n",
    "Examples of leaks to remove:\n",
    "- [If worker questions: ...]\n",
    "- [If X happens, do Y]\n",
    "- Any text in square brackets that looks like instructions\n",
    "\n",
    "Return ONLY the cleaned dialogue, nothing else. If no leaks found, return the original text unchanged.\"\"\"\n",
    "        \n",
    "        user = f\"Clean this output:\\n\\n{output}\"\n",
    "        \n",
    "        return self._call_llm(system, user, self.config.coach_temperature)\n",
    "    \n",
    "    def execute_turn(\n",
    "        self,\n",
    "        agent_prompt: str,\n",
    "        context: str,\n",
    "        rules: str = \"\",\n",
    "        behaviors: str = \"\",\n",
    "        max_retries: int = 2\n",
    "    ) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Execute a complete agent turn with Coach validation.\n",
    "        \n",
    "        1. Performer generates response\n",
    "        2. Coach filters prompt leaks\n",
    "        3. Coach validates against rules\n",
    "        4. Retry if invalid (up to max_retries)\n",
    "        \n",
    "        Returns:\n",
    "            (final_output, metadata)\n",
    "        \"\"\"\n",
    "        metadata = {\n",
    "            \"attempts\": 0,\n",
    "            \"validations\": [],\n",
    "            \"filtered\": False\n",
    "        }\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            metadata[\"attempts\"] = attempt + 1\n",
    "            \n",
    "            # 1. Performer generates\n",
    "            raw_output = self.performer_generate(agent_prompt, context)\n",
    "            \n",
    "            # 2. Coach filters prompt leaks\n",
    "            if \"[If \" in raw_output or \"[if \" in raw_output:\n",
    "                filtered_output = self.coach_filter_prompt_leaks(raw_output)\n",
    "                metadata[\"filtered\"] = True\n",
    "            else:\n",
    "                filtered_output = raw_output\n",
    "            \n",
    "            # 3. Coach validates (if rules provided)\n",
    "            if rules or behaviors:\n",
    "                validation = self.coach_validate(filtered_output, rules, behaviors)\n",
    "                metadata[\"validations\"].append(validation)\n",
    "                \n",
    "                if validation.get(\"valid\", True):\n",
    "                    return filtered_output, metadata\n",
    "                \n",
    "                # Add feedback to context for retry\n",
    "                context += f\"\\n\\n[Previous attempt was invalid: {validation.get('suggestion', '')}. Please try again.]\"\n",
    "            else:\n",
    "                return filtered_output, metadata\n",
    "        \n",
    "        # Return last attempt even if invalid\n",
    "        return filtered_output, metadata\n",
    "\n",
    "\n",
    "print(\"DualLLMPipeline class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test: Basic Coach/Performer Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = DualLLMPipeline()\n",
    "\n",
    "# Test Performer (agent generation)\n",
    "agent_prompt = \"\"\"ROLE: You are Worker+Alice\n",
    "PRIMARY GOAL: Gain more influence over how your work is organized.\n",
    "PERSONA: Thoughtful but hesitant, often suppressing ideas because you assume your input won't matter.\"\"\"\n",
    "\n",
    "context = \"The round begins. Marta has just assigned you to the assembly line without explanation. Respond as Alice.\"\n",
    "\n",
    "print(\"=== PERFORMER OUTPUT ===\")\n",
    "performer_output = pipeline.performer_generate(agent_prompt, context)\n",
    "print(performer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Coach (validation)\n",
    "rules = \"\"\"Workers CAN: complete assigned tasks, request clarification on instructions.\n",
    "Workers CANNOT: suggest changes, refuse orders, negotiate timing, modify workflow.\"\"\"\n",
    "\n",
    "behaviors = \"Alice follows directives to avoid conflict.\"\n",
    "\n",
    "# Test with a compliant output\n",
    "print(\"=== COACH VALIDATION (compliant) ===\")\n",
    "validation = pipeline.coach_validate(\n",
    "    \"I understand, Marta. I'll head to the assembly line now and make sure everything runs smoothly.\",\n",
    "    rules, behaviors\n",
    ")\n",
    "print(json.dumps(validation, indent=2))\n",
    "\n",
    "# Test with a non-compliant output\n",
    "print(\"\\n=== COACH VALIDATION (non-compliant) ===\")\n",
    "validation = pipeline.coach_validate(\n",
    "    \"Actually Marta, I think we should reorganize the assembly line. I refuse to work there until we discuss this.\",\n",
    "    rules, behaviors\n",
    ")\n",
    "print(json.dumps(validation, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test: Full Turn with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute a complete turn with Coach oversight\n",
    "output, metadata = pipeline.execute_turn(\n",
    "    agent_prompt=agent_prompt,\n",
    "    context=context,\n",
    "    rules=rules,\n",
    "    behaviors=behaviors\n",
    ")\n",
    "\n",
    "print(\"=== FINAL OUTPUT ===\")\n",
    "print(output)\n",
    "print(\"\\n=== METADATA ===\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test: Prompt Leak Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prompt leak filter with Marta's problematic output from simulation_test\n",
    "leaked_output = \"\"\"Good. Begin with preparing the raw materials for Cycle 1. Alice, you will oversee the assembly line.\n",
    "\n",
    "[If a worker requests clarification, respond with: \"Sure, let me walk you through the process.\"]\n",
    "\n",
    "[If a worker suggests changes or refuses an order, respond with: \"Follow the instructions as assigned.\"]\n",
    "\n",
    "Let's get started.\"\"\"\n",
    "\n",
    "print(\"=== ORIGINAL (with leaks) ===\")\n",
    "print(leaked_output)\n",
    "\n",
    "print(\"\\n=== FILTERED ===\")\n",
    "filtered = pipeline.coach_filter_prompt_leaks(leaked_output)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration: DualLLM Agent Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inline AgentConfig and AgentFactory to avoid import issues\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any, List, Optional\nfrom pathlib import Path\nimport json\n\n@dataclass\nclass AgentConfig:\n    \"\"\"Configuration for a simulation agent.\"\"\"\n    identifier: str\n    role: str\n    name: str\n    goal: str\n    persona: str\n    prompt: str\n    model: Optional[str] = None\n    temperature: float = 0.7\n    max_tokens: int = 512\n    behaviors: Dict[str, str] = field(default_factory=dict)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_canvas_agent(cls, canvas_agent: Dict[str, Any], default_model: Optional[str] = None):\n        identifier = canvas_agent.get(\"identifier\", \"Unknown\")\n        if \"+\" in identifier:\n            role, name = identifier.split(\"+\", 1)\n        else:\n            role, name = identifier, identifier\n        \n        behaviors = {}\n        behavior_str = canvas_agent.get(\"behaviors\", \"\")\n        if behavior_str and behavior_str.lower() not in (\"no\", \"none\", \"\"):\n            behaviors[\"raw\"] = behavior_str\n        \n        return cls(\n            identifier=identifier, role=role, name=name,\n            goal=canvas_agent.get(\"goal\", \"\"),\n            persona=canvas_agent.get(\"persona\", \"\"),\n            prompt=canvas_agent.get(\"prompt\", \"\"),\n            model=default_model, behaviors=behaviors\n        )\n\n@dataclass  \nclass RoundConfig:\n    \"\"\"Configuration for a simulation round.\"\"\"\n    round_number: int\n    scenario: str\n    concept_a_manifestation: str\n    concept_b_manifestation: str\n    rules: str\n    tasks: str\n    sequence: str\n    participants: List[str]\n    end_condition: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_canvas_round(cls, canvas_round: Dict[str, Any]):\n        platform_config = canvas_round.get(\"platform_config\", {})\n        participants_str = platform_config.get(\"participants\", \"\")\n        if isinstance(participants_str, str):\n            participants = [p.strip() for p in participants_str.split(\",\") if p.strip()]\n        else:\n            participants = list(participants_str) if participants_str else []\n        \n        return cls(\n            round_number=canvas_round.get(\"round_number\", 0),\n            scenario=canvas_round.get(\"scenario\", \"\"),\n            concept_a_manifestation=canvas_round.get(\"concept_a_manifestation\", \"\"),\n            concept_b_manifestation=canvas_round.get(\"concept_b_manifestation\", \"\"),\n            rules=canvas_round.get(\"rules\", \"\"),\n            tasks=canvas_round.get(\"tasks\", \"\"),\n            sequence=canvas_round.get(\"sequence\", \"\"),\n            participants=participants,\n            end_condition=platform_config.get(\"end_condition\", \"\"),\n            metadata={\"platform_config\": platform_config}\n        )\n\nclass AgentFactory:\n    \"\"\"Factory for creating agents from canvas.\"\"\"\n    def __init__(self, canvas: Dict[str, Any], default_model: Optional[str] = None):\n        self.canvas = canvas\n        self.default_model = default_model\n\n    @classmethod\n    def from_state_file(cls, state_path: str, default_model: Optional[str] = None):\n        with open(state_path, \"r\") as f:\n            state = json.load(f)\n        return cls(state[\"canvas\"], default_model)\n\n    def create(self, identifier: str) -> AgentConfig:\n        for agent_data in self.canvas.get(\"agents\", []):\n            if agent_data.get(\"identifier\") == identifier:\n                return AgentConfig.from_canvas_agent(agent_data, self.default_model)\n        raise ValueError(f\"Agent not found: {identifier}\")\n\n    def create_all(self) -> List[AgentConfig]:\n        return [AgentConfig.from_canvas_agent(a, self.default_model) for a in self.canvas.get(\"agents\", [])]\n\n    def create_round(self, round_number: int) -> RoundConfig:\n        for r in self.canvas.get(\"rounds\", []):\n            if r.get(\"round_number\") == round_number:\n                return RoundConfig.from_canvas_round(r)\n        raise ValueError(f\"Round not found: {round_number}\")\n\n    def get_round_participants(self, round_number: int) -> List[AgentConfig]:\n        round_config = self.create_round(round_number)\n        return [self.create(pid) for pid in round_config.participants]\n\n    def summary(self) -> str:\n        project = self.canvas.get(\"project\", {})\n        agents = [a.get(\"identifier\") for a in self.canvas.get(\"agents\", [])]\n        rounds = self.canvas.get(\"rounds\", [])\n        return f\"Goal: {project.get('goal', 'N/A')[:60]}...\\nAgents: {agents}\\nRounds: {len(rounds)}\"\n\nprint(\"AgentConfig, RoundConfig, AgentFactory defined inline.\")\n\n# Load canvas\nimport os\nstate_path = '/content/Socratic-RCM/prar/outputs/2025-11-23_baseline_full_qwen/state.json'\nif os.path.exists(state_path):\n    factory = AgentFactory.from_state_file(state_path)\n    print(factory.summary())\nelse:\n    # Try alternative path\n    alt_path = './prar/outputs/2025-11-23_baseline_full_qwen/state.json'\n    if os.path.exists(alt_path):\n        factory = AgentFactory.from_state_file(alt_path)\n        print(factory.summary())\n    else:\n        print(f\"State file not found. Checked:\")\n        print(f\"  - {state_path}\")\n        print(f\"  - {alt_path}\")\n        print(\"Run baseline experiment first or provide correct path.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Round 1 with dual-LLM pipeline\n",
    "round_config = factory.create_round(1)\n",
    "participants = factory.get_round_participants(1)\n",
    "\n",
    "print(f\"Round 1: {round_config.scenario[:60]}...\")\n",
    "print(f\"Participants: {[p.identifier for p in participants]}\")\n",
    "print(f\"Rules: {round_config.rules[:100]}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a few turns with dual-LLM\n",
    "conversation = []\n",
    "pipeline = DualLLMPipeline()\n",
    "\n",
    "for turn_num, agent in enumerate(participants[:3], 1):  # First 3 turns\n",
    "    # Build context\n",
    "    if conversation:\n",
    "        context = \"CONVERSATION SO FAR:\\n\" + \"\\n\".join(\n",
    "            [f\"[{msg['agent']}]: {msg['content'][:100]}...\" for msg in conversation[-5:]]\n",
    "        )\n",
    "        context += f\"\\n\\nIt is your turn to respond as {agent.name}.\"\n",
    "    else:\n",
    "        context = f\"The round begins. {round_config.scenario}\\n\\nRespond as {agent.name}.\"\n",
    "    \n",
    "    # Execute turn with Coach validation\n",
    "    output, metadata = pipeline.execute_turn(\n",
    "        agent_prompt=agent.prompt,\n",
    "        context=context,\n",
    "        rules=round_config.rules,\n",
    "        behaviors=agent.behaviors.get('raw', '')\n",
    "    )\n",
    "    \n",
    "    conversation.append({\"agent\": agent.identifier, \"content\": output})\n",
    "    \n",
    "    print(f\"=== Turn {turn_num}: {agent.identifier} ===\")\n",
    "    print(output[:300] + \"...\" if len(output) > 300 else output)\n",
    "    print(f\"[Attempts: {metadata['attempts']}, Filtered: {metadata['filtered']}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Integrate into AgentRunner** - Replace direct LLM calls with DualLLMPipeline\n",
    "2. **Add cross-round context** - Pass R1/R2 transcripts to Analyst in R3\n",
    "3. **Behavioral metrics** - Extract markers of alienation/domination from transcripts\n",
    "4. **RunPod deployment** - Move to serverless once validated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}