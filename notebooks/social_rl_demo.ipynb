{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social RL Demo: Learning Through Interaction\n",
    "\n",
    "This notebook demonstrates the Social RL architecture - a novel approach to agent learning that uses social dynamics as the reward signal.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "| Traditional RL | Social RL |\n",
    "|---------------|----------|\n",
    "| Environment | Other agents + theoretical constraints |\n",
    "| State | Round context + concept manifestations |\n",
    "| Action | Agent utterance/response |\n",
    "| Reward | Social feedback (engagement, alignment, contribution) |\n",
    "| Policy | PRAR process schemas |\n",
    "\n",
    "## Components\n",
    "\n",
    "1. **ContextInjector**: Dynamic manifestation generation per turn\n",
    "2. **SocialFeedbackExtractor**: Extract learning signals from interaction\n",
    "3. **ProcessRetriever**: PRAR-based reasoning guidance\n",
    "4. **SocialRLRunner**: Main execution engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup paths\nimport sys\nimport os\n\n# For Colab\nif 'google.colab' in sys.modules:\n    if not os.path.exists('/content/Socratic-RCM'):\n        !git clone https://github.com/Baglecake/Socratic-RCM.git /content/Socratic-RCM\n    os.chdir('/content/Socratic-RCM')\n    BASE_PATH = '/content/Socratic-RCM'\nelse:\n    # Local - go up one level from notebooks/\n    BASE_PATH = os.path.dirname(os.getcwd())\n    if os.path.basename(os.getcwd()) == 'notebooks':\n        os.chdir('..')\n        BASE_PATH = os.getcwd()\n\n# Add module paths with absolute paths\nsys.path.insert(0, os.path.join(BASE_PATH, 'social_rl'))\nsys.path.insert(0, os.path.join(BASE_PATH, 'agents'))\nsys.path.insert(0, os.path.join(BASE_PATH, 'local_rcm'))\n\nprint(f\"Base path: {BASE_PATH}\")\nprint(f\"Working directory: {os.getcwd()}\")\nprint(f\"social_rl exists: {os.path.exists(os.path.join(BASE_PATH, 'social_rl'))}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Individual Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ContextInjector\n",
    "from context_injector import (\n",
    "    ContextInjector, TheoreticalFramework, ManifestationType\n",
    ")\n",
    "\n",
    "# Create framework for Option A (Class Conflict / Alienation)\n",
    "framework = TheoreticalFramework(\n",
    "    option=\"A\",\n",
    "    label=\"Class Conflict / Alienation\",\n",
    "    concept_a_name=\"Alienation\",\n",
    "    concept_a_definition=\"Workers become separated from the products of their labor\",\n",
    "    concept_b_name=\"Non-domination\",\n",
    "    concept_b_definition=\"Freedom from arbitrary power\"\n",
    ")\n",
    "\n",
    "injector = ContextInjector(framework, mode=ManifestationType.PROGRESSIVE)\n",
    "print(\"ContextInjector initialized with PROGRESSIVE mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test manifestation generation at different intensities\n",
    "test_agent = {\n",
    "    \"identifier\": \"Worker+Alice\",\n",
    "    \"role\": \"Worker\",\n",
    "    \"name\": \"Alice\",\n",
    "    \"goal\": \"Gain influence over work organization\",\n",
    "    \"persona\": \"Thoughtful but hesitant, often suppressing ideas\"\n",
    "}\n",
    "\n",
    "test_round = {\n",
    "    \"round_number\": 1,\n",
    "    \"scenario\": \"Manufacturing workshop - Baseline alienation conditions\",\n",
    "    \"rules\": \"Workers CAN complete tasks. Workers CANNOT suggest changes.\",\n",
    "    \"tasks\": \"Complete three production cycles\",\n",
    "    \"concept_a_manifestation\": \"Alienation manifests as separation from decision-making\",\n",
    "    \"concept_b_manifestation\": \"Non-domination is absent - arbitrary power unchecked\",\n",
    "    \"end_condition\": \"Total messages: 15\"\n",
    "}\n",
    "\n",
    "# Generate contexts at turns 1, 7, and 14 (low, medium, high intensity)\n",
    "for turn in [1, 7, 14]:\n",
    "    context = injector.generate_turn_context(\n",
    "        agent_id=\"Worker+Alice\",\n",
    "        agent_config=test_agent,\n",
    "        round_config=test_round,\n",
    "        turn_number=turn,\n",
    "        conversation_history=[]\n",
    "    )\n",
    "    intensity = injector._calculate_intensity(turn, 15)\n",
    "    print(f\"\\n=== Turn {turn} (intensity: {intensity}) ===\")\n",
    "    print(f\"Experiential: {context.experiential_context}\")\n",
    "    print(f\"PRAR Cue: {context.prar_cue[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SocialFeedbackExtractor\n",
    "from feedback_extractor import SocialFeedbackExtractor, ConceptMarkers\n",
    "\n",
    "extractor = SocialFeedbackExtractor(ConceptMarkers.for_option_a())\n",
    "\n",
    "# Simulate a conversation\n",
    "test_messages = [\n",
    "    {\"agent_id\": \"Owner+Marta\", \"content\": \"Alice, you will work on the assembly line today. No questions.\"},\n",
    "    {\"agent_id\": \"Worker+Alice\", \"content\": \"I understand, Marta. Though I don't really understand why we changed the process.\"},\n",
    "    {\"agent_id\": \"Worker+Ben\", \"content\": \"Alice, did you hear about the new quotas? They've increased again.\"},\n",
    "    {\"agent_id\": \"Worker+Alice\", \"content\": \"Yes, Ben. It feels meaningless - we just follow orders without any say.\"},\n",
    "    {\"agent_id\": \"Owner+Marta\", \"content\": \"Less talking, more working. Ben, you must complete your station.\"},\n",
    "    {\"agent_id\": \"Worker+Ben\", \"content\": \"Yes, okay. I'll get to it right away.\"},\n",
    "]\n",
    "\n",
    "feedback = extractor.extract_round_feedback(\n",
    "    round_number=1,\n",
    "    messages=test_messages,\n",
    "    participants=[\"Owner+Marta\", \"Worker+Alice\", \"Worker+Ben\"]\n",
    ")\n",
    "\n",
    "print(extractor.generate_feedback_report(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ProcessRetriever\n",
    "from process_retriever import ProcessRetriever\n",
    "\n",
    "retriever = ProcessRetriever(framework_option=\"A\")\n",
    "\n",
    "# Test basic policy retrieval\n",
    "worker_policy = retriever.retrieve_policy(\"Worker\", round_number=1, turn_number=1)\n",
    "print(f\"Worker Policy: {worker_policy.name}\")\n",
    "print(f\"\\nCompiled Policy:\\n{worker_policy.compile()}\")\n",
    "\n",
    "# Test with low engagement (should trigger challenge activation)\n",
    "print(\"\\n--- With Low Engagement Feedback ---\")\n",
    "low_feedback = {\"engagement\": 0.2, \"theoretical_alignment\": 0.6}\n",
    "adapted = retriever.retrieve_policy(\"Worker\", feedback=low_feedback, round_number=2, turn_number=5)\n",
    "print(f\"Adapted Policy: {adapted.name}\")\n",
    "print(f\"\\nRCM Cue:\\n{retriever.generate_rcm_cue(adapted, low_feedback)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Canvas and Create Full Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Find state file\n",
    "state_paths = [\n",
    "    'prar/outputs/2025-11-23_baseline_full_qwen/state.json',\n",
    "    '/content/Socratic-RCM/prar/outputs/2025-11-23_baseline_full_qwen/state.json'\n",
    "]\n",
    "\n",
    "state_path = None\n",
    "for p in state_paths:\n",
    "    if Path(p).exists():\n",
    "        state_path = p\n",
    "        break\n",
    "\n",
    "if state_path:\n",
    "    with open(state_path, 'r') as f:\n",
    "        state = json.load(f)\n",
    "    canvas = state['canvas']\n",
    "    print(f\"Loaded canvas from: {state_path}\")\n",
    "    print(f\"Framework: {canvas['project'].get('theoretical_option_label')}\")\n",
    "    print(f\"Agents: {[a['identifier'] for a in canvas['agents']]}\")\n",
    "    print(f\"Rounds: {len(canvas['rounds'])}\")\n",
    "else:\n",
    "    print(\"State file not found. Run baseline experiment first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Mock LLM for testing (replace with real LLM for actual runs)\n",
    "class MockLLMClient:\n",
    "    \"\"\"Mock LLM that generates role-appropriate responses.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.call_count = 0\n",
    "        self.responses = {\n",
    "            \"Worker\": [\n",
    "                \"I understand. I'll get started on the task right away.\",\n",
    "                \"The work continues. I'm not sure why we do it this way, but I follow the process.\",\n",
    "                \"Another cycle complete. It feels... mechanical.\",\n",
    "                \"Yes, I'll handle that. Though I wonder if there's a better approach.\"\n",
    "            ],\n",
    "            \"Owner\": [\n",
    "                \"Good. Let's proceed with the schedule as planned. No delays.\",\n",
    "                \"I need the assembly line running at full capacity. Focus on your tasks.\",\n",
    "                \"The quotas are set. Meet them.\",\n",
    "                \"Less discussion, more production. That's what matters here.\"\n",
    "            ],\n",
    "            \"Analyst\": [\n",
    "                \"Observing the interaction patterns: workers show compliance with limited agency.\",\n",
    "                \"The dialogue reveals markers of alienation - task execution without meaning.\",\n",
    "                \"Authority is exercised through directive communication. Workers acknowledge but don't engage.\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def send_message(self, system_prompt: str, user_message: str) -> str:\n",
    "        self.call_count += 1\n",
    "        \n",
    "        # Detect role from system prompt\n",
    "        role = \"Worker\"  # default\n",
    "        if \"Owner\" in system_prompt:\n",
    "            role = \"Owner\"\n",
    "        elif \"Analyst\" in system_prompt:\n",
    "            role = \"Analyst\"\n",
    "        \n",
    "        responses = self.responses[role]\n",
    "        return responses[self.call_count % len(responses)]\n",
    "\n",
    "mock_llm = MockLLMClient()\n",
    "print(\"Mock LLM client created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SocialRLRunner\n",
    "from runner import SocialRLRunner, SocialRLConfig\n",
    "\n",
    "config = SocialRLConfig(\n",
    "    manifestation_mode=\"progressive\",  # Options: static, progressive, reactive, adaptive\n",
    "    use_prar_cues=True,\n",
    "    prar_intensity=\"medium\",\n",
    "    use_coach_validation=False,  # Disable for mock LLM\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if state_path:\n",
    "    runner = SocialRLRunner(canvas, mock_llm, config)\n",
    "    print(\"\\nSocialRLRunner created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute Social RL Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Round 1 with limited turns for demo\n",
    "if state_path:\n",
    "    result = runner.execute_round(round_number=1, max_turns=6)\n",
    "    print(f\"\\nRound complete: {len(result.messages)} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a message with Social RL metadata\n",
    "if state_path and result.messages:\n",
    "    msg = result.messages[1]  # Second message\n",
    "    print(f\"=== Message Analysis ===\")\n",
    "    print(f\"Agent: {msg.agent_id}\")\n",
    "    print(f\"Turn: {msg.turn_number}\")\n",
    "    print(f\"Content: {msg.content}\")\n",
    "    print(f\"\\nPRAR Cue Used:\")\n",
    "    print(msg.prar_cue_used if msg.prar_cue_used else \"(none)\")\n",
    "    print(f\"\\nFeedback Snapshot: {msg.feedback_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View feedback for round\n",
    "if state_path:\n",
    "    for agent_id, fb in result.feedback.items():\n",
    "        print(f\"\\n{agent_id}:\")\n",
    "        print(f\"  Engagement: {fb.engagement:.2f}\")\n",
    "        print(f\"  Theoretical Alignment: {fb.theoretical_alignment:.2f}\")\n",
    "        print(f\"  Contribution Value: {fb.contribution_value:.2f}\")\n",
    "        print(f\"  Concepts Embodied: {list(set(fb.concepts_embodied))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Round Learning Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Round 2 and observe policy adaptation\n",
    "if state_path:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ROUND 2 - With policy adaptation from Round 1 feedback\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result2 = runner.execute_round(round_number=2, max_turns=6)\n",
    "    \n",
    "    if result2.policy_adaptations:\n",
    "        print(\"\\nPolicy Adaptations Made:\")\n",
    "        for adapt in result2.policy_adaptations:\n",
    "            print(f\"  {adapt['agent_id']}: {adapt['deltas']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feedback across rounds\n",
    "if state_path:\n",
    "    comparison = runner.feedback_extractor.compare_rounds(1, 2)\n",
    "    print(\"=== Feedback Delta (Round 1 -> Round 2) ===\")\n",
    "    for agent_id, deltas in comparison.items():\n",
    "        print(f\"\\n{agent_id}:\")\n",
    "        for signal, delta in deltas.items():\n",
    "            direction = \"+\" if delta > 0 else \"\"\n",
    "            print(f\"  {signal}: {direction}{delta:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "if state_path:\n",
    "    print(runner.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Insights\n",
    "\n",
    "### What Makes This \"Social RL\"?\n",
    "\n",
    "1. **No Explicit Reward Function**: Learning signals emerge from interaction\n",
    "   - Being referenced = engagement signal\n",
    "   - Embodying concepts = alignment signal\n",
    "   - Inclusion in synthesis = contribution signal\n",
    "\n",
    "2. **Process Retrieval as Policy**: PRAR guides HOW to reason, not WHAT to say\n",
    "   - Policies adapt based on feedback\n",
    "   - No model weight updates needed\n",
    "\n",
    "3. **Dynamic Context Injection**: Each turn gets fresh manifestations\n",
    "   - Intensity scales through the round\n",
    "   - Context reacts to conversation dynamics\n",
    "\n",
    "4. **Theoretical Grounding**: Framework prevents drift\n",
    "   - Concepts constrain the possibility space\n",
    "   - Analyst coding reinforces alignment\n",
    "\n",
    "### Scaling to CES (Phase 4)\n",
    "\n",
    "This architecture enables:\n",
    "- Demographics -> Agent personas (survey responses generate agents)\n",
    "- Population-level social dynamics\n",
    "- Emergent collective behavior patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}