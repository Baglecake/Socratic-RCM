{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Socratic RCM - Run on Colab GPU\n",
    "\n",
    "Run this notebook in VS Code with the Colab extension:\n",
    "1. Select Kernel → Colab → New Colab Server\n",
    "2. Run all cells\n",
    "3. Interact with the workflow in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers accelerate torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model - Qwen 2.5 (excellent instruction following)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Choose model size based on GPU:\n# - T4 (16GB): Use 7B with float16\n# - A100: Can use larger models\nMODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # Great quality\n# Alternative smaller: \"Qwen/Qwen2.5-3B-Instruct\" (faster, less VRAM)\n\nprint(f\"Loading {MODEL_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nprint(\"Model loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple LLM client that uses the loaded model\n",
    "class ColabLLMClient:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "    \n",
    "    def send_message(self, system_prompt: str, user_message: str) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "        output = self.pipe(messages)\n",
    "        return output[0][\"generated_text\"][-1][\"content\"]\n",
    "    \n",
    "    def send_json(self, system_prompt: str, user_message: str) -> dict:\n",
    "        import json\n",
    "        response = self.send_message(system_prompt + \"\\nRespond with valid JSON only.\", user_message)\n",
    "        try:\n",
    "            return json.loads(response)\n",
    "        except:\n",
    "            return {\"valid\": True, \"feedback\": \"\"}\n",
    "\n",
    "llm_client = ColabLLMClient(model, tokenizer)\n",
    "print(\"LLM client ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_response = llm_client.send_message(\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"Say hello in one sentence.\"\n",
    ")\n",
    "print(f\"Test response: {test_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download orchestrator files from GitHub\n!pip install requests -q\n\nimport os\n\n# Always re-download to get latest versions\nprint(\"Downloading orchestrator files...\")\n!wget -q -O runtime_parser.py https://raw.githubusercontent.com/Baglecake/Socratic-RCM/main/local_rcm/runtime_parser.py\n!wget -q -O canvas_state.py https://raw.githubusercontent.com/Baglecake/Socratic-RCM/main/local_rcm/canvas_state.py\n!wget -q -O orchestrator.py https://raw.githubusercontent.com/Baglecake/Socratic-RCM/main/local_rcm/orchestrator.py\n!wget -q -O llm_client.py https://raw.githubusercontent.com/Baglecake/Socratic-RCM/main/local_rcm/llm_client.py\n!wget -q -O bios_reduced_prompt.txt https://raw.githubusercontent.com/Baglecake/Socratic-RCM/main/local_rcm/bios_reduced_prompt.txt\n!mkdir -p runtime-files\n!wget -q -O runtime-files/B42_Runtime_Phase1_Conceptualization.txt https://raw.githubusercontent.com/Baglecake/Socratic-RCM/main/local_rcm/runtime-files/B42_Runtime_Phase1_Conceptualization.txt\n!wget -q -O runtime-files/B42_Runtime_Phase2_Drafting.txt https://raw.githubusercontent.com/Baglecake/Socratic-RCM/main/local_rcm/runtime-files/B42_Runtime_Phase2_Drafting.txt\n!wget -q -O runtime-files/B42_Runtime_Phase3_Review.txt https://raw.githubusercontent.com/Baglecake/Socratic-RCM/main/local_rcm/runtime-files/B42_Runtime_Phase3_Review.txt\nprint(\"Files downloaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import orchestrator components\n",
    "from runtime_parser import Runtime\n",
    "from canvas_state import CanvasState\n",
    "from orchestrator import WorkflowOrchestrator, StudentInteractionHandler\n",
    "\n",
    "# Load runtime files\n",
    "runtime = Runtime(\n",
    "    \"runtime-files/B42_Runtime_Phase1_Conceptualization.txt\",\n",
    "    \"runtime-files/B42_Runtime_Phase2_Drafting.txt\",\n",
    "    \"runtime-files/B42_Runtime_Phase3_Review.txt\"\n",
    ")\n",
    "print(f\"Loaded {len(runtime.steps)} workflow steps\")\n",
    "\n",
    "# Load BIOS prompt\n",
    "with open(\"bios_reduced_prompt.txt\", \"r\") as f:\n",
    "    bios_prompt = f.read()\n",
    "print(\"BIOS prompt loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create student interaction handler using our Colab LLM\n",
    "class ColabStudentHandler(StudentInteractionHandler):\n",
    "    def __init__(self, llm_client, bios_prompt):\n",
    "        self.llm = llm_client\n",
    "        self.bios_prompt = bios_prompt\n",
    "    \n",
    "    def ask_question(self, question: str, rcm_cue: str = None, context: str = None) -> str:\n",
    "        # Just return the question - LLM adds RCM flavor\n",
    "        full_prompt = question\n",
    "        if rcm_cue:\n",
    "            full_prompt += f\" RCM: '{rcm_cue}'\"\n",
    "        return self.llm.send_message(self.bios_prompt, full_prompt)\n",
    "    \n",
    "    def validate_answer(self, answer: str, constraint: str, context: str = None) -> dict:\n",
    "        # Simple validation - accept most answers\n",
    "        if not answer or len(answer.strip()) < 3:\n",
    "            return {\"valid\": False, \"feedback\": \"Please provide a more detailed answer.\"}\n",
    "        return {\"valid\": True, \"feedback\": \"\"}\n",
    "\n",
    "student_handler = ColabStudentHandler(llm_client, bios_prompt)\n",
    "print(\"Student handler ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the orchestrator\n",
    "orchestrator = WorkflowOrchestrator(\n",
    "    runtime=runtime,\n",
    "    student_handler=student_handler,\n",
    "    starting_step=\"1.1\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SOCRATIC RCM - Running on Colab GPU\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou will be asked questions step by step.\")\n",
    "print(\"Type your answers and press Enter.\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run the workflow\n",
    "final_canvas = orchestrator.run_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final state\n",
    "orchestrator.save_state(\"workflow_final_state.json\")\n",
    "print(\"\\nWorkflow state saved to workflow_final_state.json\")\n",
    "print(f\"Total steps completed: {len(orchestrator.get_student_answers())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}