{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Server for Socratic RCM\n",
    "\n",
    "Run this notebook on Google Colab (with GPU runtime) to serve a model via OpenAI-compatible API.\n",
    "\n",
    "**Steps:**\n",
    "1. Change runtime to GPU (Runtime → Change runtime type → T4 GPU)\n",
    "2. Run all cells\n",
    "3. Copy the ngrok URL and use it in your local orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install vllm pyngrok -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set your ngrok auth token (get free token at https://ngrok.com)\nNGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"  # <-- Replace this!\n\n# Model to serve - Qwen 2.5 works great\nMODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyngrok import ngrok\n\n# Authenticate ngrok\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\n\n# Create tunnel to port 8000\npublic_url = ngrok.connect(8000).public_url\n\nprint(f\"\\n{'='*60}\")\nprint(f\"PUBLIC URL: {public_url}\")\nprint(f\"{'='*60}\")\nprint(f\"\\nUse this in your local orchestrator:\")\nprint(f\"python example_usage.py --mode vllm --base-url {public_url}/v1\")\nprint(f\"{'='*60}\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM server (this will run until you stop it)\n",
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --trust-remote-code \\\n",
    "    --max-model-len 4096"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}