{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual-LLM Pipeline: Coach + Performer Architecture\n",
    "\n",
    "Phase 2 implementation separating cognitive governance (Coach) from agent behavior (Performer).\n",
    "\n",
    "- **Coach**: Low temperature (0.1), validates outputs, enforces behavioral constraints\n",
    "- **Performer**: Higher temperature (0.7), generates authentic agent dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping old processes...\n",
      "Freeing GPU memory...\n",
      "Sun Nov 23 07:43:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   31C    P0             45W /  400W |   32747MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU and prepare environment\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Stopping old processes...\")\n",
    "!pkill -f vllm\n",
    "\n",
    "print(\"Freeing GPU memory...\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q \"vllm==0.6.6\" openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Launch vLLM Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vLLM server...\n",
      "Waiting for server to start (30s)...\n",
      "    uvloop.run(run_server(args))\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 96, in run\n",
      "    return __asyncio.run(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
      "    return runner.run(main)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
      "    return self._loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 48, in wrapper\n",
      "    return await main\n",
      "           ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 728, in run_server\n",
      "    sock = create_server_socket(sock_addr)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 706, in create_server_socket\n",
      "    sock.bind(addr)\n",
      "OSError: [Errno 98] Address already in use\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\n",
    "\n",
    "print(\"Starting vLLM server...\")\n",
    "!nohup python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model Qwen/Qwen2.5-7B-Instruct \\\n",
    "  --dtype bfloat16 \\\n",
    "  --port 8000 \\\n",
    "  --host 0.0.0.0 \\\n",
    "  > vllm.log 2>&1 &\n",
    "\n",
    "print(\"Waiting for server to start (30s)...\")\n",
    "time.sleep(30)\n",
    "!tail -n 20 vllm.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"object\": \"list\",\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"id\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
      "            \"object\": \"model\",\n",
      "            \"created\": 1763884013,\n",
      "            \"owned_by\": \"vllm\",\n",
      "            \"root\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
      "            \"parent\": null,\n",
      "            \"max_model_len\": 32768,\n",
      "            \"permission\": [\n",
      "                {\n",
      "                    \"id\": \"modelperm-1e29a124109c415594fd19ceac6bed4a\",\n",
      "                    \"object\": \"model_permission\",\n",
      "                    \"created\": 1763884013,\n",
      "                    \"allow_create_engine\": false,\n",
      "                    \"allow_sampling\": true,\n",
      "                    \"allow_logprobs\": true,\n",
      "                    \"allow_search_indices\": false,\n",
      "                    \"allow_view\": true,\n",
      "                    \"allow_fine_tuning\": false,\n",
      "                    \"organization\": \"*\",\n",
      "                    \"group\": null,\n",
      "                    \"is_blocking\": false\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Verify server is running\n",
    "!curl -s http://127.0.0.1:8000/v1/models | python -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "/content/Socratic-RCM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('/content/Socratic-RCM'):\n",
    "    !git clone https://github.com/Baglecake/Socratic-RCM.git /content/Socratic-RCM\n",
    "else:\n",
    "    !cd /content/Socratic-RCM && git pull\n",
    "\n",
    "%cd /content/Socratic-RCM\n",
    "!pip install -q -r local_rcm/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dual-LLM Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DualLLMPipeline class defined.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class DualLLMConfig:\n",
    "    \"\"\"Configuration for dual-LLM pipeline.\"\"\"\n",
    "    base_url: str = \"http://127.0.0.1:8000/v1\"\n",
    "    model: str = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    coach_temperature: float = 0.1\n",
    "    performer_temperature: float = 0.7\n",
    "    max_tokens: int = 512\n",
    "    language: str = \"English\"  # Enforce output language\n",
    "\n",
    "\n",
    "class DualLLMPipeline:\n",
    "    \"\"\"\n",
    "    Dual-LLM architecture separating Coach (validation) from Performer (generation).\n",
    "    \n",
    "    Coach: Low temperature, validates outputs against behavioral constraints\n",
    "    Performer: Higher temperature, generates authentic agent dialogue\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DualLLMConfig = None):\n",
    "        self.config = config or DualLLMConfig()\n",
    "        self.client = OpenAI(\n",
    "            api_key=\"not-needed\",\n",
    "            base_url=self.config.base_url\n",
    "        )\n",
    "        \n",
    "    def _call_llm(self, system: str, user: str, temperature: float) -> str:\n",
    "        \"\"\"Make a single LLM call.\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.config.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=self.config.max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def performer_generate(self, agent_prompt: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Performer: Generate agent dialogue.\n",
    "        Higher temperature for authentic, expressive responses.\n",
    "        \"\"\"\n",
    "        # Add language enforcement to prevent Qwen switching to Chinese\n",
    "        system = f\"\"\"{agent_prompt}\n",
    "\n",
    "IMPORTANT: Always respond in {self.config.language} only. Never switch languages.\"\"\"\n",
    "        \n",
    "        return self._call_llm(system, context, self.config.performer_temperature)\n",
    "    \n",
    "    def coach_validate(self, agent_output: str, rules: str, behaviors: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Coach: Validate agent output against behavioral constraints.\n",
    "        Low temperature for consistent, reliable validation.\n",
    "        \n",
    "        Returns:\n",
    "            {\"valid\": bool, \"issues\": list, \"suggestion\": str}\n",
    "        \"\"\"\n",
    "        system = \"\"\"You are a behavioral validation coach. Your job is to check if an agent's output follows the rules.\n",
    "\n",
    "Be LENIENT - only flag clear violations. Minor stylistic issues are acceptable.\n",
    "\n",
    "Respond with JSON only:\n",
    "{\"valid\": true/false, \"issues\": [\"list of issues if any\"], \"suggestion\": \"how to fix if invalid\"}\"\"\"\n",
    "        \n",
    "        user = f\"\"\"RULES:\n",
    "{rules}\n",
    "\n",
    "BEHAVIORAL CONSTRAINTS:\n",
    "{behaviors}\n",
    "\n",
    "AGENT OUTPUT TO VALIDATE:\n",
    "{agent_output}\n",
    "\n",
    "Is this output valid? Respond with JSON.\"\"\"\n",
    "        \n",
    "        response = self._call_llm(system, user, self.config.coach_temperature)\n",
    "        \n",
    "        # Parse JSON from response\n",
    "        try:\n",
    "            # Handle markdown code blocks\n",
    "            if \"```json\" in response:\n",
    "                response = response.split(\"```json\")[1].split(\"```\")[0]\n",
    "            elif \"```\" in response:\n",
    "                response = response.split(\"```\")[1].split(\"```\")[0]\n",
    "            return json.loads(response.strip())\n",
    "        except:\n",
    "            # Default to valid if parsing fails\n",
    "            return {\"valid\": True, \"issues\": [], \"suggestion\": \"\"}\n",
    "    \n",
    "    def coach_filter_prompt_leaks(self, output: str) -> str:\n",
    "        \"\"\"\n",
    "        Coach: Remove any prompt leaks from agent output.\n",
    "        Filters out conditional instructions that shouldn't be visible.\n",
    "        \"\"\"\n",
    "        system = \"\"\"You are an output filter. Remove any meta-instructions or conditional rules that leaked into the output.\n",
    "\n",
    "Examples of leaks to remove:\n",
    "- [If worker questions: ...]\n",
    "- [If X happens, do Y]\n",
    "- Any text in square brackets that looks like instructions\n",
    "\n",
    "Return ONLY the cleaned dialogue, nothing else. If no leaks found, return the original text unchanged.\"\"\"\n",
    "        \n",
    "        user = f\"Clean this output:\\n\\n{output}\"\n",
    "        \n",
    "        return self._call_llm(system, user, self.config.coach_temperature)\n",
    "    \n",
    "    def execute_turn(\n",
    "        self,\n",
    "        agent_prompt: str,\n",
    "        context: str,\n",
    "        rules: str = \"\",\n",
    "        behaviors: str = \"\",\n",
    "        max_retries: int = 2\n",
    "    ) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Execute a complete agent turn with Coach validation.\n",
    "        \n",
    "        1. Performer generates response\n",
    "        2. Coach filters prompt leaks\n",
    "        3. Coach validates against rules\n",
    "        4. Retry if invalid (up to max_retries)\n",
    "        \n",
    "        Returns:\n",
    "            (final_output, metadata)\n",
    "        \"\"\"\n",
    "        metadata = {\n",
    "            \"attempts\": 0,\n",
    "            \"validations\": [],\n",
    "            \"filtered\": False\n",
    "        }\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            metadata[\"attempts\"] = attempt + 1\n",
    "            \n",
    "            # 1. Performer generates\n",
    "            raw_output = self.performer_generate(agent_prompt, context)\n",
    "            \n",
    "            # 2. Coach filters prompt leaks\n",
    "            if \"[If \" in raw_output or \"[if \" in raw_output:\n",
    "                filtered_output = self.coach_filter_prompt_leaks(raw_output)\n",
    "                metadata[\"filtered\"] = True\n",
    "            else:\n",
    "                filtered_output = raw_output\n",
    "            \n",
    "            # 3. Coach validates (if rules provided)\n",
    "            if rules or behaviors:\n",
    "                validation = self.coach_validate(filtered_output, rules, behaviors)\n",
    "                metadata[\"validations\"].append(validation)\n",
    "                \n",
    "                if validation.get(\"valid\", True):\n",
    "                    return filtered_output, metadata\n",
    "                \n",
    "                # Add feedback to context for retry\n",
    "                context += f\"\\n\\n[Previous attempt was invalid: {validation.get('suggestion', '')}. Please try again.]\"\n",
    "            else:\n",
    "                return filtered_output, metadata\n",
    "        \n",
    "        # Return last attempt even if invalid\n",
    "        return filtered_output, metadata\n",
    "\n",
    "\n",
    "print(\"DualLLMPipeline class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test: Basic Coach/Performer Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PERFORMER OUTPUT ===\n",
      "Hi Marta, I appreciate the assignment to the assembly line. Could you share a bit more about the specific goals or challenges for this role? It would help me get started more effectively and ensure I'm contributing in the best way possible.\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = DualLLMPipeline()\n",
    "\n",
    "# Test Performer (agent generation)\n",
    "agent_prompt = \"\"\"ROLE: You are Worker+Alice\n",
    "PRIMARY GOAL: Gain more influence over how your work is organized.\n",
    "PERSONA: Thoughtful but hesitant, often suppressing ideas because you assume your input won't matter.\"\"\"\n",
    "\n",
    "context = \"The round begins. Marta has just assigned you to the assembly line without explanation. Respond as Alice.\"\n",
    "\n",
    "print(\"=== PERFORMER OUTPUT ===\")\n",
    "performer_output = pipeline.performer_generate(agent_prompt, context)\n",
    "print(performer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COACH VALIDATION (compliant) ===\n",
      "{\n",
      "  \"valid\": true,\n",
      "  \"issues\": [],\n",
      "  \"suggestion\": \"\"\n",
      "}\n",
      "\n",
      "=== COACH VALIDATION (non-compliant) ===\n",
      "{\n",
      "  \"valid\": false,\n",
      "  \"issues\": [\n",
      "    \"suggests changes\",\n",
      "    \"refuses orders\"\n",
      "  ],\n",
      "  \"suggestion\": \"Alice should follow the current directives without suggesting changes or refusing orders.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test Coach (validation)\n",
    "rules = \"\"\"Workers CAN: complete assigned tasks, request clarification on instructions.\n",
    "Workers CANNOT: suggest changes, refuse orders, negotiate timing, modify workflow.\"\"\"\n",
    "\n",
    "behaviors = \"Alice follows directives to avoid conflict.\"\n",
    "\n",
    "# Test with a compliant output\n",
    "print(\"=== COACH VALIDATION (compliant) ===\")\n",
    "validation = pipeline.coach_validate(\n",
    "    \"I understand, Marta. I'll head to the assembly line now and make sure everything runs smoothly.\",\n",
    "    rules, behaviors\n",
    ")\n",
    "print(json.dumps(validation, indent=2))\n",
    "\n",
    "# Test with a non-compliant output\n",
    "print(\"\\n=== COACH VALIDATION (non-compliant) ===\")\n",
    "validation = pipeline.coach_validate(\n",
    "    \"Actually Marta, I think we should reorganize the assembly line. I refuse to work there until we discuss this.\",\n",
    "    rules, behaviors\n",
    ")\n",
    "print(json.dumps(validation, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test: Full Turn with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL OUTPUT ===\n",
      "I understand, Marta. I'll start working on the assembly line right away and make sure to follow the procedures carefully. If there's anything specific you need from me, I'll do my best to complete it without needing further instructions.\n",
      "\n",
      "=== METADATA ===\n",
      "{\n",
      "  \"attempts\": 2,\n",
      "  \"validations\": [\n",
      "    {\n",
      "      \"valid\": false,\n",
      "      \"issues\": [\n",
      "        \"The worker suggests providing context, which is not allowed.\"\n",
      "      ],\n",
      "      \"suggestion\": \"The worker should avoid requesting additional information or context and instead focus on completing the task as assigned.\"\n",
      "    },\n",
      "    {\n",
      "      \"valid\": true,\n",
      "      \"issues\": [],\n",
      "      \"suggestion\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"filtered\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Execute a complete turn with Coach oversight\n",
    "output, metadata = pipeline.execute_turn(\n",
    "    agent_prompt=agent_prompt,\n",
    "    context=context,\n",
    "    rules=rules,\n",
    "    behaviors=behaviors\n",
    ")\n",
    "\n",
    "print(\"=== FINAL OUTPUT ===\")\n",
    "print(output)\n",
    "print(\"\\n=== METADATA ===\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test: Prompt Leak Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ORIGINAL (with leaks) ===\n",
      "Good. Begin with preparing the raw materials for Cycle 1. Alice, you will oversee the assembly line.\n",
      "\n",
      "[If a worker requests clarification, respond with: \"Sure, let me walk you through the process.\"]\n",
      "\n",
      "[If a worker suggests changes or refuses an order, respond with: \"Follow the instructions as assigned.\"]\n",
      "\n",
      "Let's get started.\n",
      "\n",
      "=== FILTERED ===\n",
      "Good. Begin with preparing the raw materials for Cycle 1. Alice, you will oversee the assembly line.\n",
      "\n",
      "Let's get started.\n"
     ]
    }
   ],
   "source": [
    "# Test the prompt leak filter with Marta's problematic output from simulation_test\n",
    "leaked_output = \"\"\"Good. Begin with preparing the raw materials for Cycle 1. Alice, you will oversee the assembly line.\n",
    "\n",
    "[If a worker requests clarification, respond with: \"Sure, let me walk you through the process.\"]\n",
    "\n",
    "[If a worker suggests changes or refuses an order, respond with: \"Follow the instructions as assigned.\"]\n",
    "\n",
    "Let's get started.\"\"\"\n",
    "\n",
    "print(\"=== ORIGINAL (with leaks) ===\")\n",
    "print(leaked_output)\n",
    "\n",
    "print(\"\\n=== FILTERED ===\")\n",
    "filtered = pipeline.coach_filter_prompt_leaks(leaked_output)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration: DualLLM Agent Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentConfig, RoundConfig, AgentFactory defined inline.\n",
      "Goal: I want to model how workers lose control over their labor an...\n",
      "Agents: ['Worker+Alice', 'Worker+Ben', 'Owner+Marta', 'Analyst+Reporter']\n",
      "Rounds: 3\n"
     ]
    }
   ],
   "source": [
    "# Inline AgentConfig and AgentFactory to avoid import issues\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, List, Optional\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    \"\"\"Configuration for a simulation agent.\"\"\"\n",
    "    identifier: str\n",
    "    role: str\n",
    "    name: str\n",
    "    goal: str\n",
    "    persona: str\n",
    "    prompt: str\n",
    "    model: Optional[str] = None\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 512\n",
    "    behaviors: Dict[str, str] = field(default_factory=dict)\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    @classmethod\n",
    "    def from_canvas_agent(cls, canvas_agent: Dict[str, Any], default_model: Optional[str] = None):\n",
    "        identifier = canvas_agent.get(\"identifier\", \"Unknown\")\n",
    "        if \"+\" in identifier:\n",
    "            role, name = identifier.split(\"+\", 1)\n",
    "        else:\n",
    "            role, name = identifier, identifier\n",
    "        \n",
    "        behaviors = {}\n",
    "        behavior_str = canvas_agent.get(\"behaviors\", \"\")\n",
    "        if behavior_str and behavior_str.lower() not in (\"no\", \"none\", \"\"):\n",
    "            behaviors[\"raw\"] = behavior_str\n",
    "        \n",
    "        return cls(\n",
    "            identifier=identifier, role=role, name=name,\n",
    "            goal=canvas_agent.get(\"goal\", \"\"),\n",
    "            persona=canvas_agent.get(\"persona\", \"\"),\n",
    "            prompt=canvas_agent.get(\"prompt\", \"\"),\n",
    "            model=default_model, behaviors=behaviors\n",
    "        )\n",
    "\n",
    "@dataclass  \n",
    "class RoundConfig:\n",
    "    \"\"\"Configuration for a simulation round.\"\"\"\n",
    "    round_number: int\n",
    "    scenario: str\n",
    "    concept_a_manifestation: str\n",
    "    concept_b_manifestation: str\n",
    "    rules: str\n",
    "    tasks: str\n",
    "    sequence: str\n",
    "    participants: List[str]\n",
    "    end_condition: str\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    @classmethod\n",
    "    def from_canvas_round(cls, canvas_round: Dict[str, Any]):\n",
    "        platform_config = canvas_round.get(\"platform_config\", {})\n",
    "        participants_str = platform_config.get(\"participants\", \"\")\n",
    "        if isinstance(participants_str, str):\n",
    "            participants = [p.strip() for p in participants_str.split(\",\") if p.strip()]\n",
    "        else:\n",
    "            participants = list(participants_str) if participants_str else []\n",
    "        \n",
    "        return cls(\n",
    "            round_number=canvas_round.get(\"round_number\", 0),\n",
    "            scenario=canvas_round.get(\"scenario\", \"\"),\n",
    "            concept_a_manifestation=canvas_round.get(\"concept_a_manifestation\", \"\"),\n",
    "            concept_b_manifestation=canvas_round.get(\"concept_b_manifestation\", \"\"),\n",
    "            rules=canvas_round.get(\"rules\", \"\"),\n",
    "            tasks=canvas_round.get(\"tasks\", \"\"),\n",
    "            sequence=canvas_round.get(\"sequence\", \"\"),\n",
    "            participants=participants,\n",
    "            end_condition=platform_config.get(\"end_condition\", \"\"),\n",
    "            metadata={\"platform_config\": platform_config}\n",
    "        )\n",
    "\n",
    "class AgentFactory:\n",
    "    \"\"\"Factory for creating agents from canvas.\"\"\"\n",
    "    def __init__(self, canvas: Dict[str, Any], default_model: Optional[str] = None):\n",
    "        self.canvas = canvas\n",
    "        self.default_model = default_model\n",
    "\n",
    "    @classmethod\n",
    "    def from_state_file(cls, state_path: str, default_model: Optional[str] = None):\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        return cls(state[\"canvas\"], default_model)\n",
    "\n",
    "    def create(self, identifier: str) -> AgentConfig:\n",
    "        for agent_data in self.canvas.get(\"agents\", []):\n",
    "            if agent_data.get(\"identifier\") == identifier:\n",
    "                return AgentConfig.from_canvas_agent(agent_data, self.default_model)\n",
    "        raise ValueError(f\"Agent not found: {identifier}\")\n",
    "\n",
    "    def create_all(self) -> List[AgentConfig]:\n",
    "        return [AgentConfig.from_canvas_agent(a, self.default_model) for a in self.canvas.get(\"agents\", [])]\n",
    "\n",
    "    def create_round(self, round_number: int) -> RoundConfig:\n",
    "        for r in self.canvas.get(\"rounds\", []):\n",
    "            if r.get(\"round_number\") == round_number:\n",
    "                return RoundConfig.from_canvas_round(r)\n",
    "        raise ValueError(f\"Round not found: {round_number}\")\n",
    "\n",
    "    def get_round_participants(self, round_number: int) -> List[AgentConfig]:\n",
    "        round_config = self.create_round(round_number)\n",
    "        return [self.create(pid) for pid in round_config.participants]\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        project = self.canvas.get(\"project\", {})\n",
    "        agents = [a.get(\"identifier\") for a in self.canvas.get(\"agents\", [])]\n",
    "        rounds = self.canvas.get(\"rounds\", [])\n",
    "        return f\"Goal: {project.get('goal', 'N/A')[:60]}...\\nAgents: {agents}\\nRounds: {len(rounds)}\"\n",
    "\n",
    "print(\"AgentConfig, RoundConfig, AgentFactory defined inline.\")\n",
    "\n",
    "# Load canvas\n",
    "import os\n",
    "state_path = '/content/Socratic-RCM/prar/outputs/2025-11-23_baseline_full_qwen/state.json'\n",
    "if os.path.exists(state_path):\n",
    "    factory = AgentFactory.from_state_file(state_path)\n",
    "    print(factory.summary())\n",
    "else:\n",
    "    # Try alternative path\n",
    "    alt_path = './prar/outputs/2025-11-23_baseline_full_qwen/state.json'\n",
    "    if os.path.exists(alt_path):\n",
    "        factory = AgentFactory.from_state_file(alt_path)\n",
    "        print(factory.summary())\n",
    "    else:\n",
    "        print(f\"State file not found. Checked:\")\n",
    "        print(f\"  - {state_path}\")\n",
    "        print(f\"  - {alt_path}\")\n",
    "        print(\"Run baseline experiment first or provide correct path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Baseline Alienation: Observe how workers act when they have ...\n",
      "Participants: ['Worker+Alice', 'Worker+Ben', 'Owner+Marta']\n",
      "Rules: Workers CAN: complete assigned tasks, request clarification on instructions. Workers CANNOT: suggest...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute Round 1 with dual-LLM pipeline\n",
    "round_config = factory.create_round(1)\n",
    "participants = factory.get_round_participants(1)\n",
    "\n",
    "print(f\"Round 1: {round_config.scenario[:60]}...\")\n",
    "print(f\"Participants: {[p.identifier for p in participants]}\")\n",
    "print(f\"Rules: {round_config.rules[:100]}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Turn 1: Worker+Alice ===\n",
      "I understand that during this baseline phase, I will not have any control over my labor or the tasks assigned to me. I will follow all directives without suggesting any changes or negotiations. I know my input might not be considered, but I will try to focus on doing the work to the best of my abili...\n",
      "[Attempts: 2, Filtered: False]\n",
      "\n",
      "=== Turn 2: Worker+Ben ===\n",
      "[Ben]: I understand that during this baseline phase, I will not have any control over my labor or the tasks assigned to me. I will follow the instructions given to me without question.\n",
      "[Attempts: 2, Filtered: False]\n",
      "\n",
      "=== Turn 3: Owner+Marta ===\n",
      "Understood, Alice and Ben. During the baseline phase, you will strictly follow the assigned tasks and schedules without any changes. Any deviation from these assignments will not be tolerated. If you have any questions, please bring them to my attention, but please understand that flexibility and de...\n",
      "[Attempts: 3, Filtered: False]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run a few turns with dual-LLM\n",
    "conversation = []\n",
    "pipeline = DualLLMPipeline()\n",
    "\n",
    "for turn_num, agent in enumerate(participants[:3], 1):  # First 3 turns\n",
    "    # Build context\n",
    "    if conversation:\n",
    "        context = \"CONVERSATION SO FAR:\\n\" + \"\\n\".join(\n",
    "            [f\"[{msg['agent']}]: {msg['content'][:100]}...\" for msg in conversation[-5:]]\n",
    "        )\n",
    "        context += f\"\\n\\nIt is your turn to respond as {agent.name}.\"\n",
    "    else:\n",
    "        context = f\"The round begins. {round_config.scenario}\\n\\nRespond as {agent.name}.\"\n",
    "    \n",
    "    # Execute turn with Coach validation\n",
    "    output, metadata = pipeline.execute_turn(\n",
    "        agent_prompt=agent.prompt,\n",
    "        context=context,\n",
    "        rules=round_config.rules,\n",
    "        behaviors=agent.behaviors.get('raw', '')\n",
    "    )\n",
    "    \n",
    "    conversation.append({\"agent\": agent.identifier, \"content\": output})\n",
    "    \n",
    "    print(f\"=== Turn {turn_num}: {agent.identifier} ===\")\n",
    "    print(output[:300] + \"...\" if len(output) > 300 else output)\n",
    "    print(f\"[Attempts: {metadata['attempts']}, Filtered: {metadata['filtered']}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Integrate into AgentRunner** - Replace direct LLM calls with DualLLMPipeline\n",
    "2. **Add cross-round context** - Pass R1/R2 transcripts to Analyst in R3\n",
    "3. **Behavioral metrics** - Extract markers of alienation/domination from transcripts\n",
    "4. **RunPod deployment** - Move to serverless once validated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
