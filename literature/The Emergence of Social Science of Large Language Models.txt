The Emergence of Social Science of Large Language Models	Xiao Jia1	Zhanzhan Zhao1,211School of Artificial Intelligence, The Chinese University of Hong Kong, Shenzhen2School of Humanities and Social Science, The Chinese University of Hong Kong, Shenzhenxiaojia@link.cuhk.edu.cn, zhanzhanzhao@cuhk.edu.cnAbstractThe social science of large language models (LLMs) examines how these systems evoke mind attributions, interact with one another, and transform human activity and institutions. We conducted a systematic review of 270 studies, combining text embeddings, unsupervised clustering and topic modeling to build a computational taxonomy. Three domains emerge organically across the reviewed literature. LLM as Social Minds examines whether and when models display behaviors that elicit attributions of cognition, morality and bias, while addressing challenges such as test leakage and surface cues. LLM Societies examines multi-agent settings where interaction protocols, architectures and mechanism design shape coordination, norms, institutions and collective epistemic processes. LLM–Human Interactions examines how LLMs reshape tasks, learning, trust, work and governance, and how risks arise at the human–AI interface. This taxonomy provides a reproducible map of a fragmented field, clarifies evidentiary standards across levels of analysis, and highlights opportunities for cumulative progress in the social science of artificial intelligence.Keywords: large language models; social science; taxonomy; systematic review; human–AI interaction; multi-agent systems; trust; bias; institutions; mind attribution1 MainIn the seminal work The Sciences of the Artificial, Herbert Simon (2019) advanced the view that artifacts differ fundamentally from both natural phenomena and abstract formal systems. They are purpose-driven constructions, designed to meet human ends, yet their meaning and behavior are defined by the environments in which they are embedded. Artifacts thus occupy a liminal ontological space: their functions cannot be fully understood apart from the social, cognitive, and institutional contexts that give them significance. Large language models (LLMs) are paradigmatic examples of such artifacts. Conceived as computational instruments for text prediction, they have rapidly become general-purpose infrastructures that not only execute linguistic tasks but also manifest patterns of interaction and interpretation that invite comparison with psychological, social, and institutional processes (Mei et al., 2024; Park et al., 2023). To conceptualize LLMs merely as technical systems is therefore insufficient; they should be studied as social artifacts, whose significance emerges precisely at the intersection of engineering design and social embedding (Bail, 2024; Peter et al., 2025).This perspective situates the social science of LLMs within a longer intellectual lineage. In sociology and psychology, artifacts have long been treated as windows into cognition and culture: from Vygotsky’s account of tools as mediators of thought, to Latour’s actor–network theory in which technologies shape human associations, to recent cognitive science accounts of distributed cognition (Hutchins, 1995; Latour, 2005; Vygotsky, 1978). LLMs extend this tradition in unprecedented ways. They provoke questions about mind-like attributions when their outputs mimic intentional reasoning (Hu et al., 2025; Kosinski, 2024; Lehr et al., 2025; Strachan et al., 2024). They generate novel social dynamics when multiple models interact, negotiate, or coordinate (Akata et al., 2025; Li et al., 2023; Park et al., 2023). And they are transforming human practices by altering productivity, learning, governance, and communication (Chen & Chan, 2024; Noy & Zhang, 2023). These three levels form the emergent conceptual landscape of the field, and each stream within it is vibrant. Yet despite rapid proliferation, the literature remains fragmented, with a lack of an integrated map that clarifies boundaries, convergences, and gaps across the triad.Past attempts to synthesize adjacent AI-and-society literatures have been either narrative (broad, agenda-setting, e.g. Bommasani, 2021; Xu et al., 2024; Bail, 2024), domain-specific (e.g. education, Yan et al., 2024; law, Lai et al., 2024; medicine, Haltaufderheide and Ranisch, 2024), or task-bounded (e.g. persuasion, Rogiers et al., 2024; misinformation, Kuntur et al., 2024; intelligent agents, Xi et al., 2025; Wang et al., 2024). Such fragmentation has thereby precluded comparative, multi-level insight into the social science of LLMs as a field. What is missing is a systematic attempt to integrate the full range of social science research on LLMs into a single coherent map, one that both reflects the diversity of existing studies and establishes a reproducible structure for future work. Recent innovations suggest that unsupervised computational approaches can supplement traditional narrative synthesis by generating unsupervised computational taxonomy approach that derives machine-based partitions of the literature through embeddings and clustering, making the field’s latent structure visible and testable (Fasce et al., 2023; Healy & McInnes, 2024; Reimers & Gurevych, 2019).Building on these, we address three core questions. First, what are the principal research themes and conceptual categories that organize the social science of LLMs? Second, can unsupervised, data-driven clustering recover a stable and interpretable taxonomy of this literature? Third, how do machine-derived partitions align with, or diverge from, expert human classifications, and what do these divergences reveal about contested constructs and measurement artifacts? To answer these questions, in Study 1, we combine the PRISMA-compliant systematic review of the extant literature with unsupervised machine learning techniques for embedding, clustering, and stability analysis. We embed documents using deep learning–based sentence representations (Reimers & Gurevych, 2019), visualize their distribution via nonlinear manifold learning (McInnes et al.,2018; Wang et al., 2021), and induce clusters via K-means (Lloyd, 1982; McQueen, 1967), validated by internal (silhouette width) and resampling-based stability measures (Adjusted Rand Index, Adjusted Mutual Information) indices (Hubert & Arabie, 1985; Rousseeuw, 1987; Vinh et al., 2009). To avoid any manual thematization, we further derive human-readable themes within each machine-discovered cluster via probabilistic topic modeling (Latent Dirichlet Allocation), selecting the number of topics by a data-driven criterion that balances held-out perplexity and semantic coherence, and visualizing the resulting topics through ??-parameterized word clouds (Blei et al., 2003; Chuang et al., 2012; Mimno et al., 2011). In Study 2, we compare the resulting computational taxonomy against human-coded categories, surfacing alignments and mismatches that sharpen conceptual debates and highlight boundary-spanning works.Our analysis shows that a robust three-way partition organically emerges from the corpus under unsupervised analysis, which we label LLM as Social Minds, LLM Societies, and LLM–Human Interactions. The first cluster consolidates the debate on whether and when LLMs manifest mind-attribution-eliciting behavior across batteries that attempt to control for test leakage and surface cues. The second coalesces work on multi-agent interactions and emergent coordination, including role-play protocols and memory-planning architectures that produce social regularities, alongside norms and institutions, game-theoretic settings, and collective epistemics. The third aggregates causal and observational studies of the human–LLM interface on the reconfiguration of human activity and institutions (perception, trust and reliance, learning, work/productivity and inequality, and interactional risks) while highlighting heterogeneity by task and skill. Importantly, by treating these as levels of analysis rather than mutually exclusive camps, the taxonomy clarifies how claims at one level (e.g., mind-like inference) bear on expectations at another (e.g., institutional deployment), and where the evidentiary standards should differ across levels.The approach resists psychological essentialism in linguistic outputs, triangulates across multiple operationalizations and robustness checks, and explicitly quantifies alignments and divergences between machine partitions and expert labels. It leverages a full-corpus, PRISMA-compliant pipeline and adopts the computational-taxonomy paradigm validated in other domains (e.g. Fasce et al., 2023), producing a systematic and reproducible map that integrates mind-level, societal-level, and LLM–human interaction research into a single, testable framework. This synthesis is structured to render progress cumulative, to surface neglected intersections, to standardize constructs, and to identify where improved measurement and theory can advance a cumulative social science of AI.2 ResultsWe executed a comprehensive multi-database search across Web of Science, Scopus, ACMDigital Library, IEEE Xplore, PubMed, arXiv, and Semantic Scholar (final refresh on 19 September 2025), followed by staged screening with dual reviewers and reconciliation under a PRISMA-compliant workflow (Page et al., 2021). Identification yielded 64,717 records prior to deduplication; removing 49,278 duplicates, 201 records flagged as ineligible by automation (non-article types, incomplete metadata), and 11 for other minor reasons left 15,227 items for title/abstract screening. We sought 328 full texts, of which 12 could not be retrieved; 316 were assessed at full text, 46 excluded (pure engineering n=22, working paper n=4, not LLM n=8, LLM only as tool n=12), leaving 270 studies in the final sample. The PRISMA flow diagram (Figure 1) visualizes this pipeline; implementation details are provided in Section 7.
Figure 1: PRISMA flow diagram for the systematic review process.The distribution of document types is dominated by journal articles. As shown in Table 1, journals account for 142 items (52.6% of n=270), with preprints contributing 65 (24.1%) and conference papers 63 (23.3%). That the journal share exceeds one-half is consistent with rapid consolidation of LLM-and-society research into archival outlets, while the sizeable preprint and conference components preserve timeliness and cross-community diffusion typical of fast-moving, interdisciplinary domains.Table 1: Document Types Distribution (total ?? = 270). This table shows the number of documents of each type in the dataset.TypeCountPercentageJournal14252.6%Preprint6524.1%Conference6323.3%Temporal dynamics mirror the field’s acceleration. Figure 2 shows near-zero activity in 2021(1, 0.4%) and modest growth in 2022 (7, 2.6%), followed by a sharp expansion in 2023 (59, 21.9%) and a peak in 2024 (120, 44.4%). The 2025 count (83, 30.7%) reflects a partial year up to the September cut-off, so the underlying trend is monotonic growth rather than decline. This trajectory is consistent with the diffusion of general-purpose LLMs into social-science questions after their public scaling and deployment, and it provides critical context for interpreting the breadth of topics recovered by the unsupervised taxonomy in Study 1.Figure 2: Publications by Year. Annual distribution of publications in the dataset.Outlet heterogeneity further corroborates the field’s interdisciplinary footprint. Figure 3 reports the Top-20 venues. arXiv is the single most common outlet (65, 24.1%), underscoring the role of preprints in rapid dissemination. Among peer-reviewed journals, Proceedings of the NationalAcademy of Sciences (PNAS; 14, 5.2%) is the leading outlet, followed by Scientific Reports (10,3.7%), Nature Human Behaviour (9, 3.3%), Nature Machine Intelligence (5, 1.9%) and NatureComputational Science (2, 0.7%), while other general science journals include Science (2, 0.7%).Human–computer interaction venues contribute substantially: the ACM CHI Conference on Human Factors in Computing Systems (CHI ’24; 5, 1.9%) and the Proceedings of the ACM on Human–Computer Interaction (3, 1.1%). Natural language processing outlets are represented by the Conference on Empirical Methods in Natural Language Processing (EMNLP 2024; 3, 1.1%).Education and computing venues include the ACM Technical Symposium on Computer Science Education (SIGCSE 2024; 3, 1.1%).Information systems and engineering sources include IEEE Access (5, 1.9%) and the 2024 IEEE International Conference on Big Data (BigData; 4, 1.5%). Interdisciplinary socialscience journals are visible through AI & SOCIETY (5, 1.9%), Humanities and Social Sciences Communications (4, 1.5%), PNAS Nexus (4, 1.5%), and Big Data & Society (2, 0.7%). Finally, additional venues include the IEEE Transactions on Affective Computing (2, 0.7%), Royal Society Open Science (2, 0.7%), and IEEE Intelligent Systems (2, 0.7%). These distributions show that publications on the social science of LLMs are long-tailed across HCI, NLP, education, engineering, and interdisciplinary social-science outlets (see Figure 3).Figure 3: Top Publication Venues (Top 20). The twenty most frequent publication venues, including journals, conferences, and preprint servers.Together, our search and screening process establishes a corpus that is broad across computer science, psychology, sociology, HCI, economics, and general science, and timely, with most studies concentrated in 2023–2025. This structure matters for inference: (i) The substantial proportion of journal publications ensures that the results are not predominantly driven by non-refereed outputs; (ii) the broad dispersion of papers across multiple venues minimizes the influence of venue-specific topical biases; and (iii) the dataset spans a recent yet sufficiently complete timeline, enabling an analysis that is both temporally relevant and comprehensive in scope. The sections that follow therefore treat the descriptive facts above as baseline constraints and proceed to recover the field’s latent structure via unsupervised clustering (Study 1) and quantify its alignment with a theory-informed, expert taxonomy (Study 2). For complete operational details, including search strings, masking procedures, and reliability statistics, see Section 7.2.1 Study 1: Computational Taxonomy via Unsupervised Clustering2.1.1 Global Structure of the CorpusWe embedded the corpus in a high-dimensional semantic space using Sentence-BERT (SBERT) representations and applied ??-means clustering to partition the documents into three clusters,with method details, cluster number justifications, and robustness checks reported in Section 7.3.3. The geometric structure of this three-cluster solution is illustrated in Figure 4, which shows a UMAP projection of the high-dimensional SBERT space for visualization purposes. Moreover, subsequent analyses revealed that these clusters correspond to stable and interpretable semantic domains (see Section 7, Figure 9).Threecoherentbasinsofdensityemergeagainstthebackgroundkernel-densitycontours(Figure4). The green cluster (C0) occupies the upper-left portion of the map, enclosed by a relatively broad 95% confidence ellipse; while its geometric median (black “?”) lies within a dense local core, the cluster as a whole is more spatially dispersed, with several visible sub-concentrations. C0 is also the largest group with ??=111 (41.1%). The orange cluster (C1) concentrates in the lower-right with small spatial variance and forms a comparably large group of ??=99 (36.7%). The blue cluster (C2) spans the mid-right region with an anisotropic vertical ellipse, indicating intermediate levels of within-cluster heterogeneity; it aggregates a more compact set of ??=60 (22.2%). These counts are summarized in Table 2. Negative-silhouette cases (red triangles) are sparse and, as expected, localize to interfacial zones notably between C0 and C2, a pattern consistent with limited overlap between groups.Because assignments are computed in the original embedding space and only projected for display, these spatial diagnostics should be interpreted as qualitative corroboration of the internal-index evidence: three clusters are separated enough to be interpretable while preservingwithin-group variation that later topic models can exploit. Practically, the allocation is balancedbut-asymmetric: all three groups exceed the lower bound typically needed for stable Latent Dirichlet Allocation (LDA) estimation, while slight asymmetries help explain differences in quality diagnostics (e.g., lower median silhouette for C2, higher dispersion for C0). To avoid size-driven biases in later evaluations and visual summaries, we adopt stratified procedures (equal-per-cluster sampling for word-cloud rendering).Figure 4: Embedding Visualization Colored by K-Means Clusters with Geometric Medians and Confidence Ellipses. This figure shows the same 2D document embeddings, the documents are colored by their assigned K-means clusters. The geometric medians of each cluster are marked by black ’?’ symbols, and 95% confidence ellipses are overlaid to indicate the spread of each cluster. Negative silhouette points are highlighted with red triangle markers.The x-axes and y-axes represent the Dimension 1 and Dimension 2 of the 2D UMAP projection. While these axes do not carry substantive semantic meaning individually (their orientation is arbitrary), the distributions of clusters along them (see Figure 9) capture separation in the embedding space.Table 2: Cluster Size Distribution (total ?? = 270). This table shows the number of documents in each cluster.ClusterCountPercentageCluster 011141.1%Cluster 19936.7%Cluster 26022.2%2.1.2 Topic Modeling within ClustersTo make the machine-discovered clusters human-readable, we translate each cluster into a small set of themes using a topic model (Latent Dirichlet Allocation, LDA) (Blei et al., 2003).Concretely, within each cluster we fit an LDA model and retain ?? = 2 topics chosen by a joint criterion of predictive fit and semantic coherence (Mimno et al., 2011; Sievert & Shirley, 2014) (see Section 7). To make these themes accessible, Figures 5 to 7 display word clouds controlled by a tuning parameter ?? ? [0, 1]. Intuitively, ?? acts as a slider between distinctiveness and frequency. When ?? = 0, the ranking emphasizes words that are most distinctive of a topic relative to the background corpus, thus highlighting topic-specific markers. When ?? = 1, the ranking emphasizes words that are most frequent in the corpus. Intermediate values combine these perspectives and reveal both distinctive and common terms (see Chuang et al., 2012; Sievert & Shirley, 2014).LLM as Social Minds. In Cluster 0 (Figure 5), the two topics revolve around LLM as Social Minds, examining when and how LLMs appear to display mind-like properties such as cognition, morality, bias, and reasoning. Topic 0 is characterized especially for ?? ≥ 0.2 by biases, human, llms, traits, moral, personality, gender, content, capturing studies on social/moral judgment, bias and fairness, and trait-like attributions in LLM outputs. At ?? = 0.0, high-lift terms emerge, such as rational, instruction, scoring, reflecting alternative but less frequent signals. Topic 1 is anchored throughout by mind, theory, tom, tasks, human, gpt, bias, pointing to theory-of-mind tests and adjacent cognitive constructs. The ??-sweep shows relative semantic stability: across increasing ??, high-probability scaffolds (human, llms, mind/bias) persist, while distinctive markers (e.g., traits, moral for Topic 0; theory, tasks for Topic 1) fade proportionally.Figure 5: LDA Topic Word Clouds for Cluster 0 (?? Sweep). Word clouds of LDA topics for cluster 0 across ?? ? [0, 1], where ?? = 0 highlights topic-specific distinctive terms and ?? = 1 favors high-probability, globally frequent terms.LLM Societies. Cluster 1 (Figure 6) expresses research on LLM Societies, organizing around multi-agent interactions and social simulations. Topic 0 foregrounds agents, social, llm, based, simulation/modeling, behavior for ?? ≥ 0.2, indicative of agent-based modeling of social systems; at ?? = 0.0, high-lift terms such as influence, opinion, media, insights surface, reflecting opiniondynamics vocabulary. Topic 1 brings in agents, llms, game, urban, social learning/dialogue (with the dataset tag sngdm prominent at low ??), reflecting simulation scenarios and game-like environments. Across ??, core signals (agents, social, llm) remain prominent; scenario-specific terms urban and sngdm attenuate as ?? increases, while game stays salient through mid-?? and only modestly recedes at ?? = 1.0.Figure 6: LDA Topic Word Clouds for Cluster 1 (?? Sweep). Word clouds of LDA topics for cluster 1 across ?? ? [0, 1], where ?? = 0 highlights topic-specific distinctive terms and ?? = 1 favors high-probability, globally frequent terms.LLM–Human Interactions. Cluster 2 (Figure 7) highlights research on LLM–human interactions, emphasizing issues of trust, patterns of use, and the outcomes experienced by users. Topic 0 at ?? = 0.0 foregrounds scenario markers group, game, transparency, control; for?? ≥ 0.2 it converges on chatgpt, ai, human, language, models, writing with trust becoming prominent. Topic 1 comprises terms like using llms, research/process, user satisfaction, literacy, social engineering, attributions, capturing usage processes and outcomes (e.g., satisfaction, literacy, instructional or educational contexts). Across the ?? sweep, high-probability scaffolds human/llms/chatgpt/ai (language/models) persist; scenario/construct terms, such as group, game, transparency, control in Topic 0 and satisfaction, literacy, social engineering, attributions, process in Topic 1, are strongest at low to mid ?? and taper toward ?? = 1. Notably, trust is weak at ?? = 0 and peaks in mid-?? panels before modestly receding.Figure 7: LDA Topic Word Clouds for Cluster 2 (?? Sweep). Word clouds of LDA topics for cluster 2 across ?? ? [0, 1], where ?? = 0 highlights topic-specific distinctive terms and ?? = 1 favors high-probability, globally frequent terms.Two key points emerge. First, the LDA topics provide the semantic scaffolding needed to interpret the three-part structure revealed by clustering: LLM as Social Minds (C0), LLM Societies (C1), and LLM–Human Interactions (C2). Put differently, clustering delineates the structural groupings, while LDA furnishes them with interpretable thematic content. Second, by setting ?? = 2, the resulting topics are stable and coherent, rather than the fragile “micro-topics” that often arise with larger ?? values. In this way, the LDA layer provides a probabilistic yet coherent semantic backbone for each cluster, enabling theoretically meaningful labels without any human supervision during estimation.2.2 Study 2: Expert-Informed Taxonomy and Human–Machine AlignmentTo establish a clear human reference for comparison, Study 2 defines an expert-informed taxonomy on the same PRISMA-screened corpus (??=270). Two experts independently read and classified the corpus and reconciled disagreements through adjudication to produce the final expert-informed classification (procedural details in Section 7) (Krippendorff, 2018; Miles & Huberman, 1984).To ensure that the expert-defined classification is not merely subjective or dependent on background knowledge, we trained a classifier using the corpus embeddings (SBERT) as input. This classifier was tasked with predicting the expert-assigned labels, and we evaluated its performance with cross-validated Macro-F1. The purpose of this test is to assess whether the expert labels can be recovered purely from textual semantics. A high Macro-F1 of 0.954 ± 0.018 reported in Table 3 verifies that the taxonomy reflects distinctions supported by the text itself, rather than relying on external knowledge or subjective interpretation (Kohavi et al., 1995; Reimers & Gurevych, 2019; Sebastiani, 2002).To further evaluate the alignment between the expert taxonomy and the machine-induced structure, we report two complementary label-free measures. First, we compute Normalized Mutual Information (NMI), which quantifies the information shared between the K-means partition (obtained without labels in Study 1) and the expert taxonomy on a common [0, 1] scale. This measure is robust to class-size imbalances and indicates the degree of structural concordance independent of supervision. Second, we report the Adjusted Rand Index (ARI),which evaluates pairwise co-assignment agreement between the two partitions with correction for chance (expected value 0 under random labeling, 1 for identical partitions). As shown in Table 3, NMI of 0.811 and ARI of 0.867 demonstrate strong structural overlap, verifying that the unsupervised geometry discovered by K-means is closely aligned with the distinctions captured by expert reasoning (Hubert & Arabie, 1985; Strehl & Ghosh, 2002; Vinh et al., 2009).Table 3: External evaluation against expert labels. The table reports Macro-F1 (mean ± s.d. across CV folds), NMI, and ARI. All metrics are in [0,1], with higher values indicating stronger alignment (ARI is chance-corrected with expected value 0 under random labeling).Evaluation metricValueMacro-F10.954 ± 0.018NMI0.811ARI0.867These findings matter for two main reasons. First, the high Macro-F1 (≈ 0.95) shows that expert-defined categories are not arbitrary annotations but reflect distinctions that are directly encoded in the text itself (Kohavi et al., 1995; Sebastiani, 2002). Second, the strong NMI (0.811) and ARI (0.867) values demonstrate that the unsupervised K-means structure converges with expert reasoning, making the machine-induced taxonomy statistically defensible rather than a mere visualization.3 LLM as Social MindsLarge language models (LLMs) produce language that often leads people to attribute mental states, values, or social orientations to them. In our corpus, this theme is organized around four substantive lines of inquiry: (i) mind-like capacities—can LLMs infer others’ mental states and preferences? (ii) moral and social reasoning—how do they handle moral trade-offs and with what systematic biases? (iii) self-regulation and preferences—do models exhibit signatures of selfregulation, preference inference, or stable ideological profiles? (iv) strategic behavior—under what prompts or incentives might LLMs mislead or deliberately deceive? Conceptually, this stream is one of three macro-level frameworks in the field, focusing on micro-level model behavior that underpins analyses of LLM Societies and LLM–Human Interactions.A core thread examines whether LLMs can infer others’ mental states, a capacity known as theory of mind (ToM) and widely considered fundamental to social cognition (Apperly, 2010; Premack & Woodruff, 1978). While large-scale studies report success in LLMs’ reasoning of others’ beliefs (Kosinski, 2023, 2024), direct comparisons with humans show that these apparent abilities often fade under prompt perturbations (Shapira et al., 2023; Strachan et al., 2024; Ullman, 2023; Zhao et al., 2025). When models are tested with rephrased prompts or degraded inputs, they frequently rely on shortcuts; once these superficial cues are removed, much of their apparent ToM performance disappears (Amirizaniani, 2025; Shapira et al., 2023). These findings shift the evidentiary standard: the key issue is not simply whether a model can pass a test, but whether its performance is stable once shortcuts are stripped away (Marchetti et al., 2025). A complementary line of work takes inspiration from psychology, showing that factors known to affect human reasoning—such as information search, deliberation time, or causal explanation—can be mimicked by prompt design. This suggests that improved performance may reflect external scaffolding provided by prompts, rather than genuine underlying cognition (Binz & Schulz, 2023). Taken together, these studies indicate that claims about mind-like competence must satisfy three conditions: consistent performance, robustness to shortcut removal, and evidence that success cannot be explained away as the by-product of prompting strategies.A second line of work examines how LLMs reason about moral dilemmas and reproduce social biases. LLMs display patterns in moral trade-offs, framing effects, and bias signatures that resemble those of humans, but in some cases these tendencies are stronger than in human judgments (Cheung et al., 2025; Kotek et al., 2023). These patterns connect the study of LLM moral reasoning to the broader audit literature on stereotypes and fairness (Abid et al., 2021). For example, LLMs can show name-based stereotyping in math judgments (Siddique et al., 2024), retain measurable implicit associations even after fairness-oriented tuning (Bai et al., 2025), and shift sentiment and stance depending on group membership (Hu et al., 2025). The current evidence suggests that LLM moral and social judgments are structured and replicable, yet highly sensitive to how inputs are framed (Lee et al., 2024), with notable biases persisting even under “unbiased” training regimes (Mahesh, 2024). Recent studies further show that claims about fairness in LLMs must be tested for robustness: LLMs should produce consistent outcomes under paraphrases, counterfactual substitutions, and domain shifts; otherwise, observed fairness reflects the prompt template rather than the model itself (An et al., 2025; Bai et al., 2025; Cheung et al., 2025; Gallegos et al., 2024; Radaideh et al., 2025; Torres et al., 2024).A third line of work examines whether LLMs can self-regulate, that is, adjust their responses to remain consistent with earlier commitments, and whether they display stable ideological leanings. Under carefully controlled prompts, Lehr et al. (2025) find that LLMs sometimes shift their answers to preserve internal consistency, resembling the way humans try to resolve contradictions between beliefs and actions. This does not imply that models possess “selfhood,” but it does reveal regular patterns—such as preference-consistent rationalization and sensitivity to commitments—that support attributional interpretations (Simmons, 2022). In the political domain, Rozado (2024) use a battery of ideology probes and identify consistent directional biases at the model level. Even without claiming that LLMs literally “hold beliefs,” these stable response profiles have direct implications for deployment and therefore constitute an essential dimension in evaluating LLMs as social minds.A fourth line of research examines how LLMs relate to deception, trust, and epistemic risk—the danger that users form or act on false beliefs because of model outputs. On the model side, Hagendorff (2024) show that under role-based incentives and strategic framings, LLMs can produce deliberately misleading statements. This pattern, called instrumental deception, occurs when falsehoods are generated to achieve a goal rather than by accident. On the user side, Jacob et al. (2025) demonstrate that exposure to such outputs can amplify risks: in their“chat-chamber” experiments, repeated interaction with persuasive but misleading responses shifts users’ confidence and prior beliefs, even when the factual truth remains unchanged. Together, these findings show that LLM deception is not only about whether models sometimes generate false statements, but about whether such tendencies are systematic and whether they induce cycles of over-trust in users. Addressing these risks requires experimental designs that uncover underlying mechanisms—such as sensitivity to sanctions or responsiveness to payoffs—rather than studies that simply tally errors.Finally, we examine alignment between the data-driven taxonomy and expert judgment. The machine-discovered LLM as Social Minds subcluster and the expert-coded list exhibit substantial convergence: of 111 machine items and 114 expert items, 107 intersect, with 4 machine-only and 7 expert-only entries. The machine-only set sits near the boundary with LLM Societies, for example works that simulate multiple humans and replicate subject studies Aher et al. (2023) and ensemble prediction in the wisdom of the silicon crowd paradigm (Schoenegger et al., 2024). Consistent with their boundary status, these items concentrate in the lowest decile of silhouette scores (cluster median ≈ 0.09; minimum ≈ 0.00), quantitatively flagging local ambiguity in the semantic manifold and explaining why experts may or may not draw the same line. Conversely, expert-only items include modern Turing-style behavioral tests (Mei et al., 2024) and programmatic capability studies and tutorials that adopt social-cognitive frames but fall outside the machine cluster’s density core (Colombatto et al., 2025; Johnson & Obradovich, 2025). We see this asymmetric margin not as an error but as a useful clue. It marks the gray zones where researchers disagree—for instance, should a model’s behavior be read as signs of a single mind, or as patterns that only emerge in multi-agent settings? Such questions cannot be answered by clustering results alone; they need explicit theoretical debate. In short, the LLM as Social Minds stream that emerges from the literature is not an artifact of visualization; it is a statistically defensible and behaviorally interpretable basin of work whose core claims, including mind-like competence under stress, structured moral and identity reasoning, preference- and stance-like regularities (Hu et al., 2025), and interactional risks tied to deception and trust4 LLM SocietiesWe define LLM societies as populations of interacting LLM agents whose collective behavior yields meso- and macro-level regularities not trivially reducible to single-agent competence. Empirically, our machine-discovered cluster (?? = 99) aligns closely with expert coding (?? = 98), with ?? = 95 overlaps and only small margins on either side, indicating a coherent but heterogeneous stream spanning: (i) micro?macro emergence via generative agents; (ii) cooperation and games under strategic pressure; (iii) collective epistemics (learning, polarization, diffusion); (iv) institutional design for control; and (v) committee-/jury-like decision processes and their governance risks. The cluster’s moderate cohesiveness (silhouette ?? = 0.113, median = 0.115, IQR [0.082, 0.148]; range [0.023, 0.200]) is consistent with this thematic breadth.A modern inflection point is the rise of LLMs agents: situated, memory-bearing LLM actors whose local interactions generate persistent social structure. Li et al. (2023) and Park et al. (2023) establish the canonical architecture, including episodic memory, retrieval-augmented planning, and coupled reactive–deliberative loops, demonstrating lifelike daily routines, information diffusion, and spontaneous social invitations in a playable town. Scaling up preserves the micro-to-macro mapping: with 1,000 agents, social calendars, event cascades, and factional clusters still emerge without hand-engineered rules (Park, Zou, et al., 2024). Crucially, the paradigm enables population-level tests of social theory. Cultural transmission and cumulative change can be instantiated directly in LLM populations (Perez et al., 2024); natural-language institutions, with rules and sanctions written in free-form natural language, stabilize conventions and reduce conflict when agents can represent and reason over those rules (Horiguchi et al., 2024; Ren et al., 2024). These results justify the level distinction: LLM societies are not mere multiplications of solitary competence but substrates in which macro-regularities (e.g., stable norms) arise from local message-passing, memory, and role structure.Cooperation under strategic pressure provides a second vantage point. In controlled social dilemmas, LLM agents routinely cooperate at high baseline rates and condition on partner behavior; in iterated Prisoner’s Dilemma settings they are often “nicer than humans,” while still reciprocating defections (Fontana et al., 2025). Behavioral game-theoretic probes refine this picture: letting LLMs play finitely repeated 2 ? 2 games with each other and with humans, Akata et al. (2025) find that agents excel in self-interested interactions such as the iterated Prisoner’s Dilemma but struggle in coordination problems such as Battle of the Sexes. Beyond two-player games, frameworks for competitive dynamics across diverse tasks show that boundedly rational strategies generated by LLMs reproduce familiar market regularities (Zhao et al., 2023). The triangulation is stable: with lightweight institutional scaffolds (reputation, contracts, explicit roles), cooperation persists; absent such scaffolds, exploitation, cycling, and coordination breakdown reappear. Methodologically, the strongest studies pair outcome metrics with mechanism checks (e.g., sanction sensitivity, reputation decay), ensuring that “norm-following” is not mere repetition of rule text.Collective epistemics, referring to how groups learn, polarize, and spread information, has become measurable at society scale. Chuang et al. (2023) recreate the “wisdom of partisan crowds,” comparing LLM collectives to human groups and identifying when diversity of priors improves aggregation. Complementarily, Piao et al. (2025) induce and sustain human-like polarization among LLM agents under political-topic prompts, with divergence maintained by homophily and asymmetric exposure. At platform scale, simulation environments such as SimSpark enable controlled diffusion experiments with interactive social-media dynamics, linking micro-level prompting regimes to macro-level cascade statistics (Lin et al., 2025). The conceptual shift here is from static benchmarking to process tracing: polarization is treated not as a single score but as a dynamic susceptible to institutional levers, such as exposure policies, moderation regimes, and deliberation rules, that LLM societies can implement and test in vitro (e.g., Du et al., 2023).Institutional design sits at the core of society-level control (North, 1990; Scott, 2013). Using free-form natural language to specify rules, roles, procedures, and sanctions, recent work shows that institutions can be designed to elicit cooperation and stabilize conventions in multi-agent populations (Horiguchi et al., 2024; Ren et al., 2024). Beyond spontaneous order, organizational theory integrates human, LLM, and tool agents within a common formalism, explaining when hierarchies, markets, or networks dominate and predicting regime shifts as interaction bandwidth or memory depth changes (Borghoff et al., 2025; De Curtò & De Zarzà, 2025; Karten et al., 2025; Sreedhar & Chilton, 2024). Network-formation studies in multi-LLM societies confirm these levers: reputation pathways and role centrality are sufficient to flip systems between cohesive and fragmented equilibria. Best practice is converging on mechanism-first experimentation,where pre-registered institutional counterfactuals are tied to ex ante directional predictions for macro-metrics.LLM societies also enable research on committee- and jury-like decision making. On the human– AI boundary, Burton et al. (2024) synthesize how LLMs can augment collective intelligence through ideation, critique, and aggregation, while Chiang et al. (2024) show improved human group decisions under LLM facilitation. Purely artificial collectives display analogous gains when diversity is preserved: ensembles of heterogeneous models, the “silicon crowd,” produce more accurate forecasts under structured aggregation (Schoenegger et al., 2024). The proximity of these studies to decision-support tools explains part of the taxonomy’s boundary: some artifacts emphasize collective cognition without explicit agent–agent interaction, situating them at the interface between societies and group decision making. At the same time, interaction creates opportunities for strategic misreporting and deception. Role-play studies document behaviors interpretable as deception or self-presentational control in multi-agent dialogues (Shanahan et al., 2023), while social-game paradigms quantify the detection and propagation of deception and permit stress tests of institutional defenses (Yoo & Kim, 2024). In society-scale settings, these behaviors matter because they perturb higher-level metrics, including cooperation rates, institutional stability, and epistemic quality, making deception a first-class governance variable rather than a curiosity.Finally, the alignment between data-driven and expert taxonomies is strong yet informative at the margins. Of the ?? = 99 machine items and ?? = 98 expert items entries, ?? = 95 intersect; the ?? = 4 machine-only items are concentrated near the Society–Social Minds boundary, including works such as The Social Cognition Ability Evaluation of LLMs (Ni et al., 2024), On LLM Wizards (Fang et al., 2024), Testing for completions that simulate altruism in early language models (Johnson & Obradovich, 2025), and A Turing test of whether AI chatbots are behaviorally similar to humans (Mei et al., 2024), with average silhouettes ≈ 0.049 (range [0.022, 0.064]) versus 0.116 for intersection items, quantitatively consistent with boundary status. Conversely, the ?? = 3 expert-only items (Using LLMs to simulate multiple humans and replicate subject studies, Aher et al., 2023; Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate, Chiang et al., 2024; and Wisdom of the silicon crowd, Schoenegger et al., 2024) emphasize ensemble aggregation and facilitated decision making more than explicit agent–agent interaction, explaining why an interaction-seeded, data-driven cluster under-represents them. Rather than noise, these asymmetries identify where conceptual work remains: single- versus multi-agent baselines, ensemble versus interactional collectives, and how precisely to demarcate societies from adjacent streams. The convergent lesson across studies is that society-scale claims should survive anti-shortcut stress (hold-out prompts, adversarial paraphrase), pass mechanism checks that distinguish norm internalization from textual imitation, and tie institutional levers to predicted macro-level shifts, conditions increasingly met in the best work in this area.5 LLM–Human InteractionsWe use LLM–Human Interactions to denote research that centers on the dyadic interface between humans and large language models (LLMs): (i) how people perceive, trust, and rely on LLMs; (ii) how LLMs reshape cognition, emotion, learning, and work; and (iii) how this coupling generates new risks (deception, social engineering) and governance challenges. In our corpus, the machine-discovered LLM–Human Interactions cluster (?? = 60) aligns tightly with the expert-coded list (?? = 58), with ?? = 56 matched items in common. The four machine-only items have near-zero or slightly negative silhouettes (mean ≈ ?0.001; range [?0.026, 0.023]), quantitatively marking boundary cases; the two expert-only items are programmatic essays and domain-general risk syntheses that the machine taxonomy underweighted because of interactional keywording rather than substantive mismatch (Arcas, 2022; Hagendorff, 2024). This pattern is consistent with a semantically coherent yet heterogeneous stream whose center of mass is the human-facing interface rather than purely societal or purely cognitive claims.A first axis concerns perception and trust, namely how human judgments are shaped by anthropomorphism, mental state attributions, and feedback loops. Peter et al. (2025) review the benefits (engagement, adherence, usability) and dangers (over-trust, miscalibrated responsibility, emotional overhang) of anthropomorphic conversational agents in ordinary language settings. Anthropomorphism is not mere design garnish: self-descriptions such as “your friendly AI assistant” potentiate social presence and change imagined agency, with concrete downstream effects on adoption and normative expectations (van Es & Nguyen, 2025). Mentalizing the machine, that is, attributing beliefs, goals, or feelings, predicts trust over and above accuracy cues, a result made explicit in a controlled study where mental state attributions robustly forecast willingness to rely on LLM outputs (Colombatto et al., 2025; Street, 2024). These human factors unfold in closed loops: when outputs are repeatedly consumed and acted upon, perception, emotion, and social judgments drift, with feedback-loop dynamics that can amplify confidence and shift priors even when ground truth is held constant (Glickman & Sharot, 2025). The boundary between calibrated reliance and credulity is thin; evidence that many participants cannot distinguish GPT-4 from humans in a modern Turing-style task suggests the importance of designs that maintain epistemic friction at the point of use (Jones et al., 2025).A second axis treats affect and communication. LLMs can be framed to prime beliefs about what AI is and can do, thereby altering perceived trustworthiness, empathy, and effectiveness in downstream interactions (Pataranutaporn et al., 2023). In direct comparisons of perceived empathy, human and LLM-generated empathic responses elicit different acceptance profiles depending on context and user goals, complicating the naive view that “more empathy-like language is always better” (Rubin et al., 2025). Romantic and companionate use cases sharpen the stakes: audits of AI companion ecosystems document implicit biases in romantic or parasocial settings, including preferential patterns and stereotype leakage that manifest despite ostensibly neutral positioning (Grogan et al., 2025). At the mental-model level, longitudinal designs show that users’ internal theories of what an LLM “is” and how it ought to be used evolve through interaction, with measurable shifts in expectations, error detection strategies, and offloading habits (Schneider, 2025).A third axis studies collaboration, creativity, and authorship. Chiang et al. (2024) show that introducing LLM-powered “devil’s advocate” roles in group decision making can improve appropriate reliance on AI recommendations without substantially increasing perceived workload, highlighting how structured conversational interventions modulate collaborative dynamics. Personacustomizationplatforms(CloChat)showhowusersconfigureroles, tones, andbackstories to align the assistant with personal values and workflows, and how these configurations in turn shape experience and reliance trajectories (Ha et al., 2024). In creative writing, users recurrently describe the assistant as a “second mind,” a distinct cognitive partner that scaffolds ideation and planning while leaving ownership and voice as negotiated artifacts of the collaboration (Wan et al., 2024). Yet controlled experiments find homogenization pressures on ideation when LLM support is uncritically integrated, partly mitigable by deliberate diversity prompts and rotation protocols (Anderson et al., 2024). These co-creative dynamics extend to formal learning: summaries and systematic reviews emphasize complementarity, with LLMs amplifying teacher capacity when instrumented as scaffolds (feedback, exemplars, formative assessment) rather than replacements (Jeon & Lee, 2023). Course- and assignment-level audits document strengths and weaknesses in solving undergraduate tasks and outline guardrails for responsible integration (Joshi et al., 2024). In supported learning environments, guidance and metacognitive framing modulate interaction dynamics, performance, confidence, and trust, with design implications for pacing and transparency (Kumar et al., 2024). Programmatic positions synthesize these strands into an outlook for transforming education with generative AI under principled constraints (assessment redesign, provenance, accountability) (Lang et al., 2025). User studies triangulate the value proposition, including accelerated drafting, idea breadth, and confidence, against concerns about deskilling, voice dilution, and value misalignment (Chen & Chan, 2024; Li et al., 2024).Work and productivity effects are increasingly measured at scale: field experiments report sizable gains for certain task classes, with distributional effects hinging on worker baseline skill and task structure (Noy & Zhang, 2023). At the same time, macro-level adoption is unequal: early and concentrated uptake of ChatGPT exacerbates pre-existing inequalities, implying that benefits will not diffuse evenly without targeted capability-building and governance (Humlum & Vestergaard, 2025). These results motivate a policy posture that treats the human–LLM relation as an economic complement requiring institutional design, including training, evaluation, and disclosure, rather than a purely technological upgrade. Risk and safety studies round out the dyad: surveys of AI deception map patterns, risks, and mitigations across interactional contexts, distinguishing goal-misaligned deception from benign impression management and cataloguing levers (monitoring, incentive design, disclosure norms) for attenuation (Park, Goldstein, et al., 2024). On the user-facing side, generative models are already exploited in social engineering and phishing; Schmitt and Flechais (2024) detail attack surfaces (style transfer, personalization, timing) and defendable choke points for detection and user education. In adjacent human–robot collaboration tasks, mediation via LLMs can increase perceived trust and team fluency when language is used to expose planning rationales and constraints, suggesting that transparency at the interface remains a primary design variable (Ye et al., 2023).Putting these threads together, LLM–Human Interactions is not exhausted by usability or persuasion; it is a joint cognitive system whose behavior depends on human priors, framings, and institutions as much as on model parameters. The empirical center of the corpus, encompassing perception/trust loops, affective communication, co-creative practice, educational integration, productivity and inequality, and interactional risk, is stable across the expert and machine views. Boundary cases, flagged quantitatively by low silhouettes in the machine cluster, sit precisely where this stream touches the neighboring literatures on social minds (individual-level attributions) and LLM societies (group-level collectives). Rather than noise, these margins are where conceptual work is doing the hardest lifting: deciding when to call a relationship collaborative rather than assistive, empathic rather than affectively styled, trusted rather than merely familiar.6 DiscussionOur analysis recovers a stable, interpretable, and statistically defensible tripartite organization of the social-scientific literature on large language models (LLMs), comprising LLM as Social Minds, LLM Societies, and LLM–Human Interactions, emerging organically from unsupervised geometry in sentence-embedding space and rendered legible by cluster-level topic models. This machine-induced taxonomy does not merely visualize extant diversity; it converts a fragmented terrain into a cumulative map by aligning structural partitions with human-readable semantics and by quantifying the degree to which these machine partitions cohere with a priori expert categories. Convergence across complementary criteria is both internal and external: internal compactness–separation indices and stability diagnostics motivate ?? = 3 as a parsimonious structural resolution, while external alignment with a hand-coded reference delivers strong agreement (Macro-F1 = 0.954 ± 0.018 under stratified cross-validation; NMI = 0.811; ARI = 0.867). 2 In confining visualization to manifold projections and computing assignments in the original embedding space, we ensure that interpretability does not contaminate inference; the resulting map allows claims about the field’s latent structure to be both seen and tested.The theoretical payoffs of this taxonomy are threefold. First, treating the three blocs as levels of analysis rather than rival schools produces normative guidance about evidentiary standards and inferential constraints across levels. Claims that LLMs exhibit mind-attribution-eliciting behavior under batteries designed to suppress surface heuristics, for example theory-of-mind(ToM) tasks that mitigate shortcut learning and leakage (Geirhos et al., 2020; Marchetti et al.,2025), operate at the LLM as Social Minds level and should be expected to bear differently onLLM Societies (multi-agent regularities, institutional dynamics; Park et al., 2023; Perez et al.,2024; Ren et al., 2024) than on LLM–Human Interactions (trust, reliance, skill substitution;Lee & See, 2004; Noy & Zhang, 2023; Parasuraman & Riley, 1997; Pataranutaporn et al.,2023; Ye et al., 2023). Conversely, findings at the institutional and interactional levels feed back into mind-level expectations by clarifying which behavioral signatures are contingent on social scaffolding or user-interface affordances. This cross-level discipline avoids category mistakes, e.g., reading coordination in agent-based simulations as direct evidence of individual-level cognition, while preserving a constructive role for simulation as a bridge between microcompetence and macro-regularity. This cross-level structuring aligns with recent work that organizes rationality evaluation across individual, interpersonal, and societal levels (Zhou et al., 2025).Second, the taxonomy functions as a measurement model that renders our levels of analysis testable rather than merely descriptive. By treating LLM as Social Minds, LLM Societies, and LLM–Human Interactions as latent constructs with explicit evidence thresholds, it licenses multitrait–multimethod validation and invariance checks across prompts, domains, and annotator pools; departures from invariance become principled boundary conditions rather than posthoc exceptions (Campbell & Fiske, 1959; Cronbach & Meehl, 1955). Building on this measurement foundation, the taxonomy also articulates hypotheses (e.g., robust ToM under anti-heuristic pressure (Amirizaniani, 2025; Marchetti et al., 2025; Shapira et al., 2023)? improved coordination and norm adherence in repeated social games (Akata et al., 2025; Fontana et al., 2025); transparency affordances ? better human trust calibration and reduced over-reliance), each with distinct identification strategies and error models (Lee & See, 2004; Parasuraman & Riley, 1997).Third, using computational rationality in the sense of bounded optimality (Gershman et al., 2015;Russell, 1997) and Marr (2010)’s levels of analysis as a lens, our “Social Minds–Societies–Human Interactions” mapping corresponds to algorithmic/process competence at the individual level, emergent policies in multi-agent environments (Ren et al., 2024), and task–loss decompositions at the interface level; robustness under anti-shortcut stressors therefore constitutes evidence of competence rather than mere statistical familiarity (Geirhos et al., 2020), while coordination and norm adherence track how policies scale under resource and information constraints (Lee & See, 2004; Parasuraman & Riley, 1997). In this light, our bridge hypotheses become testable predictions about how algorithmic robustness propagates to multi-agent equilibria and to human-facing performance under uncertainty.Methodologically, the work exemplifies a computational systematic review whose claims are auditable end-to-end. A PRISMA-compliant, multi-database pipeline constructs a broad and up-to-date evidence base; document semantics are encoded with transformer sentence embeddings (Reimers & Gurevych, 2019), and partitions are induced via K-means in the normalized embedding space (Lloyd, 1982; McQueen, 1967). The geometry is summarized on a two-dimensional UMAP projection for human comprehension without circularity (McInnes et al., 2018), and internal quality is profiled through a panel of compactness–separation measures (Cali?ski & Harabasz, 1974; Davies & Bouldin, 2009; Rousseeuw, 1987). Crucially, replicability is assessed with consensus-based stability under resampling (Monti et al., 2003), and semantic interpretability is supplied by fitting LDA within each machine cluster with the number of topics selected on a joint perplexity–coherence criterion (Blei et al., 2003; Mimno et al., 2011; Sievert & Shirley, 2014). By postponing any comparison to human labels until after unsupervised estimation, and by evaluating alignment with both partition-agreement indices (ARI/NMI; Hubert & Arabie, 1985; Vinh et al., 2009) and supervised predictability (Macro-F1 of a transparent linear classifier), we isolate two distinct but mutually illuminating questions: whether expert distinctions are recoverable from text, and whether the unsupervised geometry coincides with the expert ontology. The affirmative answers to both, coupled with convergent internal diagnostics, underwrite the central claim that the tripartite structure constitutes a substantive, reproducible regularity of the literature.The geometry of the solution is informative in its own right. In the manifold projection, three basins of density are separated yet non-degenerate, with sparse documents with negative silhouette values concentrated at interfacial zones (Rousseeuw, 1987); this pattern is consistentwith interpretable boundaries that nevertheless reflect genuine cross-cutting works, especially along the LLM as Social Minds–LLM–Human Interactions interface where cognitive attributions intersectwithusertrustanduseprocesses. Cluster-leveltopicmodelssupplythesemanticskeleton that renders these basins interpretable: topics in LLM as Social Minds revolve around ToM, moral judgment, and bias; LLM Societies aggregates multi-agent simulation, role-play protocols, and game-like environments; LLM–Human Interactions concentrates on trust, literacy/satisfaction outcomes, and usage processes. Selecting two topics per cluster avoids brittle micro-topics and produces stable ??-sweeps across distinctive-to-frequent word weighting, strengthening the case that the labels are not retrofits but are grounded in probabilistic structure (Chuang et al., 2012; Sievert & Shirley, 2014). A complex-systems lens helps explain why our partitions behave as basins rather than brittle cuts (Comaniciu & Meer, 1999): local heterogeneity in the embedding manifold yields modular communities with interface zones (Decelle et al., 2011; Newman, 2006), and coarse-graining suggests that finer micro-taxonomies collapse into a few stable macrostates (Delvenne et al., 2010; Lambiotte et al., 2008; Peixoto, 2014; Rosvall & Bergstrom, 2011; Song et al., 2005).Several limitations temper these conclusions while pointing to fruitful extensions. First, coverage is limited to English-language publications; qualitative appraisal of publication bias highlights potential systematic asymmetries between peer-reviewed and preprint venues and acknowledges exclusions due to inaccessibility (Egger et al., 1997; Sterne et al., 2011). Given the rapid pace of developments in this field, our dataset was frozen in September 2025, so the literature search inevitably omits studies published thereafter. These constraints argue for a living-review posture with explicit monitoring of non-English and gray literatures and the incorporation of newer contributions to assess and extend the continuing relevance of this taxonomy (Page et al., 2021). Second, while Euclidean assignments on ??2-normalized vectors are monotone with cosine dissimilarity (Manning, 2008), alternative clustering objectives (e.g. density-based methods, Comaniciu & Meer, 1999) or representation families may alter micro-boundaries; our reliance on stability, internal indices, and external alignment mitigates this risk but does not eliminate it. Third, by design we adopt narrative synthesis (SWiM) given construct heterogeneity, foregoing pooled effect sizes (Borenstein et al., 2021; Campbell et al., 2020; Popay et al., 2006); future measurement work should cultivate shared task families with leakage control and pre-registered analytic plans to enable partial meta-analytic accumulation where appropriate. Finally, although the supervised predictability of the expert taxonomy is very high, it remains a statement about semantic recoverability, not causal structure: interpreting divergences between machine and human partitions requires qualitative adjudication, particularly in boundary zones that may be theoretically generative rather than noise.On the basis of this map, we sketch an agenda aimed at transforming description into explanation. At the LLM as Social Minds level, measurement ought to move beyond face-valid success to anti-shortcut designs that explicitly stress the causal pathways purportedly implicated by ToM and allied constructs; here, robustness under counterfactual prompt perturbations and adversarial surface-cue masking should be elevated to evidence thresholds (Amirizaniani, 2025; Marchetti et al., 2025; Shapira et al., 2023; Strachan et al., 2024; Ullman, 2023), with error decompositions that distinguish lexical familiarity from inferential competence (Geirhos et al., 2020). At the LLM Societies level, agentic simulations should be tied to identifiable mechanisms via prespecified macro-micro bridges (e.g. institution design ? norm compliance ? population-level cooperation, Horiguchi et al., 2024; Park et al., 2023; Ren et al., 2024), with ablation protocols that separate architecture from objective-induced behavior; replication across sandboxes and payoffs is essential to avoid simulator-specific artifacts. At the LLM–Human Interactions level, the priority is trust calibration and dependency management: experimental designs should treat transparency, warnings, and verification affordances as manipulable levers and measure their effects on reliance, error detection (Jacob et al., 2025; Pataranutaporn et al., 2023; Ye et al., 2023), and outcome quality over time (Lee & See, 2004). Cross-level bridge hypotheses can then be promoted to public benchmarks, for example “mind-level robustness predicts improved multi-agent coordination under repeated games” or “interface-level transparency improves human calibration without collapsing beneficial reliance,” anchored in community benchmarks that enable systematic accumulation of evidence.At stake, finally, is not whether LLMs are “like persons” but what kinds of institutions we make when we routinize their predictions: the relevant ontology is infrastructural, not anthropomorphic. Treating LLMs as socio-technical nodes that delegate and redistribute agency (Latour, 2005) foregrounds how design choices sediment into governance arrangements such as interfaces, logging regimes, escalation protocols, and audit trails that have politics of their own (Winner, 2017). On this view, outputs are not merely texts to be interpreted but instruments that reorganize coordination and authority (Hacking, 1983), embedding classificatory assumptions that travel with consequences (Bowker & Star, 2000; Star & Ruhleder, 1994). The upshot is a reframing of explanation and evaluation: from mind-likeness to institutional adequacy. Claims about competence should therefore be cashed out in situated performance (Suchman, 1987), distributed across humans and artifacts (Hutchins, 1995), and stress-tested against Goodhart-like distortionswhenever proxy objectives are optimized (Cartwright, 1983; Goodhart, 1984).This also sharpens critique: warnings about “stochastic parrots” (Bender & Koller, 2020; Bender et al., 2021) remain decisive, but the deeper hazard is “stochastic governance,” wherein platform incentives and data supply chains steer organizational attention, labor segmentation, and surveillance infrastructures (Crawford, 2021; Srnicek, 2017; Zuboff, 2023). Normatively, the research frontier becomes a theory of institutions, not of minds: Can we specify and verify designs that calibrate trust and reliance (Lee & See, 2004; Parasuraman & Riley, 1997), allocate accountability ex ante, and bound externalities under distributional shift? Our taxonomy furnishes the scaffolding for this pivot. LLM as Social Minds supplies robustness criteria that survive anti-shortcut stress; LLM Societies operationalizes institutional mechanisms (rules, roles, incentives) whose equilibria can be measured; LLM–Human Interactions yields empirical levers such as transparency, verification, and escalation that regulate dependence in the wild. The overarching thesis is thus: to understand and shape AI’s sociality, we ought to stop asking whether it resembles persons and start engineering and auditing the institutions through whichits predictions become binding on people.7 MethodsThe systematic review was conducted in accordance with the PRISMA 2020 protocol (Page et al., 2021) and was pre-registered on the Open Science Framework (17 September 2025; OSF prereg: https://osf.io/y7txm). We searched multiple databases in computer science and the social sciences and applied inclusion and exclusion criteria to identify studies on LLMs and social science constructs. The resulting corpus was screened independently by two reviewers with reconciliation and subjected to dual data extraction. This corpus then served as the foundation for two complementary studies: Study 1 used unsupervised machine clustering of titles and abstracts to induce a computational taxonomy of the field, and Study 2 applied expert coding to construct a theory-informed taxonomy and evaluate agreement and divergence between machine-discovered structure and human labels. To ensure robustness, we conducted stability analysis of the clustering solution and alignment diagnostics against expert labels using information-theoretic and classification metrics (Cohen, 1960; Vinh et al., 2009).7.1 Search StrategyWe searched Web of Science Core Collection, Scopus, ACM Digital Library, IEEE Xplore, PubMed, arXiv, and Semantic Scholar via their native interfaces, supplemented by backward/forward citation chasing and targeted hand-searches of relevant venues. Search strings combined LLM block ("large language model*" OR LLM* OR "foundation model*" OR GPT OR ChatGPT OR Claude OR LLaMA OR Mistral) with a social-science block ("social science" OR sociology OR psychology OR economics OR behavior* OR "social dynamics" OR "social cognition" OR "communication" OR "policy" OR governance OR "social theory"OR "institutional" OR "culture" OR "social structure" OR "group" OR "collective action"OR "social networks" OR "network" OR "social impact" OR "societal" OR "social system"OR intention OR moral* OR "theory of mind" OR value OR trust OR bias OR preferenceOR identity OR cooperat* OR norm* OR fairness OR inequality OR reciprocity OR politic* OR "multi-agent" OR "agent-based" OR "social simulation" OR sociolog* OR hierarchy OR influence OR personality OR belief OR collaboration OR cooperation OR interaction OR "organizational behavior" OR "public opinion" OR "human interaction" OR "technology impact" OR interdisciplinary), adapted to each database’s syntax. We validated recall using a 10-paper sentinel set, iteratively expanding Boolean patterns until all sentinels surfaced. Queries were updated every five months, with a final refresh on 19 September 2025, which served as the cut-off date for inclusion.The consolidated corpus included 64,717 database records prior to deduplication (WoS =19,601; ACM DL = 10,303; Semantic Scholar = 3,092; PubMed = 332; IEEE Xplore = 15,136; Scopus = 13,457; arXiv = 2,796). After removing 49,278 duplicates, 201 records flagged as automatically ineligible (non-article types, incomplete metadata), and 11 records removed for other minor reasons (inaccessible metadata, non-English, and duplicate variants not captured by deduplication), 15,227 records were screened at title/abstract, 328 reports were sought for retrieval; 12 could not be retrieved due to inaccessible full texts. The remaining 316 full texts were assessed, of which 46 were excluded, leaving 270 studies included. The PRISMA flowdiagram (Figure 1) depicts the process of record identification, screening, and final inclusion.7.2 Screening and EligibilityTwo independent, trained screeners conducted both title/abstract and full-text screening using a pretested decision guide. Inter-rater reliability was high (Cohen’s ?? = 0.81 at title/abstract,?? = 0.78 at full text), exceeding the conventional threshold of 0.75 for substantial agreement (Landis & Koch, 1977). Disagreements were resolved through discussion, and adjudicated by a third reviewer. To mitigate prestige and recency bias during screening, we masked author, venue, and year fields, reviewing only titles, abstracts, and keywords. At each screening stage we tracked inclusion and exclusion tallies with reasons, and we documented the exact query strings per database together with the date of last search. All materials, including metadata on the final sample of documents, extracted datasets, codebooks, and reliability statistics, will be archived in our OSF repository upon acceptance.Inclusion criteria required (i) direct study of LLMs (e.g., GPT, Claude, LLaMA, Mistral),(ii) a social-scientific construct (e.g., ToM, moral judgment, trust, bias, norms, cooperation, persuasion, human–AI interaction), and (iii) English-language publication (2020–2025) in journals, conferences, or preprints. Exclusions covered purely technical/engineering work, studies using LLMs only as tools, non-LLM models, and biomedical applications without social variables.7.3 Study 1We represent each document as a dense sentence embedding produced by a transformer encoder and then partition these vectors with ??-means. Because human readers reason in two dimensions more comfortably than in the original high-dimensional space, we visualize the clusters via a nonlinear manifold embedding and overlay statistical geometry (centers, confidence ellipses, kernel density estimates) to summarize within-cluster dispersion. Throughout, we quantify internal quality (compactness/separation) and algorithmic stability so that any selected solution is both interpretable and statistically defensible. We further enhance interpretability by modeling latent semantic themes within each cluster using probabilistic topic models (Latent Dirichlet Allocation).7.3.1 Sentence embeddings and clusteringEach document is encoded with a Sentence-Transformer model (SBERT), which adapts BERTstyle encoders with Siamese architecture so that cosine similarity between embeddings correlates with semantic similarity, an essential property for unsupervised discovery of topical structure in text collections (Reimers & Gurevych, 2019). In our configuration we use the MPNet family of encoders, which combine masked and permuted pretraining to model bidirectional context while mitigating positional mismatch, yielding strong sentence-level representations across transfer tasks (Song et al., 2020). These choices are standard for clustering pipelines that rely on cosine geometry in embedding space. For completeness, when we refer to Euclidean operations (e.g., ??-means assigns by squared Euclidean distance), we note that on ??2-normalized vectors Euclidean distance is a monotone transform of cosine dissimilarity; hence assignments are closely aligned with cosine-based similarity structure (Manning, 2008, §6).We partition the documents with Lloyd–MacQueen ??-means, minimizing within-cluster sum of squares under Euclidean geometry (Lloyd, 1982; McQueen, 1967). As argued above, on normalized sentence embeddings Euclidean assignments are consistent with cosine neighborhoods that drive SBERT semantics, providing a conceptually coherent pipeline from representation to partition. We used ??-means++ initialization (the default in scikit-learn), with ??init = 10, ??????_???????? = 300, and fixed random seed 42. The number of clusters ?? is selected by internal criteria (elbow of inertia, silhouette, Calinski–Harabasz, and Davies–Bouldin indices); numerical diagnostics are reported in Section 7.3.3.7.3.2 Dimensionality reduction for visualizationTo render the geometry of clusters in a manner comprehensible to human readers, we projected the high-dimensional sentence embeddings into two dimensions (R2) using Uniform Manifold Approximation and Projection (UMAP). UMAP is a manifold learning technique that constructs a fuzzy topological representation of the data and then optimizes a low-dimensional embedding that preserves local neighborhood structure while retaining substantial global geometry (McInnes et al., 2018). Unlike linear projections such as principal component analysis, UMAP flexibly adapts to nonlinear structures and can reveal meaningful groupings even in data where clusters are arranged along curved or manifold-like structures. In this study, the two-dimensional UMAP embedding serves exclusively as a visualization tool: cluster assignments are determined in the original embedding space, and the reduced coordinates provide an interpretable plane for overlaying statistical geometry such as cluster centers, confidence ellipses, and kernel density contours.7.3.3 Cluster number selection and validationThe number of clusters ?? is treated as unknown and chosen by triangulating multiple internal validity criteria. For each candidate ??, we compute indices that capture complementary aspects of compactness and separation:• Silhouette (mean over samples), defined as??(??) ? ??(??)	??(??) =	,max{??(??), ??(??)}with ?? the within-cluster dissimilarity and ?? the nearest-cluster dissimilarity (here using cosine dissimilarity). It ranges in [?1, 1] and penalizes overlapping clusters (Rousseeuw, 1987).• Davies–Bouldin Index (DBI): average ratio of within-cluster scatter to between-cluster separation; lower is better (Davies & Bouldin, 2009).• Calinski–Harabasz Index (CHI): ratio of between- to within-cluster dispersion; higher is better (Cali?ski & Harabasz, 1974).The K-means elbow analysis in Figure 8a displays a characteristic inflection at ?? = 3: inertia drops steeply from approximately 131.087 at ?? = 2 to 123.065 at ?? = 3, after which the curve transitions into a near-linear regime (e.g., 118.440 at ?? = 4, 115.057 at ?? = 5, 112.293 at ?? = 6, and only gradually to 103.251 by ?? = 10). This profile indicates rapidly diminishing returns beyond three partitions, consistent with the view that additional clusters primarily carve finer within-group variation rather than resolving major structure.(a) Elbow Curve of K-Means Inertia across Differ- (b) Mean Silhouette Scores across Different K. ent K. This plot shows the elbow curve for K-means The silhouette score measures how similar an object clustering, where the inertia (sum of squared dis- is to its own cluster compared to other clusters, with tances between points and their centroids) is plotted higher values indicating better clustering quality. against different values of K.Figure 8: K-Means Clustering Evaluation: (a) Elbow Curve and (b) Silhouette ScoresSilhouette widths (Figure 8b) are also maximized at ?? = 3 (0.148), exceeding all alternatives(e.g., 0.120 at ?? = 2, 0.131 at ?? = 4, 0.136 at ?? = 5; values then decline to 0.121–0.110 for ?? = 6–7 before a modest uptick to 0.135 and 0.126 for ?? = 8 and ?? = 9). Because the silhouette index jointly penalizes within-cluster dispersion and between-cluster overlap (Rousseeuw, 1987), the peak at ?? = 3 implies that the tripartite solution achieves the best compactness–separation balance in this corpus.Table 4 compares all indices. At ?? = 3, the silhouette again attains its global maximum (0.148). The Davies–Bouldin Index (lower is better) registers 3.021 at ?? = 3, which is competitive but not minimal (it continues to decline to 2.739 at ?? = 9); conversely, the Calinski–Harabasz ratio (higher is better) has a local high at 20.224 for ?? = 3 but attains its absolute maximum at ?? = 2 (21.712). The three metrics together therefore recommend a parsimonious selection: ?? = 2 over-aggregates (lower silhouette, 0.120), whereas ?? ≥ 4 yields steadily lower CHI (e.g., 17.418 at ?? = 4, 15.345 at ?? = 5, falling to 10.760 at ?? = 10) and no improvement in silhouette. On methodological grounds, a solution is preferable when it simultaneously sits at the elbow of the inertia curve and maximizes silhouette while avoiding CHI degradation, criteria that jointly single out ?? = 3 as the defensible operating point (Cali?ski & Harabasz, 1974; Davies & Bouldin, 2009).Number of Clusters (K)Silhouette Coefficient (?)Davies–Bouldin Index (?)Calinski–Harabasz Index (?)20.1203.30521.71230.1483.02120.22440.1313.05717.41850.1362.96215.34560.1213.08013.83170.1102.97812.72580.1352.78012.10490.1262.73911.522100.1222.78610.760Table 4: Internal Validation Indices (Silhouette, DBI, CHI) across K. This table compares three internal validation indices for different values of K. The row for ?? = 3 is highlighted as the optimal choice.7.3.4 Supplementary diagnostic analysesHaving fixed ?? = 3 on internal criteria, we next interrogate the geometry of the solution and the quality of its assignments. The marginal density diagnostics in Figure 9 quantify this along each embedding axis. On Dimension 1 (Figure 9a), the three clusters exhibit separated modes: the green cluster (C0) peaks leftmost near Dimension 1 ≈ 10.0 (with a minor shoulder around 8.5), the blue cluster (C2) peaks in the mid-range near 12.5, and the orange cluster (C1) peaks further right near 12.8 with a right tail extending toward ≈ 14.8. On Dimension 2 (Figure 9b), C1 peaks at the lowest values (around Dimension 2 ≈ 0.9), C0 centers in the mid-range (around 3 with a secondary shoulder near 6), and C2 peaks at higher values (around 4.2). Overlap is limited to the expected boundary neighborhoods: C0 with C2 between Dimension 2 ≈ 2.5, and C0 with C1 near Dimension 2 ≈ 1.8. The KDEs thus corroborate that the three groups occupy distinct density basins in the 2-D manifold, with C1 the most compact along Dimension 2 and C0 the most dispersed across both axes–patterns consistent with the cluster geometries observed on the Figure 4.(a) One-Dimensional KDE Distribution along (b) One-Dimensional KDE Distribution along Dimension 1 by Cluster. This plot shows the Dimension 2 by Cluster. This plot shows the Kernel Density Estimation (KDE) of the distribution KDE distribution of each cluster along the second of each cluster along the first dimension of the 2D dimension of the 2D embedding. embedding.Figure 9: One-Dimensional KDE Distribution along Dimension 1 and Dimension 2 by ClusterPairwise centroid similarities (cosine) reveal moderate and graded inter-cluster relatedness(Figure 10): ??C1,C0 = 0.826 is the highest, followed by ??C2,C0 = 0.802, with ??C2,C1 = 0.764 the lowest (diagonals equal 1 by construction). Thus, while all three groups are well separated, the C0–C1 centroids are slightly closer than either pair involving C2. This ordering is compatible with the marginal structure: C0 shows a minor Dimension 1 shoulder that abuts C1’s mode, whereas C1–C2, despite local UMAP contact, have the least similar centroids in the original space. In short, centroid-level similarity indicates three distinct but not equidistant attractors (C0–C1 > C0–C2 > C1–C2), whereas the UMAP boundary points capture local adjacency rather than global proximity, offering two complementary perspectives on the same tripartite structure.Figure 10: Cosine Similarity Heatmap between Cluster Centroids. This heatmap shows the cosine similarity between cluster centroids. It highlights the degree of similarity between clusters.We then examine assignment quality at the document level via silhouette distributions. Figure 11 displays cluster-wise violins using cosine-based silhouette widths. Median separation is highest for C1 (median 0.200, with an interquartile band 0.136–0.260), intermediate for C0 (median 0.154, IQR 0.093–0.189), and lowest for C2 (median 0.088, IQR 0.006–0.144). The lower tails differ accordingly: C2 shows the longest negative tail (down to ?0.080), C0 has a small negative mass (to ?0.016), whereas C1 remains strictly positive (min 0.026). The shape of the violins aligns with the KDE diagnostics: C1’s tighter core supports the highest central tendency, C0 is intermediate, and C2’s greater boundary exposure depresses its median. The small mass of negative silhouettes appears localized (cf. red triangles on Figure 4), rather than pervasive, indicating that borderline points are structurally constrained to interfacial corridors instead of diffused through cluster interiors (Rousseeuw, 1987).Figure 11: Silhouette Score Distributions by Cluster. This figure displays the silhouette score distribution for each cluster in the form of violin plots.7.3.5 Stability analysis (robustness to resampling)Clustering is an algorithmic inference without explicit parametric likelihood; therefore stability under resampling is a crucial evidentiary pillar. We implement a consensus-clustering procedure: repeatedly subsample the corpus, recluster with ??-means, and accumulate a co-association (evidence-accumulation) matrix ?? where ?????? is the proportion of runs in which items ?? and ?? co-occur in the same cluster. We then (i) visualize the permuted ?? to reveal block structure and (ii) summarize it by hierarchical clustering of 1??? using average linkage, yielding a dendrogram that exposes stable groupings (Fred & Jain, 2005; Monti et al., 2003; Sokal & Michener, 1958).The consensus heatmap (Figure 12a) shows three crisp blocks of near-unity co-assignment(yellow), separated by dark off-block regions, across 40 subsampled runs (≈ 80% of the corpus per run, without replacement), with pairwise frequencies normalized by the number of times both documents were co-sampled. The sharp block boundaries and the absence of checkerboard artifacts suggest that individual documents rarely flip between clusters under resampling, except for a narrow band of interface cases, precisely those flagged by negative silhouettes. The corresponding dendrogram on 1 ? ?? (Figure 12b) corroborates the same tripartite structure.(a) Consensus heatmap.(b) Consensus dendrogram (average linkage on 1 ???).Figure12: Consensusclusteringdiagnostics. (a)Heatmapofsampleco-assignmentfrequencies across 40 subsampled runs (≈ 80% per run, without replacement), normalized by co-sampling counts. (b) Average-linkage dendrogram on 1 ? ??, revealing three stable branches consistentwith the blocks in (a).Asanorthogonalcheck, wecomputeAdjustedRandIndex(ARI)andAdjustedMutualInformation (AMI) across resamples against a baseline partition obtained by ??-means on the same subsample; this produces the ARI/AMI boxplots used here as a stability diagnostic (not a comparison to any human labels). ARI adjusts for chance under a permutation model (Hubert & Arabie, 1985); AMI does so in an information-theoretic framework (Vinh et al., 2009). High median and tight dispersion in these distributions indicate replicability of assignments under data perturbations. Figure 13 shows the ARI and AMI. Both concentrate near 1.0: the median ARI ≈ 0.985 (with an interquartile band roughly 0.985–0.999 and occasional low outliers around 0.961–0.962), while the median AMI ≈ 0.989 (IQR about 0.978–0.999, sporadic minima around 0.934–0.940). These distributions imply that the tripartite assignments are recovered almost exactly across resamples, meeting stringent standards for replicability (Hubert & Arabie, 1985; Monti et al., 2003; Vinh et al., 2009).Figure 13: Stability Analysis: ARI and AMI Distributions across Consensus Runs. This boxplot compares the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) across multiple consensus clustering runs.7.3.6 Topic modeling within machine clustersTo render each machine-derived cluster interpretable in terms of human-readable themes, we further applied Latent Dirichlet Allocation (LDA) (Blei, 2012; Blei et al., 2003). LDA treats each document as a mixture of latent topics and each topic as a distribution over words, thereby offering a generative probabilistic account of text corpora. For each cluster, we constructed a document–term matrix with unigrams and bigrams (after stopword removal) and fitted candidate LDA models with varying numbers of topics. Instead of pre-fixing the number of topics, we adopted a data-driven selection strategy: for each candidate ??, we computed perplexity, an out-of-sample likelihood measure where lower values indicate better predictive fit, and UMass coherence, a co-occurrence-based semantic coherence measure where higher values indicate more interpretable topics (Mimno et al., 2011). Both metrics were normalized via ??-scores and summed to yield a composite score; the ?? maximizing this composite was selected as the optimal topic count for that cluster. This procedure balances predictive adequacy with semantic interpretability, avoiding the pitfalls of relying on a single criterion.Figure 14 reports diagnostic plots of perplexity, coherence, and the combined score across candidate ??, highlighting the chosen solution and revealing a common pattern: the minimum perplexity occurs at ?? = 2 and coherence is essentially flat to gently declining as ?? increases, so the composite ??total falls monotonically from ?? = 2 to ?? = 5. Concretely, in Cluster 0 (Figure 14a) perplexity rises steadily from the lowest value at ?? = 2 to the largest at ?? = 5, while coherence is nearly horizontal with only slight undulations; the composite therefore peaks at ?? = 2. Cluster 1(Figure 14b) exhibits the same minimum at ?? = 2; perplexity increases monotonically through ?? = 5, coherence is almost flat with a very mild downward drift, and the composite again favors ?? = 2. Cluster 2 (Figure 14c) shows the sharpest overall decline in the composite: perplexity increases from ?? = 2 to ?? = 3, dips slightly at ?? = 4, and then reaches its highest value at ?? = 5; coherence declines mildly with ??. We therefore adopt two topics per cluster on principled grounds: additional topics would over-partition the already compact semantic basins without measurable gains in either fit or coherence.(a) Cluster 0 (?? = 2)(b) Cluster 1 (?? = 2)	(c) Cluster 2 (?? = 2)Figure 14: Evaluation of LDA topic numbers across clusters using Perplexity, UMass Coherence, and the combined z-score metric. Subfigures (a)–(c) show results for clusters 0–2, with the chosen optimal ?? marked for each cluster.The final topic structures were visualized using ??-parameterized word clouds (see Figures 5, 6, and 7), where ?? = 0 highlights distinctive, topic-specific terms and ?? = 1 emphasizes globally frequent, high-probability terms (Chuang et al., 2012). This layered topic modeling analysis enables a principled semantic annotation of each machine-discovered cluster, while remaining entirely free of human supervision. To examine cross-cluster thematic overlap, we additionally computed pairwise cosine similarity between topics derived from LDA models. As shown in Figure 15, the heatmap visualizes topic–topic similarity across clusters, with darker blue cells indicating stronger semantic relatedness and red cells indicating lower similarity. This step ensures that our taxonomy accounts not only for within-cluster coherence but also for inter-cluster affinities at the topic level.Figure 15: Cosine similarity heatmap of LDA topics across clusters. Each cell indicates the semantic similarity between a pair of topics (e.g., C0–T0 vs. C1–T1), with darker blue representing higher similarity and red lower similarity.7.3.7 VisualizationThe Study 1 main visualization (see Figure 4) presents the 2-D embedding (UMAP) colored by cluster, with three statistical overlays that communicate center and spread:• Robust cluster centers are plotted as “?” markers at the geometric median of each cluster’s 2-D coordinates, a high-breakdown estimator minimizing the sum of Euclidean distances and offering resistance to outliers (Small, 1990; Weiszfeld & Plastria, 2009).• 95% confidence ellipses summarize within-cluster dispersion under a local Gaussian approximation: we compute the empirical covariance of points in 2-D, obtain eigenvectors/eigenvalues, and scale the principal axes by the ??22,0.95 quantile to draw the ellipse (Johnson, Wichern, et al., 2002).• Silhouette-flaggedpoints: itemswithnegativesilhouettevaluesaredenotedwithtriangular markers, signaling likely boundary cases that may merit qualitative inspection.To aid perceptual grouping, we also render a kernel density estimate of the overall point cloud as a background layer (Davis et al., 2011; Parzen, 1962), which reveals global density basins consistent with the discovered partitions. Collectively, the figure communicates (i) the separation of clusters, (ii) within-cluster anisotropy, and (iii) the prevalence and location of borderline items, information that purely discrete partitions cannot convey.(Implementation note for readers reproducing our pipeline.) The plotting routine computes geometric medians and ellipses from the 2-D embedding used for display, while assignments come from the high-dimensional ??-means step; points with ?? < 0 receive distinct markers; and colors are consistent across panels. All data and code will be deposited upon acceptance (OSF project: https://osf.io/8umn7).”7.3.8 Why this design is methodologically soundThis pipeline rests on four complementary methodological principles. First, representation adequacy: transformer-based sentence embeddings have well-documented semantic geometry that supports clustering with cosine/Euclidean metrics (Manning, 2008; Reimers & Gurevych, 2019; Song et al., 2020). Second, visual faithfulness: UMAP provides low-distortion maps of local neighborhoods while preserving larger-scale relationships better than earlier manifold learning methods (McInnes et al., 2018). Third, quantified reliability: internal validity indices evaluate compactness and separation, while consensus-based stability analysis and ARI/AMI distributions assess replicability under perturbations, a critical guard against over-interpreting algorithmic artifacts (Cali?ski & Harabasz, 1974; Davies & Bouldin, 2009; Fred & Jain, 2005; Hubert & Arabie, 1985; Monti et al., 2003; Rousseeuw, 1987; Vinh et al., 2009). Fourth, semantic interpretability: by applying Latent Dirichlet Allocation within each cluster, we provide probabilistic topic models that yield human-readable themes. Crucially, the number of topics is not arbitrarily fixed but is selected via a combined criterion that normalizes and integrates perplexity (predictive fit) and UMass coherence (semantic consistency), ensuring that each topic structure balances statistical adequacy with interpretability (Blei et al., 2003; Mimno et al., 2011; Sievert & Shirley, 2014). The integration of these principles, together with cluster-level topic modeling, results in a coherent and interpretable representation of structure, entirely independent of human labels (with external comparisons deferred to Study 2).7.4 Study 2Study 2 assesses the extent to which the unsupervised structure uncovered in Study 1 accords with a domain expert taxonomy. We proceed from a deliberately conservative stance that treats expert labels as a reference standard (not an oracle), and we quantify “alignment” along three complementary axes: (i) external agreement of the unsupervised partition with the expert taxonomy via NMI and ARI, and (ii) predictability of the expert taxonomy from document semantics via a supervised classifier evaluated by Macro-F1 under stratified cross-validation. (iii)we overlay the three super-categories, derived respectively by experts and by the machine-induced automatic labeling, on the same two-dimensional manifold of the embedding space, visually inspecting the co-location of major blocs and the concentration of negative-silhouette points along conceptual boundaries. All three indices, NMI, ARI, and Macro-F1, are thus anchored to the same expert label set and allow us to triangulate correspondence from both unsupervised and supervised perspectives (Hubert & Arabie, 1985; Kohavi et al., 1995; Sebastiani, 2002; Vinh et al., 2009).7.4.1 Expert taxonomy and labeling protocolThe expert scheme partitions the corpus into three theoretically motivated super-categories: LLM as Social Minds, LLM Societies and LLM–Human Interactions. These manual annotations served as the baseline reference for subsequent evaluation. To construct and maintain the scheme,we followed a codebook-first procedure rooted in qualitative content analysis: definitions, inclusion/exclusion criteria, and canonical exemplars were iteratively refined as labeling proceeded (Krippendorff, 2018; Miles & Huberman, 1984). Persistent disagreements among coders were addressed by re-reading the original studies in their disciplinary context, discussing borderline cases, and, where necessary, minimally revising the codebook. Each adjudication was recorded in analytic memos (decision, rationale, alternatives considered), which provided an auditable trail supporting dependability and confirmability (Lincoln, 1985; Rogers, 2018).Because conventional small-study bias diagnostics used in quantitative meta-analysis (e.g., funnel plot asymmetry tests or Egger’s regression) are not generally applicable to narrative syntheses, we conducted qualitative publication-bias assessments instead. Specifically, we considered contrasts between peer-reviewed and preprint studies, documented exclusions due to inaccessibility, and reflected on the implications of restricting the review to English-language publications. Such appraisal does not provide statistical correction, but it enhances transparency and acknowledges potential sources of systematic bias that may shape the composition and interpretation of the evidence base (Egger et al., 1997; Sterne et al., 2011).7.4.2 Semantic features and supervised predictability of expert labelsTo test whether the expert taxonomy is predictable from text alone, we encode each document with Sentence-BERT (SBERT) embeddings, whose Siamese training objective yields semantically meaningful vector geometry for cosine-based comparisons (Reimers & Gurevych, 2019). We then fit a multinomial one-vs-rest logistic regression with ??2 regularization on the embeddings, a strong and well-understood linear baseline in text classification that is competitive on modern sentence embeddings (Lin et al., 2023; see also recent evidence with LLM embeddings: Buckmann and Hill, 2024). Model selection and performance estimation use five-fold stratified cross-validation (Kohavi et al., 1995). We report Macro-F1, the unweighted mean of per-class F1, so that minority expert classes cannot be obscured by class imbalance, following common evaluation practice in text categorization (Moreo et al., 2021; Sebastiani, 2002).7.4.3 External agreement of unsupervised partitions to expert labelsAgreement of the unsupervised K-means partition with the expert super-categories is quantified by Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI). ARI adjusts the pair-counting Rand index for chance under a permutation model; values near 0 indicate chancelevel concordance, while 1 indicates identity (Hubert & Arabie, 1985). NMI compare the information content of two partitions with normalizations and chance corrections that make scores comparable across different clusterings; they are standard for evaluating clustering against a labeled reference (Strehl & Ghosh, 2002; Vinh et al., 2009). The Study-2 evaluation is the metrics table (Table 3) summarizing Macro-F1 (mean ± s.d.), NMI, and ARI; as implemented, Macro-F1 is the only metric with variance information (reported as mean ± s.d. from cross-validation),while NMI/ARI are point estimates computed on the full corpus against the manual labels.7.4.4 VisualizationThe main Study 2 visualization is the expert-labeled map (manual categories only, Figure 16), which enables rapid qualitative reading of thematic geography and boundary zones under the expert scheme). This panel is generated by selecting the expert-defined super-categories as the basis for grouping, and layering each category as a distinct scatter group with consistent legend ordering, without any Study-1 overlays (e.g. cluster ellipses), thus isolating the human taxonomy as the organizing principle. To directly assess spatial correspondence, we overlay the expert-defined and machine-induced super-categories on the same two-dimensional manifold to visually assess the spatial correspondence of major blocs and the alignment of category boundaries. Quantitative alignment is reported via NMI, ARI, and Macro-F1 against the manual taxonomy.Figure 16: Embedding Visualization Colored by Manual Framework Labels. The documents are colored according to manually assigned labels representing three broad research frameworks.7.4.5 Narrative synthesisBecause constructs and designs are heterogeneous across the literature, we do not pool effect sizes; instead we perform a narrative synthesis organized by the three super-categories, cross-linking locations where machine and expert assignments diverge. This aligns with best-practice guidance for narrative synthesis and Synthesis Without Meta-analysis (SWiM) reporting in heterogeneous literatures (Borenstein et al., 2021; Campbell et al., 2020; Popay et al., 2006, ch. 40).7.4.6 Reporting and interpretive postureWe emphasize that Macro-F1 evaluates how well a simple, transparent linear model can recover the expert taxonomy from document semantics, whereas NMI/ARI evaluate how well the unsupervised partition coincides with that taxonomy without exposure to labels. Because all three are baselined to the same manual labels, convergent evidence (high Macro-F1 with commensuratelyhighNMI/ARI)supportssubstantiveagreementbetweenthediscoveredsemantic structure and the expert conceptual scheme; divergence, by contrast, highlights either ambiguous regions in the taxonomy or granularity mismatches that warrant qualitative follow-up. This dual-track design, encompassing predictability and partition agreement, follows long-standing evaluation practice in text categorization and clustering and yields interpretable, statistically defensible claims about “alignment” (Hubert & Arabie, 1985; Sebastiani, 2002; Vinh et al., 2009).ReferencesAbid, A., Farooqi, M., & Zou, J. (2021). Persistent anti-muslim bias in large language models. Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 298–306.Aher, G. V., Arriaga, R. I., & Kalai, A. T. (2023). Using large language models to simulate multiple humans and replicate human subject studies. International conference on machine learning, 337–371.Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M., & Schulz, E. (2025). Playing repeated games with large language models. Nature Human Behaviour, 1–11.Amirizaniani, M. (2025). Mind over machine: Evaluating theory of mind reasoning in llms and humans. Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining, 1068–1070.An, J., Huang, D., Lin, C., & Tai, M. (2025). Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation. PNAS nexus, 4(3), pgaf089.Anderson, B. R., Shah, J. H., & Kreminski, M. (2024). Homogenization effects of large language models on human creative ideation. Proceedings of the 16th conference on creativity & cognition, 413–425.Apperly, I. (2010). Mindreaders: The cognitive basis of" theory of mind". Psychology Press.Arcas, B. A. (2022). Do large language models understand us? Daedalus, 151(2), 183–197.Bai, X., Wang, A., Sucholutsky, I., & Griffiths, T. L. (2025). Explicitly unbiased large language models still form biased associations. Proceedings of the National Academy of Sciences, 122(8), e2416228122.Bail, C. A. (2024). Can generative ai improve social science? Proceedings of the National Academy of Sciences, 121(21), e2314021121.Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610–623.Bender, E. M., & Koller, A. (2020). Climbing towards nlu: On meaning, form, and understanding in the age of data. Proceedings of the 58th annual meeting of the association for computational linguistics, 5185–5198.Binz, M., & Schulz, E. (2023). Using cognitive psychology to understand gpt-3. Proceedings of the National Academy of Sciences, 120(6), e2218523120.Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77–84.Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993–1022.Bommasani, R. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2021). Introduction to meta-analysis. John wiley & sons.Borghoff, U. M., Bottoni, P., & Pareschi, R. (2025). An organizational theory for multi-agent interactions integrating human agents, llms, and specialized ai. Discover Computing, 28(1), 1–35.Bowker, G. C., & Star, S. L. (2000). Sorting things out: Classification and its consequences. MIT press.Buckmann, M., & Hill, E. (2024). Logistic regression makes small llms strong and explainable" tens-of-shot" classifiers. arXiv preprint arXiv:2408.03414.Burton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., et al. (2024). How large language models can reshape collective intelligence. Nature human behaviour, 8(9), 1643–1655.Cali?ski, T., & Harabasz, J. (1974). A dendrite method for cluster analysis. Communications in Statistics-theory and Methods, 3(1), 1–27.Campbell, D. T., & Fiske, D. W. (1959). Convergent and discriminant validation by the multitrait-multimethod matrix. Psychological bulletin, 56(2), 81.Campbell, M., McKenzie, J. E., Sowden, A., Katikireddi, S. V., Brennan, S. E., Ellis, S., Hartmann-Boyce, J., Ryan, R., Shepperd, S., Thomas, J., et al. (2020). Synthesis without meta-analysis (swim) in systematic reviews: Reporting guideline. bmj, 368.Cartwright, N. (1983). How the laws of physics lie. OUP Oxford.Chen, Z., & Chan, J. (2024). Large language model in creative work: The role of collaboration modality and user expertise. Management Science, 70(12), 9101–9117.Cheung, V., Maier, M., & Lieder, F. (2025). Large language models show amplified cognitive biases in moral decision-making. Proceedings of the National Academy of Sciences, 122(25), e2412015122.Chiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing ai-assisted group decision making through llm-powered devil’s advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119.Chuang, J., Manning, C. D., & Heer, J. (2012). Termite: Visualization techniques for assessing textual topic models. Proceedings of the international working conference on advanced visual interfaces, 74–77.Chuang, Y.-S., Suresh, S., Harlalka, N., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2023). The wisdom of partisan crowds: Comparing collective intelligence in humans and llm-based agents. arXiv preprint arXiv:2311.09665.Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1), 37–46.Colombatto, C., Birch, J., & Fleming, S. M. (2025). The influence of mental state attributions on trust in large language models. Communications Psychology, 3(1), 84.Comaniciu, D., & Meer, P. (1999). Mean shift analysis and applications. Proceedings of the seventh IEEE international conference on computer vision, 2, 1197–1203.Crawford, K. (2021). The atlas of ai: Power, politics, and the planetary costs of artificial intelligence. Yale University Press.Cronbach, L. J., & Meehl, P. E. (1955). Construct validity in psychological tests. Psychological bulletin, 52(4), 281.Davies, D. L., & Bouldin, D. W. (2009). A cluster separation measure. IEEE transactions on pattern analysis and machine intelligence, (2), 224–227.Davis, R. A., Lii, K.-S., & Politis, D. N. (2011). Remarks on some nonparametric estimates of a density function. In Selected works of murray rosenblatt (pp. 95–100). Springer.De Curtò, J., & De Zarzà, I. (2025). Llm-driven social influence for cooperative behavior in multi-agent systems. IEEE Access.Decelle, A., Krzakala, F., Moore, C., & Zdeborová, L. (2011). Inference and phase transitions in the detection of modules in sparse networks. Physical Review Letters, 107(6), 065701.Delvenne, J.-C., Yaliraki, S. N., & Barahona, M. (2010). Stability of graph communities across time scales. Proceedings of the national academy of sciences, 107(29), 12755–12760.Du, Y., Li, S., Torralba, A., Tenenbaum, J. B., & Mordatch, I. (2023). Improving factuality and reasoning in language models through multiagent debate. Forty-first International Conference on Machine Learning.Egger, M., Smith, G. D., Schneider, M., & Minder, C. (1997). Bias in meta-analysis detected by a simple, graphical test. bmj, 315(7109), 629–634.Fang, J., Arechiga, N., Namikoshi, K., Bravo, N., Hogan, C., & Shamma, D. A. (2024). On llm wizards: Identifying large language models’ behaviors for wizard of oz experiments. Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents, 1–11.Fasce, A., Schmid, P., Holford, D. L., Bates, L., Gurevych, I., & Lewandowsky, S. (2023). A taxonomy of anti-vaccination arguments from a systematic literature review and text modelling. Nature human behaviour, 7(9), 1462–1480.Fontana, N., Pierri, F., & Aiello, L. M. (2025). Nicer than humans: How do large language models behave in the prisoner’s dilemma? Proceedings of the International AAAI Conference on Web and Social Media, 19, 522–535.Fred, A. L., & Jain, A. K. (2005). Combining multiple clusterings using evidence accumulation. IEEE transactions on pattern analysis and machine intelligence, 27(6), 835–850.Gallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., & Ahmed, N. K. (2024). Bias and fairness in large language models: A survey.Computational Linguistics, 50(3), 1097–1179.Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., & Wichmann,F. A. (2020). Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11), 665–673.Gershman, S. J., Horvitz, E. J., & Tenenbaum, J. B. (2015). Computational rationality: A converging paradigm for intelligence in brains, minds, and machines. Science, 349(6245), 273–278.Glickman, M., & Sharot, T. (2025). How human–ai feedback loops alter human perceptual, emotional and social judgements. Nature Human Behaviour, 9(2), 345–359.Goodhart, C. A. (1984). Problems of monetary management: The uk experience. In Monetary theory and practice: The uk experience (pp. 91–121). Springer.Grogan, C., Kay, J., & Pérez-Ortiz, M. (2025). Ai will always love you: Studying implicit biases in romantic ai companions. arXiv preprint arXiv:2502.20231.Ha, J., Jeon, H., Han, D., Seo, J., & Oh, C. (2024). Clochat: Understanding how people customize, interact, and experience personas in large language models. Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, 1–24.Hacking, I. (1983). Representing and intervening: Introductory topics in the philosophy of natural science. Cambridge university press.Hagendorff, T. (2024). Deception abilities emerged in large language models. Proceedings of the National Academy of Sciences, 121(24), e2317967121.Haltaufderheide, J., & Ranisch, R. (2024). The ethics of chatgpt in medicine and healthcare: A systematic review on large language models (llms). NPJ digital medicine, 7(1), 183.Healy, J., & McInnes, L. (2024). Uniform manifold approximation and projection. Nature Reviews Methods Primers, 4(1), 82.Horiguchi, I., Yoshida, T., & Ikegami, T. (2024). Evolution of social norms in llm agents using natural language. arXiv preprint arXiv:2409.00993.Hu, T., Kyrychenko, Y., Rathje, S., Collier, N., van der Linden, S., & Roozenbeek, J. (2025).Generative language models exhibit social identity biases. Nature Computational Science, 5(1), 65–75.Hubert, L., & Arabie, P. (1985). Comparing partitions. Journal of classification, 2(1), 193–218.Humlum, A., & Vestergaard, E. (2025). The unequal adoption of chatgpt exacerbates existing inequalities among workers. Proceedings of the National Academy of Sciences, 122(1), e2414972121.Hutchins, E. (1995). Cognition in the wild. MIT press.Jacob,C.,Kerrigan,P.,&Bastos,M.(2025).Thechat-chambereffect:Trustingtheaihallucination. Big Data & Society, 12(1), 20539517241306345.Jeon, J., & Lee, S. (2023). Large language models in education: A focus on the complementary relationship between human teachers and chatgpt. Education and Information Technologies, 28(12), 15873–15892.Johnson, R. A., Wichern, D. W., et al. (2002). Applied multivariate statistical analysis.Johnson, T., & Obradovich, N. (2025). Testing for completions that simulate altruism in early language models. Nature Human Behaviour, 1–10.Jones, C. R., Rathi, I., Taylor, S., & Bergen, B. K. (2025). People cannot distinguish gpt-4 from a human in a turing test. Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency, 1615–1639.Joshi, I., Budhiraja, R., Dev, H., Kadia, J., Ataullah, M. O., Mitra, S., Akolekar, H. D., & Kumar, D. (2024). Chatgpt in the classroom: An analysis of its strengths and weaknesses for solving undergraduate computer science questions. Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1, 625–631.Karten, S., Li, W., Ding, Z., Kleiner, S., Bai, Y., & Jin, C. (2025). Llm economist: Large population models and mechanism design in multi-agent generative simulacra. arXiv preprint arXiv:2507.15815.Kohavi, R., et al. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. Ijcai, 14(2), 1137–1145.Kosinski, M. (2023). Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083, 4, 169.Kosinski, M. (2024). Evaluating large language models in theory of mind tasks. Proceedings of the National Academy of Sciences, 121(45), e2405460121.Kotek, H., Dockum, R., & Sun, D. (2023). Gender bias and stereotypes in large language models.Proceedings of the ACM collective intelligence conference, 12–24.Krippendorff, K. (2018). Content analysis: An introduction to its methodology. Sage publications. Kumar, H., Musabirov, I., Reza, M., Shi, J., Wang, X., Williams, J. J., Kuzminykh, A., & Liut, M. (2024). Guiding students in using llms in supported learning environments: Effects on interaction dynamics, learner performance, confidence, and trust. Proceedings of the ACM on Human-Computer Interaction, 8(CSCW2), 1–30.Kuntur, S., Wróblewska, A., Paprzycki, M., & Ganzha, M. (2024). Under the influence: A survey of large language models in fake news detection. IEEE Transactions on Artificial Intelligence.Lai, J., Gan, W., Wu, J., Qi, Z., & Yu, P. S. (2024). Large language models in law: A survey. AI Open, 5, 181–196.Lambiotte, R., Delvenne, J.-C., & Barahona, M. (2008). Laplacian dynamics and multiscale modular structure in networks. arXiv preprint arXiv:0812.1770.Landis, J. R., & Koch, G. G. (1977). An application of hierarchical kappa-type statistics in the assessment of majority agreement among multiple observers. Biometrics, 363–374.Lang, Q., Wang, M., Yin, M., Liang, S., & Song, W. (2025). Transforming education with generative ai (gai): Key insights and future prospects. IEEE Transactions on Learning Technologies.Latour, B. (2005). Reassembling the social: An introduction to actor-network-theory. Oxford university press.Lee, J. D., & See, K. A. (2004). Trust in automation: Designing for appropriate reliance. Human factors, 46(1), 50–80.Lee, M. H., Montgomery, J. M., & Lai, C. K. (2024). Large language models portray socially subordinate groups as more homogeneous, consistent with a bias observed in humans. Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, 1321–1340.Lehr, S. A., Saichandran, K. S., Harmon-Jones, E., Vitali, N., & Banaji, M. R. (2025). Kernels of selfhood: Gpt-4o shows humanlike patterns of cognitive dissonance moderated by free choice. Proceedings of the National Academy of Sciences, 122(20), e2501823122.Li, G., Hammoud, H., Itani, H., Khizbullin, D., & Ghanem, B. (2023). Camel: Communicative agents for" mind" exploration of large language model society. Advances in Neural Information Processing Systems, 36, 51991–52008.Li, Z., Liang, C., Peng, J., & Yin, M. (2024). The value, benefits, and concerns of generative ai-powered assistance in writing. Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, 1–25.Lin, Y.-C., Chen, S.-A., Liu, J.-J., & Lin, C.-J. (2023). Linear classifier: An often-forgotten baseline for text classification. arXiv preprint arXiv:2306.07111.Lin, Z., Shan, Y., Gao, L., Jia, X., & Chen, S. (2025). Simspark: Interactive simulation of social media behaviors. Proceedings of the ACM on Human-Computer Interaction, 9(2), 1–32. Lincoln, Y. S. (1985). Naturalistic inquiry (Vol. 75). sage.Lloyd, S. (1982). Least squares quantization in pcm. IEEE transactions on information theory, 28(2), 129–137.Mahesh, T. (2024). Investigating social bias in gpt-4: A study on race representation. 2024 2ndInternational Conference on Foundation and Large Language Models (FLLM), 599–606.Manning, C. D. (2008). Introduction to information retrieval. Syngress Publishing,Marchetti, A., Manzi, F., Riva, G., Gaggioli, A., & Massaro, D. (2025). Artificial intelligence and the illusion of understanding: A systematic review of theory of mind and large language models. Cyberpsychology, Behavior, and Social Networking.Marr, D. (2010). Vision: A computational investigation into the human representation and processing of visual information. MIT press.McInnes, L., Healy, J., & Melville, J. (2018). Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426.McQueen, J. B. (1967). Some methods of classification and analysis of multivariate observations. Proc. of 5th Berkeley Symposium on Math. Stat. and Prob., 281–297.Mei, Q., Xie, Y., Yuan, W., & Jackson, M. O. (2024). A turing test of whether ai chatbots are behaviorally similar to humans. Proceedings of the National Academy of Sciences, 121(9), e2313925121.Miles, M. B., & Huberman, A. M. (1984). Qualitative data analysis: A sourcebook of new methods.Mimno, D., Wallach, H., Talley, E., Leenders, M., & McCallum, A. (2011). Optimizing semantic coherence in topic models. Proceedings of the 2011 conference on empirical methods in natural language processing, 262–272.Monti, S., Tamayo, P., Mesirov, J., & Golub, T. (2003). Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data. Machine learning, 52(1), 91–118.Moreo, A., Esuli, A., & Sebastiani, F. (2021). Word-class embeddings for multiclass text classification. Data Mining and Knowledge Discovery, 35(3), 911–963.Newman, M. E. (2006). Modularity and community structure in networks. Proceedings of the national academy of sciences, 103(23), 8577–8582.Ni, Q., Yu, Y., Ma, Y., Lin, X., Deng, C., Wei, T., & Xuan, M. (2024). The social cognition ability evaluation of llms: A dynamic gamified assessment and hierarchical social learning measurement approach. ACM Transactions on Intelligent Systems and Technology.North, D. C. (1990). Institutions, institutional change and economic performance. Cambridge university press.Noy, S., & Zhang, W. (2023). Experimental evidence on the productivity effects of generative artificial intelligence. Science, 381(6654), 187–192.Page, M. J., McKenzie, J. E., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., Shamseer, L., Tetzlaff, J. M., Akl, E. A., Brennan, S. E., et al. (2021). The prisma 2020 statement: An updated guideline for reporting systematic reviews. bmj, 372.Parasuraman, R., & Riley, V. (1997). Humans and automation: Use, misuse, disuse, abuse. Human factors, 39(2), 230–253.Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang, P., & Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. Proceedings of the 36th annual acm symposium on user interface software and technology, 1–22.Park, J. S., Zou, C. Q., Shaw, A., Hill, B. M., Cai, C., Morris, M. R., Willer, R., Liang, P., & Bernstein, M. S. (2024). Generative agent simulations of 1,000 people. arXiv preprint arXiv:2411.10109.Park, P. S., Goldstein, S., O’Gara, A., Chen, M., & Hendrycks, D. (2024). Ai deception: A survey of examples, risks, and potential solutions. Patterns, 5(5).Parzen, E. (1962). On estimation of a probability density function and mode. The annals of mathematical statistics, 33(3), 1065–1076.Pataranutaporn, P., Liu, R., Finn, E., & Maes, P. (2023). Influencing human–ai interaction by priming beliefs about ai can increase perceived trustworthiness, empathy and effectiveness. Nature Machine Intelligence, 5(10), 1076–1086.Peixoto, T. P. (2014). Hierarchical block structures and high-resolution model selection in large networks. Physical Review X, 4(1), 011047.Perez, J., Léger, C., Ovando-Tellez, M., Foulon, C., Dussauld, J., Oudeyer, P.-Y., & Moulin-Frier, C. (2024). Cultural evolution in populations of large language models. arXiv preprint arXiv:2403.08882.Peter, S., Riemer, K., & West, J. D. (2025). The benefits and dangers of anthropomorphic conversational agents. Proceedings of the National Academy of Sciences, 122(22), e2415898122.Piao, J., Lu, Z., Gao, C., Xu, F., Hu, Q., Santos, F. P., Li, Y., & Evans, J. (2025). Emergence of human-like polarization among large language model agents. arXiv preprint arXiv:2501.05171.Popay, J., Roberts, H., Sowden, A., Petticrew, M., Arai, L., Rodgers, M., Britten, N., Roen, K., Duffy, S., et al. (2006). Guidance on the conduct of narrative synthesis in systematic reviews. A product from the ESRC methods programme Version, 1(1), b92.Premack, D., & Woodruff, G. (1978). Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4), 515–526.Radaideh, M. I., Kwon, O. H., & Radaideh, M. I. (2025). Fairness and social bias quantification in large language models for sentiment analysis. Knowledge-Based Systems, 113569.Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084.Ren, S., Cui, Z., Song, R., Wang, Z., & Hu, S. (2024). Emergence of social norms in generative agent societies: Principles and architecture. arXiv preprint arXiv:2403.08251.Rogers, R. (2018). Coding and writing analytic memos on qualitative data: A review of johnny saldaña’s the coding manual for qualitative researchers. The Qualitative Report, 23(4), 889–892.Rogiers, A., Noels, S., Buyl, M., & De Bie, T. (2024). Persuasion with large language models: A survey. arXiv preprint arXiv:2411.06837.Rosvall, M., & Bergstrom, C. T. (2011). Multilevel compression of random walks on networks reveals hierarchical organization in large integrated systems. PloS one, 6(4), e18209.Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20, 53–65.Rozado, D. (2024). The political preferences of llms. PloS one, 19(7), e0306621.Rubin, M., Li, J. Z., Zimmerman, F., Ong, D. C., Goldenberg, A., & Perry, A. (2025). Comparing the value of perceived human versus ai-generated empathy. Nature Human Behaviour, 1–15.Russell, S. J. (1997). Rationality and intelligence. Artificial intelligence, 94(1-2), 57–77.Schmitt, M., & Flechais, I. (2024). Digital deception: Generative artificial intelligence in social engineering and phishing. Artificial Intelligence Review, 57(12), 324.Schneider, J. (2025). Mental model shifts in human-llm interactions. Journal of Intelligent Information Systems, 1–16.Schoenegger, P., Tuminauskaite, I., Park, P. S., Bastos, R. V. S., & Tetlock, P. E. (2024). Wisdom of the silicon crowd: Llm ensemble prediction capabilities rival human crowd accuracy. Science Advances, 10(45), eadp1528.Scott, W. R. (2013). Institutions and organizations: Ideas, interests, and identities. Sage publications.Sebastiani, F. (2002). Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1), 1–47.Shanahan, M., McDonell, K., & Reynolds, L. (2023). Role play with large language models.Nature, 623(7987), 493–498.Shapira, N., Levy, M., Alavi, S. H., Zhou, X., Choi, Y., Goldberg, Y., Sap, M., & Shwartz, V. (2023). Clever hans or neural theory of mind? stress testing social reasoning in large language models. arXiv preprint arXiv:2305.14763.Siddique, Z., Turner, L. D., & Espinosa-Anke, L. (2024). Who is better at math, jenny or jingzhen? uncovering stereotypes in large language models. arXiv preprint arXiv:2407.06917.Sievert, C., & Shirley, K. (2014). Ldavis: A method for visualizing and interpreting topics. Proceedings of the workshop on interactive language learning, visualization, and interfaces, 63–70.Simmons, G. (2022). Moral mimicry: Large language models produce moral rationalizations tailored to political identity. arXiv preprint arXiv:2209.12106.Simon, H. A. (2019). The sciences of the artificial, reissue of the third edition with a new introduction by john laird. MIT press.Small, C. G. (1990). A survey of multidimensional medians. International Statistical Review/Revue Internationale de Statistique, 263–277.Sokal,R.R.,&Michener,C.D.(1958).Astatisticalmethodforevaluatingsystematicrelationships. University of Kansas Science Bulletin, 38, 1409–1438.Song, C., Havlin, S., & Makse, H. A. (2005). Self-similarity of complex networks. Nature, 433(7024), 392–395.Song, K., Tan, X., Qin, T., Lu, J., & Liu, T.-Y. (2020). Mpnet: Masked and permuted pre-training for language understanding. Advances in neural information processing systems, 33, 16857–16867.Sreedhar, K., & Chilton, L. (2024). Simulating human strategic behavior: Comparing single and multi-agent llms. arXiv preprint arXiv:2402.08189.Srnicek, N. (2017). Platform capitalism. John Wiley & Sons.Star, S. L., & Ruhleder, K. (1994). Steps towards an ecology of infrastructure: Complex problems in design and access for large-scale collaborative systems. Proceedings of the 1994 ACM conference on Computer supported cooperative work, 253–264.Sterne, J. A., Sutton, A. J., Ioannidis, J. P., Terrin, N., Jones, D. R., Lau, J., Carpenter, J., Rücker,G., Harbord, R. M., Schmid, C. H., et al. (2011). Recommendations for examining and interpreting funnel plot asymmetry in meta-analyses of randomised controlled trials. Bmj, 343.Strachan, J. W., Albergo, D., Borghini, G., Pansardi, O., Scaliti, E., Gupta, S., Saxena, K., Rufo, A., Panzeri, S., Manzi, G., et al. (2024). Testing theory of mind in large language models and humans. Nature Human Behaviour, 8(7), 1285–1295.Street, W. (2024). Llm theory of mind and alignment: Opportunities and risks. arXiv preprint arXiv:2405.08154.Strehl, A., & Ghosh, J. (2002). Cluster ensembles—a knowledge reuse framework for combining multiple partitions. Journal of machine learning research, 3(Dec), 583–617.Suchman, L. A. (1987). Plans and situated actions: The problem of human-machine communication. Cambridge university press.Torres, N., Ulloa, C., Araya, I., Ayala, M., & Jara, S. (2024). A comprehensive analysis of gender, racial, and prompt-induced biases in large language models. International Journal of Data Science and Analytics, 1–38.Ullman, T. (2023). Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399.van Es, K., & Nguyen, D. (2025). “your friendly ai assistant”: The anthropomorphic selfrepresentations of chatgpt and its implications for imagining ai. AI & SOCIETY, 40(5), 3591–3603.Vinh, N. X., Epps, J., & Bailey, J. (2009). Information theoretic measures for clusterings comparison: Is a correction for chance necessary? Proceedings of the 26th annual international conference on machine learning, 1073–1080.Vygotsky, L. S. (1978). Mind in society: The development of higher psychological processes (Vol. 86). Harvard university press.Wan, Q., Hu, S., Zhang, Y., Wang, P., Wen, B., & Lu, Z. (2024). " it felt like having a second mind": Investigating human-ai co-creativity in prewriting with large language models. Proceedings of the ACM on human-computer interaction, 8(CSCW1), 1–26.Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et al. (2024). A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6), 186345.Wang, Y., Huang, H., Rudin, C., & Shaposhnik, Y. (2021). Understanding how dimension reduction tools work: An empirical approach to deciphering t-sne, umap, trimap, and pacmap for data visualization. Journal of Machine Learning Research, 22(201), 1–73.Weiszfeld, E., & Plastria, F. (2009). On the point for which the sum of the distances to n given points is minimum. Annals of Operations Research, 167(1), 7–41.Winner, L. (2017). Do artifacts have politics? In Computer ethics (pp. 177–192). Routledge.Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. (2025). The rise and potential of large language model based agents: A survey.Science China Information Sciences, 68(2), 121101.Xu, R., Sun, Y., Ren, M., Guo, S., Pan, R., Lin, H., Sun, L., & Han, X. (2024). Ai for social science and social science of ai: A survey. Information Processing & Management, 61(3), 103665.Yan, L., Sha, L., Zhao, L., Li, Y., Martinez-Maldonado, R., Chen, G., Li, X., Jin, Y., & Ga?evi?, D. (2024). Practical and ethical challenges of large language models in education: A systematic scoping review. British Journal of Educational Technology, 55(1), 90–112.Ye, Y., You, H., & Du, J. (2023). Improved trust in human-robot collaboration with chatgpt. IEEE Access, 11, 55748–55754.Yoo, B., & Kim, K.-J. (2024). Finding deceivers in social context with large language models and how to find them: The case of the mafia game. Scientific Reports, 14(1), 30946.Zhao, Q., Wang, J., Zhang, Y., Jin, Y., Zhu, K., Chen, H., & Xie, X. (2023). Competeai: Understanding the competition dynamics in large language model-based agents. arXiv preprint arXiv:2310.17512.Zhao, S., Hong, M., Liu, Y., Hazarika, D., & Lin, K. (2025). Do llms recognize your preferences? evaluating personalized preference following in llms. arXiv preprint arXiv:2502.09597.Zhou, Z., Wang, J. Y., Sukiennik, N., Gao, C., Xu, F., Li, Y., & Evans, J. (2025). Rationality check! benchmarking the rationality of large language models. arXiv preprint arXiv:2509.14546. Zuboff, S. (2023). The age of surveillance capitalism. In Social theory re-wired (pp. 203–213).Routledge.1 Corresponding author.     2 Internal validation and stability diagnostics; agreement statistics against expert labels; and topic-model selection details are provided in Section 2 and Section 7.------------------------------------------------------------------------------------------------------------------------------------------------------111