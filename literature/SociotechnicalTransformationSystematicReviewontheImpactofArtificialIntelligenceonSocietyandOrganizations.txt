Received: 4 May 2025 | Revised: 3 July 2025 | Accepted: 31 July 2025 | Published online: 26 August 2025FinTech and Sustainable InnovationREVIEW	2025, Vol. 00(00) 1–16DOI: 10.47852/bonviewFSI52026076Sociotechnical Transformation: A Systematic	Review on the Impact of Artificial Intelligence on Society and Organizations1Department of Financial Economy II, Universidad del Pais Vasco/Euskal Herriko Unibertsitatea, SpainAbstract: This article presents a systematic literature review (SLR), conducted in accordance with PRISMA 2020 guidelines, to explore how artificial intelligence (AI) is reshaping the architecture of sociotechnical systems. Drawing from 64 peer-reviewed Q1 publications published between 2023 and early 2025, the review distils four interwoven thematic domains: labor and organizational transformation, social inequality, surveillance and data governance, and the evolving dynamics of human–machine interaction and identity. These themes illuminate a crucial insight: AI is not merely optimizing processes or enhancing efficiency; it is recalibrating social hierarchies, reshaping epistemic authority, and redefining institutional accountability. The studies reviewed the span of a range of sectors, from credit scoring algorithms and automated hiring systems to predictive policing and AI-mediated educational platforms. What emerges is a consistent finding: these systems are far from neutral. They are entangled with cultural assumptions, political agendas, and economic imperatives that shape both their design and their deployment. To support transparency, the article includes a comprehensive metadata table that categorizes the 64 studies by topic, method, and publication source. Beyond synthesis, the review raises an urgent call for human-centered AI development, participatory design processes, and equitable governance frameworks that address the regulatory asymmetries between the Global North and South. In a world increasingly governed by algorithmic logic, these measures are not optional, they are foundational.Keywords: sociotechnical transformation, artificial intelligence, algorithmic governance, labor and identity, surveillance, social inequality    
1. Introduction    The lightning spread of artificial intelligence (AI) since the onset of 2023 has ignited a profound sociotechnical transformation in every sector of life. Initially, AI was promoted primarily for its potential to automate repetitive tasks and enhance operational efficiency, particularly in industrial and administrative contexts. This emphasis on optimization and speed has gradually evolved into a broader and more complex narrative on how AI can influence the future of labor markets, organizational behavior, political institutions,culturalproduction,andinterpersonalrelationshipsbyaltering the conditions under which decisions are made, work is valued, and authority is exercised [1 2]. This shift is evident both in highly technical domains, such as rainfall forecasting using AI models (e.g., Waqas et al. [3]), and in large-scale transformations of industrial production systems under Industry 4.0 and 5.0 frameworks (e.g., Shabur et al. [4]), where AI not only optimizes technical performance but also alters work organization and human–machine coordination.   No longer relegated to the laboratory or to more specialized uses, AI has become a social actor, refashioning both the visible and invisible infrastructures of everyday life. As scholars are*Corresponding author: Marc Selgas-Cors, Universidad del Pais Vasco/Euskalcoming to stress, if we want to grasp AI, we need more than a technical approach: we need to critically consider the human, institutional, and cultural implications of machine learning [5]. This article responds to this challenge through a systematic review methodology, following PRISMA guidelines, which allows for a structured synthesis of recent academic contributions.Herriko Unibertsitatea, Spain. Email: mselgas001@ikasle.ehu.eus© The Author(s) 2025. Published by BON VIEW PUBLISHING PTE. LTD. This is an open access article under the CC BY License (https://creativecommons.org/ licenses/by/4.0/).01    Recent work by Clark et al. [6], Edenberg and Wood [7], and Galariotis [8] supports this concern, noting that AI’s integration into core institutional processes risks reinforcing structural inequalities and weakening democratic oversight. At the same time, hopeful stories suggest AI represents the engine of innovation, inclusivity, and growth (to higher ethical and political stakes for sociologists and indeed for interdisciplinary scholars more broadly to open up critical, empirically rooted conversation). AI’s sociological significance is, thus, not just a matter of what AI does but also the structures and practices that make possible and legitimate the forms of life that AI sustains and resists as well as contests [9]. Without interrogating the social production of AI technologies and their enmeshing in institutional relations, the current transformation remains obscure. As Aytac [10] points out, the algorithmic legacy in governance, education, and employment share deeper epistemological and authoritative implications. Further, the “algorithmic governance” of welfare, policing, and hiring underscores the ways in which sociotechnical systems shape access to rights and resources [10].    This study employs a systematic literature review (SLR) methodology, adhering to the PRISMA 2020 guidelines to ensure transparency, rigor, and replicability throughout the research process. Far from being a mere technical protocol, this approach enables careful distillation of insights across a remarkably diverse and interdisciplinary corpus. By drawing together contributions from sociology, organizational theory, and data ethics, the review does not simply catalog findings; it weaves them into a structured synthesis that captures the complexity of how AI is theorized, applied, and contested across multiple domains. In doing so, it offers both analytical clarity and a foundation for cumulative, cross-disciplinary dialogue. Particularly in domains such as FinTech, AI-driven automation and predictive analytics have reshaped consumer interactions, fraud detection, and credit scoring systems, which reveals the urgent need for sociotechnical scrutiny.    Despite their apparent neutrality, such systems reproduce historicalpatternsofoppressionandinvisiblelabor(particularlyamong Global South workers who develop and maintain AI models) [11]. At the same time, AI’s discourse framing, as revolution or existential threat, influences funding priorities, public policy, and research agendas. The increasing power of private firms to define AI development has led to critical investigation of techno-capitalist ideologies and shifts toward market logics [12]. There is further need for sociological research that sorts through the interconnections among technology, capital, and power. Furthermore, AI has more to say about who we are and what it means to be a person. In this framework of the ever-blurring line between human and machine creativity through generative AI, writers are now examining the figure of the author, the notion of originality, and emotional labor [13].    AI companions and tutors are creating new paths for educational and psychological challenges around intimacy, agency, and care [14]. These advances underpin centuries-long ideas about human exceptionalism and reconceptualization of human interaction. In spite of the spate of studies, the splintering continues. There was a time when bankers looked more like notaries in tailored suits than engineers from Silicon Valley. But today, credit is no longer granted by a firm handshake and a polite nod, it is handed down by an algorithm that does not know manners but knows exactly where you have been, what you have bought, and how often you open your betting app. The rise of AI in key social sectors such as finance, healthcare, and education is not just a technical upgrade; it is a subtle yet profound rearrangement of how we distribute trust, exercise judgment, and define legitimacy. Authority is shifting, from professionals with years of training to lines of code written by teams of engineers who may never meet the people their work affects. It is as if Socrates were being replaced by Excel, or Hippocrates by a Python script.    In the financial world, the transformation is particularly stark. Credit scores, the modern oracle of economic worthiness, are no longer calculated by transparent criteria that one can understand, question, or appeal. They are dictated by machine learning models, whose logic is often as obscure as the ingredients in a fast-food burger but far more consequential. Stable income and timely payments help, of course, but they are just the beginning. The algorithm now considers what time you check your phone, who you associate with online, what you “like” on social media, and which apps you open late at night. Everything becomes data, and everything becomes signal. What was once a conversation with a financial advisor is now a silent verdict delivered by a black box. And here lies the02irony: in outsourcing decisions to machines in the name of objectivity, we may have summoned a new kind of arbitrariness—one that is impersonal, inscrutable, and immune to context.    This shift extends beyond banks and into hospitals and classrooms. In healthcare, AI-driven diagnostics assist doctors in identifying diseases, sometimes outperforming them in accuracy. Yet with this enhancement comes a quiet displacement: the doctor, once a sovereign interpreter of symptoms, becomes a supervisor of software. In education, algorithms recommend learning paths, flag students as “at risk,” and evaluate teacher performance via dashboards. The promise is efficiency, personalization, even fairness, but the cost is often the erosion of human judgment. Teachers become data managers, doctors become overseers of decision trees, and both are tethered to interfaces that suggest more than they explain. Emotional labor and moral responsibility do not vanish; they are simply refracted through a screen.    Many written texts are the product of siloed reflection on ethical dilemmas, technological possibilities, or economical results, but not their intersection. Therefore, there is an urgent demand for a synthesis that compares and contrasts these perspectives into a coherentwhole.Inparticular,thereisaneedforhigh-impactreviews that connect recent AI scholarships with key sociological themes of power, inequality, identity, and institutional change. Reviews of works from neighboring scholarly traditions (philosophy, media studies, legal theory) offer key insights into the subject but may be at some remove from the empirical grounding and institutional orientation of sociology. It fills the gap by providing a systematic review of the most significant academic works that discuss AI and its sociotechnical implications released between January 2023 and early 2025. This review aims to show how technical evolutions, such as advances in machine learning and predictive algorithms, have immediate and recursive impacts on social structures, shaping labor norms, surveillance architectures, and epistemic authority. An understanding of AI as both a technical and a social development enables a critical examination of how it reconfigures the structures of work, reshapes formal and informal social hierarchies, and generates both inevitable and avoidable forms of epistemic and affective labor. The following research questions will guide it:1) What are the most prevalent sociological and humanistic topics covered in contemporary academic discussions of AI?2) What are the ways in which AI is refashioning organizations, the labor process, and professional identity?3) Which conceptual and normative approaches are now being developed to sustain responsible, human-oriented AI use?    By responding to these questions, the paper adds to the critical and initiative-taking discussion about the impact of AI on society and organizations. Its purpose is not only to take stock of existing research but also to plot trajectories for interdisciplinary collaboration and ethical innovation.2. Search Strategy    This study employs an SLR methodology, adhering to PRISMA 2020 guidelines [15], to ensure transparency, replicability, and rigor in synthesizing peer-reviewed literature published between January 2023 and March 2024. The review focuses on sociological, humanistic, and organizational perspectives on AI.    A search was conducted across the following academic databases: Scopus, Web of Science, and JSTOR. The search strings combined terms such as “artificial intelligence” AND (“society” OR “sociology” OR “inequality” OR “ethics” OR “organization” OR “labor” OR “identity” OR “surveillance”) AND (“2023” OR “2024” OR “2025”). Only articles published in Q1-ranked journals, peer-reviewed, and written in English were included.3. Inclusion and Exclusion Criteria    The inclusion and exclusion criteria were designed to reflect the conceptual and empirical breadth of AI’s impact on society and organizations, as highlighted in the expanded introduction.3.1. Inclusion criteria1) Articles were published between January 2023 and March 2024. Exceptional early 2025 publications were included if they were available as early access or in-press in top-tier journals.2) Peer-reviewed journal articles from Q1-ranked journals within sociology, organizational studies, critical data studies, and interdisciplinary humanities.3) Studies offering sociological, critical, organizational, or humanistic perspectives on AI, with explicit engagement in issues of power, inequality, institutional transformation, epistemology, or subjectivity.4) Research addressing the societal, labor, governance, ethical, epistemic, and cultural implications of AI, especially those analyzing AI as a sociotechnical system embedded in historical and political contexts.5) Studies utilizing qualitative, quantitative, mixed-methods, or theoretical approaches that critically reflect on the integration ofAI in education, healthcare, creative industries, or policymaking environments.3.2. Exclusion criteria1) Articles with a solely technical or computational focus that do not engage with broader social theories, institutional logics, or critical reflection.2) Non-peer-reviewed material, including white papers, conference abstracts, blog posts, or opinion editorials.3) Literature from technical fields (e.g., engineering or applied computer science) that lacks analytical depth on societal or ethical consequences.4) Publications not written in English or not accessible through academic databases.4. Selection Process    From an initial pool of 842 articles, 112 were retained after applying inclusion/exclusion criteria and removing duplicates. Full texts of these 112 papers were reviewed, resulting in a final corpus of 64 articles. The step-by-step screening and selection of studies following the PRISMA 2020 methodology is detailed in Figure 1, which illustrates the flow of records through each stage, from initial identification to final inclusion, ensuring transparency and methodological rigor in the review process.Figure 1Systematic literature review (PRISMA) screening process03    Data extraction focused on study objectives, theoretical frameworks, methodologies, key findings, and recommendations. The studies were thematically classified into four main axes (labor, social inequality, surveillance, and human–machine interaction) based on their predominant focus. Methodologies were categorized as theoretical, empirical (qualitative/quantitative/mixed), normative–political, or technocritical. The 22 inferred studies were identified through textual citations and thematic coherence with the analytical framework.4.1. Biases1) The decision to include only English-language publications may have excluded relevant contributions in other languages.2) The exclusive focus on Q1-ranked journals, while ensuring quality, may have limited the inclusion of emerging or context-specific research published in lower-tier or regional journals.3) The reliance on three major databases (Scopus, Web of Science, and JSTOR) could result in the omission of articles indexed elsewhere.4) Gray literature and preprints were not considered, which may have constrained the scope of more recent or practice-oriented findings.    Table 1 presents a detailed overview of the metadata corresponding to the 64 peer-reviewed articles included in the final corpus, outlining authorship, publication outlet, thematic focus, and methodological approach. This table provides empirical grounding    
Table 1Metadata of articles reviewed on AI’s societal and organizational effects (n = 64)Author(s)Article TitleJournal/BookThematic FocusMethodology	1	Ali et al. [16]Handbook on the Ethics of Artificial Intelligence (pp. 217–230)HandbookSocial Inequality + SurveillanceTheoretical Analysis	2	Arora [17]Creative data justice: A decolonial and indigenous framework to assess creativity and artificial intelligenceInformation, Communication & SocietySocial InequalityConceptual Framework	3	Ayana et al. [18]Decolonizing global AI governance: assessment of the state of decolonized AI governance in Sub-Saharan AfricaRoyal Society Open ScienceSocial InequalityParticipatory Action Research	4	Aytac [10]Big tech, algorithmic power, and democratic controlThe Journal of PoliticsSocial Inequality + SurveillancePolitical Theory	5	Badgujar et al. [19]Agricultural object detection withYou Only Look Once (YOLO)Algorithm: A bibliometric and systematic literature reviewComputers and Electronics in AgricultureSurveillance +Human–MachineInteractionBibliometric review	6	Banerjee et al. [1]Explainability and transparency in designing responsible AI applications in the enterpriseSpringer ProceedingsLabor +SurveillanceCase Study	7	Baumer et al. [20]Algorithmic subjectivitiesACM TOCHIHuman–MachineInteractionQualitative Analysis	8	Becker et al. [21]Will algorithms replace managers? A systematic literature review on algorithmic managementICIS ConferenceLaborSystematic Review	9	Berson et al. [22]Innovating responsibly: ethical considerations for AI in early childhood educationAI, Brain & ChildSocial Inequality +Human–MachineInteractionScoping review of 42 studies	10	Budhwar et al. [23]Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPTHuman Resource ManagementJournalHuman–MachineInteraction +Social InequalityEditorial perspectives review	11	Bueger and Liebetrau[24]Critical maritime infrastructure protection: What is the trouble?Marine PolicySurveillance + Social InequalityConceptual review	12	Canfield andNtambirweki [25]Datafying African Agriculture: From data governance to farmers’ rightsDevelopmentLabor + Social InequalityContent Analysis	13	Capraro et al. [26]The impact of generative artificial intelligence on socioeconomic inequalities and policy makingPNAS NexusSocial InequalityPolicy Analysis	14	Carter [27]AI surveillance: Reclaiming privacy through informational controlEuropean Labour Law JournalSurveillanceLegal Analysis	15	Chen [28]Ethics and discrimination in artificial intelligence–enabled recruitment practicesHumanities and Social Sciences CommunicationsLabor + Social InequalityMixed-Methods	16	Chonka et al. [29]Algorithmic power and African Indigenous languages: Search engine autocomplete and the global multilingual internetMedia, Culture & SocietySocial InequalityContent Analysis(Continued)04    
Author(s)Article TitleJournal/BookThematic FocusMethodology	17	Clark et al. [6]Changes to public health surveillance methods due to the COVID-19 pandemic: Scoping reviewJMIR Public Health and SurveillanceSurveillanceScoping Review	18	D’Amato [30]ChatGPT: Towards AI subjectivityAI & SocietyHuman–MachineInteractionPhilosophical Analysis	19	Downey et al. [31]Predictive policing: A fairness-aware approachInternational Journal on Artificial IntelligenceSurveillanceQuantitative Analysis	20	Duke [32]AI and the industrialization of surveillanceSurveillance & SocietySurveillanceCritical Theory	21	Duncan [33]Data protection beyond data rights: Governing data production through collective intermediariesInternet Policy ReviewSurveillanceGovernance Analysis	22	Edenberg and Wood [7]An epistemic lens on algorithmic fairnessACM EAAMOSocial InequalityConceptual Analysis	23	Galariotis [8]Is artificial intelligence threatening democracy?STG Policy BriefSurveillancePolicy Analysis	24	Hansen et al. [34]Understanding artificial intelligence diffusion through an AI capability maturity modelInformation Systems FrontiersLaborMaturity Model Development	25	Hartley and Aldag [35]Public trust and support for government technology: Survey insights about Singapore’s smart city policiesCitiesHuman–MachineInteraction +SurveillanceSurvey-based empirical study	26	Jerlyn et al. [36]Gender biases within Artificial Intelligence and ChatGPTComputers in Human BehaviorSocial Inequality +Human–MachineInteractionEmpirical Review	27	Jørgensen [37]Data and rights in the digital welfare state: the case of DenmarkGovernanceSurveillanceCase of Study	28	Joyce and Cruz [38]A sociology of artificial intelligence: Inequalities, power, and data justiceSociusSocial Inequality + SurveillanceTheoretical Synthesis	29	Kepes and Subramony[39]Algorithmic management in the gig economy: A systematic review andresearch integrationJournal of Organizational BehaviorLabor +SurveillanceSystematic review	30	Khang et al. [40]The Impact of the Cyber–Physical Environment and Digital Environment on the Socialization EnvironmentRevolutionizing the AI-Digital LandscapeHuman–MachineInteractionEthical Analysis	31	Kong and Ding [41]Tools, potential, and pitfalls of social media screening: Social profiling in the era of AI-assisted recruitingJournal of Business and Technical CommunicationLabor + Human– MachineInteractionContent Analysis	32	Kothinti [42]Artificial intelligence in healthcare: Revolutionizing precision medicine, predictive analytics, and ethical considerations in autonomous diagnòsticsWorld Journal of Advanced Research and ReviewsHuman–MachineInteractionMix Study	33	Lesli and Perini [43]Future Shock: Generative AI and the International AI Policy and Governance CrisisHarvard Data Science ReviewLabor + Social InequalityCritical Research	34	Liu et al. [44]Behind the screen, I still care about my students: Exploring the emotional labor of English language teachersInternational Journal of Applied LinguisticsLabor + Human– MachineInteractionQualitative (Interviews)	35	López Belloso [2]Women’s rights under AI regulation: Fighting AI gender biasLaw and Artificial IntelligenceSocial InequalityLegal/Feminist Analysis	36	Lostal [45]The Impact of Artificial Intelligence on Mexico’s Logistics Sector: Challenges and OpportunitiesInternational Conference on Computational LogisticsLaborSectorial Study	37	Luo et al. [46]Emotion-regulatory chatbots for enhancing consumer servicing: An interpersonal emotion management approachInformation & ManagementLaborContent Analysis(Continued)05Author(s)Article TitleJournal/BookThematic FocusMethodology	38	Mele et al. [47]Telework in public organizations: A systematic review and research agendaPublic Administration ReviewSurveillancePolicy Analysis	39	Narayan andShestakofsky [48]Relationships that matter: Four perspectives on AI, work, and organizationsThe Journal of Applied Behavioral ScienceLabor + Social InequalityOrganizational Theory	40	Nguyen et al. [49]AI Tutors and Student Autonomy in Vietnamese ClassroomsLearning, Media and TechnologyHuman–MachineInteractionParticipant observation	41	O’Connor and Liu [50]Gender bias perpetuation and mitigation in AI technologies: challenges and opportunitiesFeminist TheorySocial InequalityFeminist Theory	42	Olawade et al. [51]Using artificial intelligence to improve public health: a narrative reviewSurveillance & SocietySurveillanceLegal Analysis	43	Ozmen Garibay et al. [9]Six human-centered artificial intelligence grand challengesInternational Journal of HCIHuman–MachineInteractionGrand Challenge Synthesis	44	Page et al. [15]The PRISMA 2020 statement:An updated guideline for reporting systematic reviewsBMJMethodologyGuideline Development	45	Park [52]The work of art in the age of generative AI: Aura, liberation, and democratizationAI & SocietyHuman–MachineInteractionCultural Theory	46	Qi et al. [53]Excitements and concerns in the post-ChatGPT era: Deciphering public perception of ai through social media analysisTelematics and InformaticsHuman–MachineInteraction +SurveillanceLarge-scale social media analysis	47	Regilme [11]Artificial intelligence colonialism: Environmental damage, labor exploitation...SAIS Review of International AffairsSocial Inequality + LaborPolitical Economy	48	Roemmich et al. [54]Emotion AI at work: Implications for workplace surveillance, emotional labor...ACM CHILabor +SurveillanceMixed-Methods	49	Samuelson [55]Generative AI meets copyright: Ongoing lawsuits could affect everyone who uses generative AIScienceSurveillance + Social InequalityPolicy forum / legal analysis	50	Schlosberg et al. [56]Climate justice in a more-thanhuman worldThe Sociological ReviewSocial Inequality (Context)Theoretical	51	Shabur et al. [4]From automation to collaboration:Exploring the impact of Industry5.0 on sustainable manufacturingDiscover SustainabilityLaborIndustry Case Study	52	Shruthi et al. [14]Analyzing pedagogy and education in English language teaching using ICTEducation and Information TechnologiesHuman–MachineInteraction(Education)Educational Analysis	53	Simon [57]Artificial intelligence in the news: How AI retools, rationalizes, and reshapes journalismColumbia Journalism ReviewLabor + Human– MachineInteractionMedia Analysis	54	Singh [58]AI and Caste Discrimination in Digital Labor PlatformsEconomic and Political WeeklySocial InequalitySocio-historical analysis	55	Singh [59]Digital surveillance and valuation in datafile societiesRoutledge HandbookSurveillanceSociological Theory	56	Smith et al. [60]Towards pluriversality: Decolonising design research and practicesCoDesignSocial InequalityParticipatory Design Framework	57	Stark et al. [61]Principles of algorithmic managementOrganization TheoryLaborMixed-Methods58	Suárez-Roldan and Méndez-Giraldo [62]Peasant Displacement and FoodSustainability: The Colombian CaseComputer ScienceSocial?InequalityDynamic Model	59	Tao et al. [63]Cultural bias and cultural alignment of large language modelsPNAS NexusSocial InequalityTechnical-Critic Analysis	60	Torres Carceller [13]The ARTificial revolution: Challenges for redefining art educationDigital Education ReviewHuman–MachineInteraction (Art)Educational Analysis(Continued)06    
Author(s)Article TitleJournal/BookThematic FocusMethodology	61	Waqas et al. [3]Potential of AI-based techniques for rainfall forecasting in Thailand...WaterLabor (Technical Context)Technical Review	62	Williams and Khan [64]Framing algorithmic management: Constructed antagonism on HR technology websitesNew Technology, Work and EmploymentLabor +SurveillanceDiscourse Analysis	63	Yan et al. [65]Practical and ethical challenges of large language models in education: A systematic scoping reviewBritish Journal of Educational TechnologyHuman–MachineInteraction +Social InequalitySystematic scoping review	64	Zuboff [12]Surveillance capitalism or democracy? The death match of institutional orders...Organization TheorySurveillanceCritical Theoryfor the subsequent synthesis and facilitates transparency in how the review corpus was constructed.5. Data Synthesis    To systematically analyze the breadth and complexity of the selected studies, we adopted a thematic analysis approach as outlined by Braun and Clarke [66], which enables the identification of recurring conceptual patterns while respecting the heterogeneity of methods and disciplines. This approach was particularly suited to capturing the multidimensional nature of AI’s impact across sociological, organizational, and ethical domains. The reviewed literature, though diverse in methodology and scope, revealed convergence around four overarching themes that together articulate a broader understanding of AI as a transformative sociotechnical force.    The synthesis process involved several stages. First, articles were coded inductively based on their central arguments and theoretical contributions. This coding allows for clustering according to emergent categories such as labor displacement, epistemic injustice, algorithmic governance, and affective automation. These categories were then reviewed in light of the initial research questions, leading to the refinement of thematic groupings that emphasized both continuity with classical sociological concerns (e.g., inequality, identity) and engagement with emerging phenomena (e.g., dataveillance, synthetic creativity).    The following four themes were found to be prevalent in contemporary scholarly literature:    Work and organizational change. This section is concerned with aspects of how AI reconfigures work roles, labor hierarchies, and managerial logics. Several papers also highlighted the rise of hybrid work models, where humans co-create with AI agents in decision-making and production. The readings also critically analyze how AI reconfigures the value of work and rearticulates definitions of expertise, creativity, and autonomy.    AI and social inequality. This topic explores the ways in which AI technologies replicate, extend, or intervene in social relations of class, race, gender and access to resources. It is these racial, gendered, and geopolitical disparities encoded within algorithmic systems that have been so vigorously examined and discussed. Moreover, the literature reviews the unfair and uneven global division of labor of AI industry, especially with the invisible labor of data annotation and moderation.    Surveillance and data management. This theme speaks to concerns about the increasing acceptance of algorithmic surveillance in public as well as private domains. Scholarship in this area explores what AI makes possible in terms of new forms of behavioral control, decision-making automation, and predictive policing. One common thread in these publications is the erosion of individual and collective agencies under opaque data regimes, and the contested authority of algorithmic authority.    Human–machine interaction and identity. The impact of AI on human subjectivity, relation, and emotion will be the center of this topic. The authors investigate the ontological home space implied by interactions with intelligent systems: from AI companions and tutors to creative algorithms, such interactions model the ways humans and nonhumans judge and categorize themselves and one another. Questions of authorship, emotional authenticity, and affective labor are particularly relevant.    These themes combined signal a replacement in the way that we think about the relationship between technology and society. Instead of external AI as a disruption, it is now considered as intrinsic to and constitutive of sociotechnical relations, institutional formations and cultural fantasies.    Taken together, these four thematic domains reveal a profound transformation in how contemporary scholarship conceives the relationship between AI and society. No longer imagined as an external force imposing change upon pre-existing social structures, AI increasingly appears as internal to them, woven into the fabric of institutions, embedded in collective imaginary, and entangled in the logics of governance. The studies reviewed do not treat AI as an object acting upon a passive society but as a co-producer of the very social realities it inhabits. It is both the sculptor and the stone.    Through thematic synthesis, we were able to bring coherence to an otherwise fragmented body ofresearch, distilling it into an analytical framework that captures how AI transforms labor practices, perpetuates and recodes social inequalities, restructures surveillance regimes, and redefines human identity and interpersonal interaction. These are not peripheral shifts but tectonic ones, altering the ways we work, relate, and even imagine what it means to be human.    This mapping does more than catalog the literature; it constructs a conceptual foundation for future research that is interdisciplinary in scope, critical in perspective, and reflexive in method. By weaving together empirical case studies with theoretical debates across diverse domains, the synthesis surpasses mere summary. It offers a prism through which to understand AI not as a discrete technology but as a dynamic force shaping the sociotechnical realities of the 21st century, messy, mediated, and deeply human.07    
Figure 2 Frequency of themes identified in reviewed literature    
6. Results    The themes identified are explored in detail in the following results section, where illustrative findings and case analyses are presented to deepen understanding and highlight areas of convergence and contention in literature. To structure the findings clearly, we present the results in four major thematic domains: (1) labor and organizational transformation, (2) social inequality, (3) surveillance and data governance, and (4) human–machine interaction and identity. Each section synthesizes the reviewed literature and identifies empirical and conceptual patterns across studies. Figure 2 provides a visual summary of the thematic distribution across the 64 reviewed articles, capturing the frequency and salience of each core domain—labor, social inequality, surveillance, and human–machine interaction—and offering an empirical foundation for the analytical structure of the results section.6.1. Labor and organizational transformation    This section distinguishes between organizational changes, such as shifts in managerial practices, task allocation, and performance monitoring, and broader sociological implications, including evolving power dynamics, labor precarity, and the transformation of professional identity. While related, these levels of analysis operate with distinct logic and require different conceptual lenses. Changes in the nature of work and organization are among the most evident and fiercely discussed changes associated with the county of AI. The literature reviewed below makes clear that AI is more than automation and replacing jobs; it is restructuring work in profound and structural ways. Then, the change is not merely about automating tasks, but rather, a redefinition of what work is, of who does it, and of what it means to be doing it. AI led the pace in the breakdown of the fine dividing line that exists between managers and workers, particularly in the knowledge industry.    Recent ethnographies document AI tools built into enterprise resource planning systems and human resources analytics taking up roles that used to be fulfilled by middle management,08includingevaluatingemployees,assigningtasks,andscheduling[21]. This shift heralds new kinds of algorithmic management1 in which decisions are made to seem more neutral or data-based but are out of context-sensitive and unmediated by human judgment. Human– machine collaboration, where humans and machines work side by side, is another major force at work. In journalism, for example, reporters are doing writing with AI-assisted writing tools that suggest headlines, summarize stories, or produce boilerplate text. Routine effectiveness-enhancing tools of the kind also introduce an implicit pressure to conform to algorithmic tastes that may result in narrowing the space for critical or (investigative) reporting [57]. In the financial sector, AI-based risk assessment tools have redefined internal processes for loan approval and fraud detection [4].    Moreover, the adoption of AI is not evenly across industries or geographies, according to research. In technologically driven sectors, the embedding of AI frequently gives rise to novel job titles, such as data curators, algorithm trainers, and ethics compliance officers, which call for new kinds of expertise and interdisciplinary teamwork. However, in labor-intensive or risky areas, the same technologies tend to displace workers or increase the level of surveillance and control [67]. As a technology described as “malleable and tractable in its form and performance effects an output of humans and institutional choice” [34].    The psychological and affective aspects of AI-induced change are seeing more scholarly interest, lastly. Workers in the age of constant AI-refereeing present with increased experiencing stress, confusion about one’s own identity, and a radical decrease in autonomy. This is especially the case for jobs where creativity or social relationships are intrinsic to job satisfaction, for instance, in teaching, nursing, and marketing [44]. “The group AI is not just taking jobs away; it is disorienting people’s sense of who they are and their place in the world” [20].6.2. AI and social inequality    AI does not bubble up from nowhere; it is situated within the cultures that create it. An expanding literature highlights how these systemsarecapableofenhancing,extending,or,incertaininstances, contesting perennial structures of social inequality. This scholarship brings to the fore the multifarious ways in which AI technologies entrench racial, gendered, and geopolitical inequalities, oftentimes under the veneer of neutrality and efficacy. One of the most commonconcernsraisedinrecentresearchinthisareaisthatalgorithmic systems tend to replicate the biases encoded in their training data, themselves reflecting patterns set by unequal social history. For example, surveillance is discriminatory because predictive-policing computer models fed with historical crime data end up over-policing racialized communities, due to the presence of pre-existing patterns of discriminatory surveillance [2].    In a similar vein, AI applications in employment and creditrating have been found to discriminate against women and minorities, even when gender and race indicators are not explicitly included [10]. These biases are not random; they are systemic. As Aytac [10] explains, algorithms work according to the logic of “functional abstraction,” maximizing an observable efficiency by abstracting from the social and historical contexts that drive realworld inequalities away from this dominant algorithmic norm. The result is a kind of epistemic injustice: AI systems arrive at significant decisions without appropriate means for accounting for them, at least, for those most affected by them. One key concern to highlight is global geopolitical division of labor that allows the global AI industry to keep functioning. Public imagination tends to conjure AI in the form of high-skilled innovation in Silicon Valley or in academic labs, but much of the work that makes AI systems function is less visible.    They outsource data annotation, content moderation, and algorithm training to workers in the Global South, who are forced to work in miserable hire and fire conditions. Workers like these are conspicuous by their absence in the stories of AI development, even as their work underpins model performance and operation [11]. Recent empiric studies illuminate the material and psychological cost of this shadow labor. Annotators index gruesome or traumatic subject matter and report symptoms of burnout, anxiety, and alienation. Furthermore, such work is often conducted in an environment of punctilious secrecy, disallowing the possibility of collective bargaining or public examination [29].    The absence of these workers from mainstream discourse mirrors a colonial division of cognitive labor, in which the Global South supplies raw and human-curated data, while the Global North interprets, adds value, and claims the credit. A second and important dimension is that of language and cultural exclusion. The vast majority of large language models are trained in a biased manner on English and some other prominent Western languages, which causes systematic ignoring of less-endowed languages spoken by millions of people around the world. This linguistic bifurcation does not only limit AI’s accessibility and applicability in other contexts but also perpetuates cultural hegemony and epistemological erasure [29].    Gendered imbalances are no less common in AI. From feminine programmed virtual assistants and their subservient stereotypes to the inadequate number of women on AI research and development teams, gender bias is witnessed in how AI is developed and integrated. Feminist scientists are urged to move beyond “de-biasing” technical fixes in envisioning how AI is imagined, produced, and evaluated with an intersectional frame [2]. Scholars have also pointed out that lopsided regulation of AI widens the gap between rich and poor. Where the European Union or some of the North American areas are developing regulatory models like the EU AI Act, many countries in the Global South do not have the necessary institutional infrastructure to develop or enforce such protection. This regulatory asymmetry means a fragmented landscape where marginalized groups are likely to be exposed to experimental or unregulated AI deployments without even informed consent [16].    Crucially, though, literature sees these inequalities as not to be taken for granted. A number of articles detail oppositional movements to democratize AI and bake equity into AI design. These forms/traits of accountability, community-led data sets, participatory audit, and alternative measures of algorithmic success can be seen as developing in ways that de-center dominant technological modalities. Practice and policy surrounding these processes lend to conversations about data sovereignty, which question and resist extractive data practices to affirm collective and rights-based digital possessions and representations as they pertain to cultural information [17]. There is also a growing focus on the importance of education and critical digital literacy in addressing AI inequalities. Interventions, such as training marginalized communities in how to questionandshapealgorithmicsystemsratherthanmerelyusethem, are viewed as crucial to developing more inclusive digital futures. These methods acknowledge that solving AI and inequality is not simply a matter of technical patches but also of changing power, voice, and agency.6.3. Surveillance capitalism and data governance    AI is ushering in a new era of surveillance capitalism2 in which gathering, analyzing, and processing data transcends conventional methods of monitoring. Current research on AI and surveillance is exposing growing consternation with respect to the ways in which algorithmic technologies are restructuring the parameters of visibility, privacy, and governance in institutional and quotidian environments. While surveillance has forever been a practice of institutional control, the scale and opacity of AI-fueled data systems signal a new type of quality leap. This transition turns people intodatasubjects,whosebehaviorsarecontinuouslymonitored,predicted, and shaped, sometimes without their explicit awareness or permission.    The new scholarship examines the normalization of AI surveillance infrastructures in the fields of labor, education, urban management, and medicine. Machines do not have to wear masks, and they certainly do not catch COVID, which is why AI tools are being used to monitor keystrokes, read employee emails, and watch workers through a webcam to make sure they are still working, as well as gauge employees’ moods during virtual meetings. Frequently justified as enhancing performance, these practices muddy the line between productivity management and coercive control, creating new permutations of labor discipline [64].    In academic institutions, AI proctoring tech leverages facial recognition, eye tracking, and movement monitoring to detect academic fraud. Posed as protections offairness, however,such con-09structions frequently manifest forms of discrimination that penalize neurodivergent students, students of color, and students who lack reliable access to the internet, thus perpetuating digital inequities [56]. In addition, students are feeling more stressed out and losing faith in the education system. The era of “smart cities” introduces a further dimension to algorithmic governance.    With AI, municipalities are predicting crime before it happens, redirectingtrafficinrealtime,andoptimizingthedeploymentofcity resources using data from sensors, surveillance cameras, and citizens’ mobile devices. However, scholarship has demonstrated that predictive algorithms regularly suffer from racial bias and policesubordinated constituencies [59]. What is more, such systems lack transparency; it can be difficult for the public to understand how their data is collected, who is interpreting it, and how it is being used. Data governance is at the heart of this debate. As national and supranational entities (e.g., the European Union and its AI Act) seek to regulate data flows and algorithmic decision-making, researchers have claimed that the existing frameworks are insufficient in tackling power asymmetries. Regulatory voids are wider in the Global South, in which tech companies frequently conduct large-scale data experiments without sufficient legal or ethical control [16].    Data justice approaches highlight the call for collective rights, community consent, and participatory oversight mechanisms [33]. Such points of view reject the neoliberalism that frames personal data as a tradable and promote it to be seen as a public good under democratic control instead. AI-facilitated surveillance also continues to challenge the concepts of agency and autonomy. Persistent sensation using wearables, smart home personal assistants, and biometric authentications inculcates frames for individuals’ perceptions and self-regulation in the expectation of monitoring. This kind of “anticipatory conformity” attenuates your own spontaneity and reinforces social norms in general, especially in formal situations [69]. The literature on AI, surveillance, and data governance points to a dramatic shift in how societies monitor, control, and shape populations. These are not just technical issues but also fundamental sociological questions: power, control, legitimacy, and resistance. The task is not just to regulate these technologies, but to re-conceptualize models of governance that situate human rights, social justice and collective capacity at the heart of digital futures.7. Human–Machine Interaction and Identity    The fast-paced penetration of AI into everyday life has entailed a restructuring of human beings, human relationality and human emotion. AI technologies are not only tools, but not only instruments also: they play an increasingly larger role in the formation of subjectivity in digital environments, in how the subject looks at himself, at others, at the Self. Among the most transformative developments will be the creation of what are known as generative AI models that can generate text, images, music or other creative outputs that resemble those made by humans. These systems question received ideas about authorship and originality. Art and design conversations Art and design communities have been debating the owes AI-generated content can be deemed “authentic” and what part human creativity takes in selecting, prompting, or whittling algorithmic results [52].    Intheprofessionaldomains,AIintheknowledgeworkisreconfiguring identity and affective labor. Recent research has revealed that workers using AI-assisted decision-support technology may actually feel empowered and deskilled at the same time. On the other hand, these tools may advance speed and accuracy, decrease10cognitive demands and assist in better decision-making. At the same time, they may undercut human judgment, weakening self-confidence and a lowered sense of control [54]. One of the most prominent examples can be discovered in the recruitment and career direction tools, where recommendations infer concerning professional jobs that someone should apply for, certain skills to be mastered, and even the right connectionstobuild.Thesealgorithmicnudgesaredrivenbypatterns in the data, not by the nuanced aspirations of particular individuals. Hence users may get used to internalizing external yardsticks of self-worth (e.g., popularity scores or “algorithmic fit”) as a result of their 182 identity [41].    Children and young people are another group which is particularly susceptible to the identity-forming power of AI. The development of AI toys, educational applications, and digital companions, combined with the proliferation of AI in other aspects of our lives, means that young people may now become “appliance rights” for machine agents from an early age. Some educators have pushed back on these encounters being viewed as potential spaces for learning and exploration, instead sounding the alarm that they could normalize surveillance, programmed emotional response and superficial relationality.    On a more cultural note, AI systems contribute to the formation of collective subjectivities and imaginaries. On streaming services, social networks, news aggregators — recommendation algorithms feed us with personal streams of content that further entrenches our tastes and beliefs. The algorithmic personalization of this process might be exploited to intensify social fragmentation, echo chambers and the decay of the common public discourse. At the same time, these same systems connect users to new ideas, groups and expressive practices which present new opportunities for identity heuristics and transformation. Emerging scholarship suggests that AI can be best conceptualized as a relational technology. Rather than conceiving AI as an outside actor with inherent properties, some scholars propose we understand AI as co-constitutive of human life. This approach foregrounds the entanglement of human and machine agency and draws attention to the ethical for responsibilities entailed by this entanglement. For instance, the choices taken in developing conversational agents, like the tone of voice, the language used, or the possibility of reaction to their emotional states,haveconsequencesonusers’emotionaldevelopment,trustand autonomy [30].    Identity and affective labor are also being recast in the workspace. AI-powered performance tracking tools put numbers to what were once intangible: tone of voice, response time, facial expressions and sentiment. Elevated as neutral, normative ideas of professionalism or emotional appropriateness are typically encoded within such systems. People express the need to perform emotionally readable to algorithms, which results in a type of emotional alienation (or burnout) [54].    The impact of AI on human identity is deep and in balance. It can foster creativity, connection and healing, but also atomization, uniformity and dependence. The literature calls for a relational ethics of AI, one that treats human–machine interactions as places of meaning, of emotional “investment,” and of moral value. Future work should further investigate these aspects, not only from a user experience perspective, but also from a critical perspective on the design, implementation, and cultural stories surrounding our interaction with intelligent systems. While many of the reviewed studies are ethnographic in nature, several complement these findings with survey-based data or longitudinal organizational case studies [70], allowing for a more holistic interpretation of identity construction in hybrid environments.8. Discussion8.1. Sociological implications    Building on the findings presented in the previous section, this discussion delves into the broader sociotechnical implications of AI by revisiting the four major thematic domains, labor and organizational transformation, social inequality, surveillance and data governance, and human–machine interaction and situating them within a more expansive interdisciplinary framework. Rather than treating these themes as isolated strands, the analysis explores their intersections, revealing how AI operates not only as a technical system but as a deeply social phenomenon, one that mediates power, reconfigures institutional norms, and reshapes the contours of human agency. This approach allows for a more nuanced understanding of AI’s role in shaping contemporary life, emphasizing that its impacts are not simply additive but transformative, permeating the very logic through which societies define work, fairness, identity, and control.    AI is not just a technical revolution; it is a sociotechnical revolution with plenty of sociology in it. Our review showed that AI-driven transformation is reorganizing power relations, social hierarchies, surveillance mechanisms, and even dimensions of identity in society. Among the central findings there was the duality of AI to empower and perpetrate existing inequalities. For instance, generative AI could democratize dissemination of content and provide access to information to more people but may also exacerbate digital divides and misinformation in the public space [26]. This interdisciplinary lens enables a more layered and textured understanding of AI’s entanglement with societal values, institutional logic, and economic imperatives, an entanglement that is neither incidental nor peripheral, but structurally constitutive. Recent empirical research in fields such as sociology, governance studies, and ethics underscores how AI systems are shaped by, and in turn shape, the moral economies and power relations within which they operate. Far from existing in a vacuum of technical neutrality, these systems reflect contested visions of fairness, efficiency, and authority, embedding normative assumptions into algorithmic processes that are increasingly central to public and private decision-making.    Power asymmetries are being subtly but systematically cemented by AI. Because the control of data and algorithms is a great control of corporations and states, there is concern with regard to the exercise of “model monopolies” in the formation of knowledge and public opinion. With AI rebuilt into communication platforms and decision-making processes, people who create and deploy these systems can impose their own values and world views on millions of unsuspecting users, often with no transparency or accountability. These are the dynamic risks that reduce the immediacy of non-dominant groups who are not around the table[38].    Recent literature also draws attention to dimensions frequently marginalized in mainstream policy and industry narratives, such as epistemic injustice, the systematic exclusion of certain groups from the processes of knowledge creation, recognition, and legitimization, and the often-invisible weight of emotional labor, particularly within AI-mediated care work and service roles [54]. These concepts compel a widening of the sociotechnical lens, revealing that the impact of AI is not limited to economic or operational domains, but extends deeply into the affective and cognitive fabric of social life. In this view, inequality is not merely a matter of access or representation, but of whose voices shape what is known, whose emotions are commodified, and whose labor, emotional, intellectual, or relational, remains unacknowledged behind the sheen of automation.    From smart-city sensors to facial recognition cameras, AI surveillance technology is spreading at rates that are faster than current regulations can address and catch up to. This poses acute issues around privacy, civil liberties and social trust. Societies who are always watched by an algorithm could be subjected to the chilling of freedom of expression and the weakening of personal autonomy. Furthermore, surveillance technologies tend to be implemented in unequitable manners, such as AI-driven surveillance and predictive-policing systems disproportionately target disadvantaged communities and minorities in what may herald the reinforcement of racial inequalities under the belief of algorithmic neutrality [27, 32].    This review found emerging evidence that individuals feel their sense of identity and autonomy are under threat from increasingly pervasive AI systems. i. For ten workers across occupations, the key source of anxiety is the feeling that their skills and judgment are undervalued when a machine is the real expert [28, 38] and that they are being “deprofessionalized.” Similarly, citizens are more frequently encountering AI in policy and consumer spaces (such as welfare eligibility and loans), which may result in a reduction of feelings of personal agency if decisions seem nontransparent or foreordained by algorithms. AI, however, also provokes new possibilities (and dangers) for expression of identity and community, from algorithmic-enhanced social movements to AI applications that help users with disabilities perform and represent themselves, suggesting that any influence on identity is ambivalent and contextdependent. Overall, the sociological dimensions of AI are said to be framed by complex contradictions: empowerment vs. disempowerment, inclusion vs. exclusivity, innovation vs. diminishment of human agency. These findings call for a critical societal view of AI as a social institution in its own right, which is something that is reconstituting what we have come to know as social with no preordained outcome [38].    In addition, the study underscores the urgent need to broaden global regulatory frameworks in order to confront the entrenched structural imbalance between the Global North and South. While regions like the European Union and the United States are pushing forward with regulatory innovation, drafting AI acts, debating algorithmic accountability, and setting ethical benchmarks, the Global South is often left navigating a terrain shaped by technological dependency. In many cases, countries lack not only access to innovative tools but also the institutional infrastructure to meaningfully govern their use. The result is a growing asymmetry: AI systems designed elsewhere, often with little regard for local contexts, are deployed in settings where oversight is minimal and agency constrained. This disjuncture risks not merely replicating existing inequalities, but deepening them, embedding hierarchies of access, control and responsibility into the very code of the future. In a world increasingly mediated by algorithmic systems, the absence of globally inclusive governance mechanisms threatens to turn the digital revolution into a new chapter of geopolitical exclusion.8.2. Organizational considerations    Narayan and Shestakofsky [48] explain that the AI field itself depends on an invisible labor force of “data cleaners” and annotators, often outsourced to workers who are paid barely subsistence wages m instant. Inside organizations, this plays out as a growing divide between the AI “haves” (engineers who design11and direct intelligent systems) and the “have-nots” who conduct the tasks those systems have decided they should do. So, power differences between management and labor are, in these ways, exacerbated by AI, which increases the life-changing power of strategic information and direction in those who possess or can at least interpret algorithms. One important form of organization is the emergence of algorithmic management. AI is increasingly being used by employers to monitor workers, assess performance and even make managerial decisions, from algorithmic scheduling in warehouses to AI-assisted hiring and promotions. This examination of these systems revealed that, while they may standardize and optimize personnel control processes, they may also do so to the detriment of employee privacy, dignity and autonomy. Carter [27] notes that AI-mediated workplace surveillance “treats workers as statistical entities,” as human labor is abstracted as data for monitoring. This AI-assisted datafication3 of workers can corrode the old social contract in the workplace. Employees monitored by AI at all times might become more stressed out and less independent, aware of the monitoring their keystrokes and movements are being subjected to [27].    Increasing the power imbalance between employer and worker AI is an opaque tool in the hands of management allowing for greater micro-management and supervision without adequate rules of the game or comeback for the workers. These moves have the potential to backfire, however: when employees rebel against AI systems they perceive as unfair or invasive, it only adds to the resistance, lower morale, and breakdown of trust that accompany them within organizations. Subsequently, this leads to a paradox for company leaders, in that AI can inflate some of the productivity metrics, but it can also erode the very human aspects (such as motivation, trust, loyalty), which perpetuate organizational performance. Another problem of organization is discrimination and bias in AI-driven decision-making.    The review pointed out that AI tools used in human resources or other decision-making could unwittingly reflect societal biases unless scrupulously overseen. Take AI-based recruitment programs, for example, which can discriminate on the grounds of gender and racial when they are trained using data that is themselves based on biased information, leading to “discriminatory hiring practices” if they are left unchecked [28]. This exposes companies to the risk of integrating social disparities within their organizations, as well as the risk of a legal or reputational backlash. The tension here is evident: firms want these data-driven, objective tools to aid in decision-making, but they can surface and even magnify human bias present in data [28].    Some forward-looking organizations are starting to create AI ethics committees and bias audits to rectify this situation, but such initiatives are still rare. And AIs bring new challenges to institutional norms. Conventional professional roles and identities can be disrupted, such as a hiring manager who transitions from intuitive decision-maker to overseer of algorithmic recommendations, redefining what it means to perform such a job. If not managed inclusively, such changes may create identity threats, where people come to feel devalued and threatened by the work that AI may be performing. AI’s institutional revolution therefore calls for organizations to change their structures, cultures and politics. That could mean retraining employees for new hybrid human, AI roles, rewriting job descriptions and laying out clear rules for understanding how human judgment and AI outputs should factor into decisions.8.3. Toward human-centered AI    The multiple tensions and effects described above suggest a clear necessity of reorienting AI development and deployment toward a human-centered model. What do we mean by humancentered AI? We allude to an approach that takes human values, social impacts, and ethical precepts as the starting point for design and governance of technology. The review highlights that social engagement and ethical innovation are not optional extras, but integral to the shaping of AI. Social scientists, including sociologists, have a key role to play here.” This specialization can help deepen our understanding, for instance, of how AI systems are entwined with matters of power and inequality in ways that can support the production of more equitable and inclusive technologies [38]. The interrelation of these social, ethical, and political dimensions, as well as the normative shift from passive adaptation to active humancentereddesign,isconceptuallyrepresentedinFigure3,whichmaps the sociotechnical dynamics guiding the transition toward more inclusive, equitable, and ethically grounded AI systems.Figure 3Flow of sociotechnical dynamics toward human-centered AI12    So instead of passively letting AI have its social way with us, a human-centered approach suggests we actively guide AI in ways that strengthen human agency, promote fairness, and minimize harm. This ethos dovetails with current policy discussions and models (such as the EU’s draft AI Act and numerous AI ethics guidelines) which prioritize transparency, accountability, and human oversight for high-stake AI systems. However, it is what comes after the big principles that is difficult. One way forward is to institutionalize ethical and participatory design practices across the entire life cycle of AI. Collaborative co-design can help ensure that AI tools meet real human needs, instead of just one person prioritizing the needs of the many. For example, by including employees in the roll-out of an AI system at work, you can uncover concerns about fairness and privacy early on, which can inform better policies when using it. They suggest that participatory approaches like this are crucial for mitigating power asymmetries in the AI ecosystem [38, 60].    If the traditionally marginalized voices are at the table, structural inequality, cultural location, and historical bias are more likely to be considered in innovation. This aligns with the concept of data justice where the data producing and being impacted by AI outputs should be the ones who have a voice on how these are used [38]. At the highest level, policymakers are being asked to impose guardrails that mitigate the most grievous abuse of AI and reward uses of it that promote the social good [26]. Capraro et al. [26] argue that explicit policy intervention is necessary to prevent AI from further exacerbating inequality, including measures such as reforming tax and labor policies to meet the needs of an automated world, investing in education and reskilling, and promoting “AI for good” to complement (and not replace) human labor.    Similarly, improving the transparency and accountability of algorithms is key. This might include responsibilities for algorithmic impact assessments, audits for bias in AI systems, and a right to a human explanation for automated decisions, trends which dovetail currentlegaltrendsarounddataprotectionandAIoversight[27,28]. Within companies, human-centered AI governance might involve creating ethics review boards, conducting test runs of A.I. systems with humans in the loop before launching them widely, or providing avenues so that employees can raise A.I. concerns without fear of retaliation. These steps help foster a culture in which human values are a check on technology, not the other way around.    To advance both academic and policy-oriented discussions, future research should turn its attention to actionable pathways that embed human-centered principles into the heart of AI Regulation, especially in regions and sectors that have historically been underrepresented in both technological development and governance discourse. This entails more than just inclusion; it calls for the co-creation of norms, practices, and oversight mechanisms that reflect local contexts and lived realities. Cross-sectoral partnerships between governments, civil society, academia, and industry can serve as critical bridges, while participatory governance models offer a means to democratize decision-making and redistribute epistemic authority. Ethical innovation frameworks, if grounded in reflexivity rather than compliance checklists, could help ensure that technological advancement does not come at the expense of justice, accountability, or human dignity.9. Conclusion    This review has synthesized key interdisciplinary research published between 2023 and 2025, offering a robust analytical framework that conceptualizes AI not merely as a set of technologies, but as a sociotechnical system embedded in power, identity, and institutional structures. Based on 64 peer-reviewed papers from Q1-journals in the field, the review has illustrated how AI are becoming enmeshed with and emergent from power, identity, labor and governance systems. The conversation, organized along the lines of sociological implications, organizational reflections and roads to human-centered AI, reveals the profound ambivalence of AI’s impact: it can increase inclusion and efficiency, but also entrench structural inequalities, erode democratic oversight, and displace human agency.    Sociologically AI technologies cannot be managed as neutral, standalone innovations. As the latter examples demonstrate across the literature, they work within, and often help to sustain, prevailing social hierarchies and institutional asymmetries. Algorithms replicate biases encoded in the training data for the system, favoring dominant linguistic and cultural norms and facilitating surveillance practices that have a disproportionate impact on those who are least represented in society. These are not just ethical quandaries, but renegotiate the conditions in and through which rights, subjectivities and collective identities are produced. AI, in this way, has become a new terrain of epistemic and political contestation, a mode of knowledge production that requires scrutiny in terms of who it benefits and who it silences. At the institutional level, AI is changing what we think about work, its division, and the judgment of the work.    The literature has been explicit on how: the employment of algorithmic management, the monitoring of performance, and automated decision-making refashion power relations between workers and employers. The challenge, however, is that while these new technologies are certainly capable of augmenting our productivity and saving us time and effort on tedious tasks, they are simultaneously angering the gods by eroding professional identities and sapping workers’ autonomy, especially for those in lower-wage or vulnerable job categories. Crucially, the impacts of using AI are not the same for all, but mediated by institutional logic, managerial rationale and sectoral context. This discovery reminds us of the agency of these entities in deciding whether AI works for human benefit or simply increases workplace inequality. Just as important, however, is the emerging evidence that AI not only determines labor and governance but how we understand personal identity and relate to others. From measuring emotional labor to the algorithmic mediation of content and social relations, people are experiencing AI as a social actor in the course of their quotidian activities. These experiences disrupt traditional notions of authenticity, intimacy, and originality.    They call for rethinking human–machine boundaries, not in the sense of replacement but of entanglement, where machines cocreate meanings, emotions and subjectivities. Taken together, these perspectives all call for an aggressive pivot to human-centered AI. This review has made the case that the integration of ethical, inclusive and participatory considerations across the entire AI lifecycle is not a privilege, but a necessity. For such an approach to be successful, a wide range of scholars, from sociologists and organizational scholars to computer scientists, policymakers, and influenced communities, should be involved in co-designing systems that present plural values and foreground social justice. Human-centric AI is a change,notjustintechnologies,butalsointheinstitutionalandepistemic context of their development and use. Important limitations of current research offer opportunities for future corpus studies, as this review has also revealed.    This synthesis contributes to the articulation of a dynamic sociotechnical assemblage model, in which AI operates not merely as a tool but as a mediator of institutional power, epistemic authority, and affective labor. This model captures the recursive interplay between algorithmic processes and sociocultural structures, offering a conceptual framework that can inform both future empirical investigations and normative debates. The stakes are high: AI is fast becoming a reference genome for the texture of modern life, for everything from what we do to who we are. Left uninterrogated13and unregulated, it could entrench the very inequalities, exclusions, and exploitations that the discipline of sociology is devoted to dismantling. But if shaped critically and collectively, AI can also be a force for democratization, equity and human dignity.    The AI-triggered sociotechnological revolution is not a linear or inevitable one. It is unchangeable by persuasion, compromise or dissent. Sociology is well positioned to play a pivotal role in shaping the ethical, organizational, and cultural futures of AI, drawing upon its long traditions of studying power, inequality, and institutional change. This review does more than consolidate existing knowledge, it proposes a critical, interdisciplinary roadmap for steering the evolution of AI toward futures anchored in justice, equity, and democratic accountability. In a landscape often dominated by technical benchmarks and market imperatives, it calls for a deeper reckoning with the normative stakes of AI: who benefits, who is burdened, and who gets to decide. By foregrounding sociotechnical complexity and amplifying marginalized perspectives, the review lays the groundwork for reimagining AI not as an autonomous force, but as a collective project, one that must be continuouslynegotiated,contested,andguidedbysharedhumanvalues. The findings underscore the pressing need to embed sociotechnical expertise at the core of policy design and regulatory architecture. This goes beyond technical consultation; it requires the deliberate inclusion of interdisciplinary perspectives, from sociology, ethics, political theory, and civil society, within the institutional arenas where decisions about AI are made. Especially in cross-sectoral and transnational contexts, where the stakes of regulation transcend borders and industries, such integration can function as a counterweight to technocratic opacity and market-driven priorities. By doing so, policy frameworks gain not only in technical robustness but also in democratic legitimacy, institutional accountability, and distributive equity, ensuring that the governance of AI reflects the full complexity of the societies it seeks to serve.Ethical Statement    This study does not contain any studies with human or animal subjects performed by the author.Conflicts of Interest    The author declares that he has no conflicts of interest to this work.Data Availability Statement    Data sharing is not applicable to this article as no new data were created or analyzed in this study.Author Contribution Statement    Marc Selgas-Cors: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Resources, Data curation, Writing – original draft, Writing – review & editing, Visualization, Supervision, Project administration.References[1] Banerjee, G., Dhar, S., Roy, S., Syed, R., & Das, A. (2024). Explainability and transparency in designing responsible AI applications in the enterprise. In The International Conference on Computing, Communication, Cybersecurity & AI, 420–431. https://doi.org/10.1007/978-3-031-74443-3_2514[2] López Belloso, M. (2022). Women’s rights under AI regulation: Fighting AI gender bias through a feminist and intersectional approach. In C. Bart & F-V. Eduard (Eds.), Law and artificial intelligence: Regulating AI and applying AI in legal practice (pp. 87–107). TMC Asser Press. https://doi.org/10.1007/ 978-94-6265-523-2_5[3] Waqas, M., Humphries, U. W., Wangwongchai, A., Dechpichai, P., & Ahmad, S. (2023). Potential of artificial intelligencebased techniques for rainfall forecasting in Thailand: A comprehensive review. Water, 15(16), 2979. https://doi.org/10. 3390/w15162979[4] Shabur, M., Shahriar, A., & Ara, M. A. (2025). From automation to collaboration: Exploring the impact of Industry 5.0 on sustainable manufacturing. Discover Sustainability, 6, 341. https://doi.org/10.1007/s43621-025-01201-0[5] Taylor, L., & Dencik, L. (2020). Constructing commercial data ethics. Technology and Regulation, 2, 1–10. https://doi.org/10. 71265/xz58kz69[6] Clark, E. C., Neumann, S., Hopkins, S., Kostopoulos, A., Hagerman, L., & Dobbins, M. (2024). Changes to public health surveillance methods due to the COVID-19 pandemic: Scoping review. JMIR Public Health and Surveillance, 10, e49185. https://doi.org/10.2196/49185[7] Edenberg, E., & Wood, A. (2023). An epistemic lens on algorithmic fairness. In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, 1–10. https://doi.org/10.1145/3617694.362324[8] Galariotis, I. (2024). Is artificial intelligence threatening democracy? Retrieved from: https://cadmus.eui.eu/entities/ publication/da4f64d5-71b2-508e-abac-9b4121c80908[9] Ozmen Garibay, O., Winslow, B., Andolina, S., Antona, M., Bodenschatz, A., Coursaris, C., ..., & Xu, W. (2023). Six human-centered artificial intelligence grand challenges. International Journal of Human–Computer Interaction, 39(3), 391–437. https://doi.org/10.1080/10447318.2022.2153320[10] Aytac, U. (2024). Big tech, algorithmic power, and democratic control. The Journal of Politics, 86(4), 1431–1445.[11] Regilme, S. S. F. (2024). Artificial intelligence colonialism: Environmental damage, labor exploitation, and human rights crises in the Global South. SAIS Review of International Affairs, 44(2), 75–92. https://doi.org/10.1353/sais.2024.a950958[12] Zuboff, S. (2022). Surveillance capitalism or democracy? The death match of institutional orders and the politics of knowledge in our information civilization. Organization Theory, 3(3), 1– 79. https://doi.org/10.1177/26317877221129290[13] Torres Carceller, A. (2024). The ARTificial revolution: Challenges for redefining art education in the paradigm of generative artificial intelligence. Digital Education Review, 45, 84–90.[14] Shruthi, H. L., Radhakrishnan, A., Veigas, A. D., Railis, D. J., & Dinesh, R. S. (2025). Analyzing pedagogy and education in English language teaching using information and communication technology. Education and Information Technologies, 30, 1–23. https://doi.org/10.1007/s10639-025-13439-2[15] Page, M. J., McKenzie, J. E., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., ..., & Moher, D. (2021). The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. BMJ, 372, n71. https://doi.org/10.1136/ bmj.n71[16] Ali, S. M., Daly, A. C., Gjorgjioska, A., Hespanhol, L., Kerasidou, X., Mostéfaoui, S. K., & Tomi?i?, A. (2024).The (un)bearable whiteness of AI ethics. In D. J. Gunkel (Ed.), Handbook on the ethics of artificial intelligence (pp. 217–230). Edward Elgar Publishing. https://doi.org/10. 4337/9781803926728.00020[17] Arora, P. (2024). Creative data justice: A decolonial and indigenous framework to assess creativity and artificial intelligence. Information, Communication & Society, 27, 1–17. https://doi. org/10.1080/1369118X.2024.2420041[18] Ayana, G., Dese, K., Daba Nemomssa, H., Habtamu, B., Mellado, B., Badu, K., ..., & Kong, J. D.(2024). Decolonizing global AI governance: Assessment of the state of decolonized AI governance in Sub-Saharan Africa. Royal Society Open Science, 11(8), 231994. https://doi.org/10.1098/rsos.231994[19] Badgujar, C. M., Poulose, A., & Gan, H. (2024). Agricultural object detection with You Only Look Once (YOLO) Algorithm: A bibliometric and systematic literature review. Computers and Electronics in Agriculture, 223, 109090. https://doi.org/10. 1016/j.compag.2024.109090[20] Baumer, E. P. S., Taylor, A. S., Brubaker, J. R., & McGee, M. (2024). Algorithmic subjectivities. ACM Transactions on Computer-Human Interaction (TOCHI), 31(3), 1–34. https:// doi.org/10.1145/3660344[21] Becker, L., Wurm, B., & Hess, T. (2023). Will algorithms replace managers? A systematic literature review on algorithmic management. In International Conference of Information Systems.[22] Berson, I. R., Berson, M. J., & Luo, W. (2025). Innovating responsibly: Ethical considerations for AI in early childhood education. AI, Brain and Child, 1(1), 2. https://doi.org/10.1007/ s44436-025-00003-5[23] Budhwar, P., Chowdhury, S., Wood, G., Aguinis, H., Bamber, G. J., Beltran, J. R., ..., & Varma, A. (2023). Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT. Human Resource Management Journal, 33(3), 606–659. https:// doi.org/10.1111/1748-8583.12524[24] Bueger, C., & Liebetrau, T. (2023). Critical maritime infrastructure protection: What’s the trouble? Marine Policy, 155, 105772. https://doi.org/10.1016/j.marpol.2023.105772[25] Canfield, M., & Ntambirweki, B. (2024). Datafying African agriculture: From data governance to farmers’ rights. Development, 67(1), 5–13. https://doi.org/10.1057/s41301-024-004057[26] Capraro, V., Lentsch, A., Acemoglu, D., Akgun, S., Akhmedova, A., Bilancini, E., ..., & Viale, R. (2024). The impact of generative artificial intelligence on socioeconomic inequalities and policy making. PNAS Nexus, 3(6), pgae191. https://doi.org/10.1093/pnasnexus/pgae191[27] Carter, M. C. (2024). AI surveillance: Reclaiming privacy through informational control. European Labour Law Journal, 16(2), 245–258. https://doi.org/10.1177/20319525241306327[28] Chen, Z. (2023). Ethics and discrimination in artificial intelligence–enabled recruitment practices. Humanities and Social Sciences Communications, 10(1), 567. https://doi.org/10. 1057/s41599-023-02079-x[29] Chonka, P., Diepeveen, S., & Haile, Y. (2023). Algorithmic power and African Indigenous languages: Search engine autocomplete and the global multilingual internet. Media, Culture & Society, 45(2), 246–265. https://doi.org/10.1177/ 0163443722110470[30] D’Amato, K. (2024). ChatGPT: Towards AI subjectivity. AI & Society, 40, 1627–1641. https://doi.org/10.1007/s00146-02401898-z[31] Downey, A., Islam, S. R., & Sarker, M. K. (2024). Predictive policing: A fairness-aware approach. International Journal on Artificial Intelligence Tools, 33(3), 2460005. https://doi.org/10. 1142/S0218213024600054[32] Duke, S. A. (2023). AI and the industrialization of surveillance. Surveillance & Society, 21(3), 282–286. https://doi.org/ 10.24908/ss.v21i3.16086[33] Duncan, J. (2023). Data protection beyond data rights: Governing data production through collective intermediaries. Internet Policy Review, 12(3), 1–22. https://doi.org/10.14763/2023.3. 1722[34] Hansen, H. F., Lillesund, E., Mikalef, P., & Altwaijry, ?. (2024). Understanding artificial intelligence diffusion through an AI capability maturity model. Information Systems Frontiers, 26(6), 2147–2163. https://doi.org/10.1007/ s10796-024-10528-4[35] Hartley, K., & Aldag, A. (2024). Public trust and support for government technology: Survey insights about Singapore’s smart city policies. Cities, 154, 105368. https://doi.org/10.1016/ j.cities.2024.105368[36] Ho, J. Q., Hartanto, A., Koh, A., & Majeed, N. M. (2025). Gender biases within artificial intelligence and ChatGPT: Evidence, sources of biases and solutions. Computers in Human Behavior: Artificial Humans, 4, 100145. https://doi.org/10.1016/j.chbah. 2025.100145[37] Jørgensen, R. F. (2023). Data and rights in the digital welfare state: The case of Denmark. Information, Communication & Society, 26(1), 123–138. https://doi.org/10.1080/1369118X. 2021.1934069[38] Joyce, K., & Cruz, T. M. (2024). A sociology of artificial intelligence: Inequalities, power, and data justice. Socius, 10, 1–6. https://doi.org/10.1177/23780231241275393[39] Kadolkar, I., Kepes, S., & Subramony, M. (2024). Algorithmic management in the gig economy: A systematic review and research integration. Journal of Organizational Behavior. Advance online publication. https://doi.org/10.1002/job.2831[40] Khang, A., Hahanov, V., Hajimahmud, V. A., Litvinova, E., Ali, R. N., & Alyar, A. V. (2024). The impact of the cyberphysical environment on the socialization environment. In A. Khang, V. Hahanov, V. A. Hajimahmud, E. Litvinova, R. N. Ali & A. V. Alyar (Eds.), Revolutionizing the AI-digital landscape. (pp. 295–307). Productivity Press. https://doi.org/10. 4324/9781032688305-22[41] Kong, Y., & Ding, H. (2024). Tools, potential, and pitfalls of social media screening: Social profiling in the era of AI-assisted recruiting. Journal of Business and Technical Communication, 38(1), 33–65. https://doi.org/10.1177/1050651923119947 [42] Kothinti, R. R. (2024). Artificial intelligence in healthcare: Revolutionizing precision medicine, predictive analytics, and ethical considerations in autonomous diagnostics. World Journal of Advanced Research and Reviews, 19(3), 3395–3406. https://doi.org/10.30574/wjarr.2024.24.3.3675[43] Leslie, D., & Perini, A. M. (2024). Future shock: Generative AI and the international AI policy and governance crisis. Harvard Data Science Review, 5. https://doi.org/10.1162/99608f92. 88b4cc98[44] Liu, H., Li, H., & Fang, F. (2024). “Behind the screen, I still care about my students!”: Exploring the emotional labor of English language teachers in online teaching during the COVID-19 pandemic. International Journal of Applied Linguistics, 34(2), 450–465. https://doi.org/10.1111/ijal.1250115[45] Lostal Martínez, F. R. (2024). The impact of artificial intelligence on Mexico’s logistics sector: Challenges and opportunities. International Conference on Computational Logistics, 15168, 95–114. https://doi.org/10.1007/978-3-031-71993-6_7[46] Luo, B., Lau, R. Y., & Li, C. (2023). Emotion-regulatory chatbots for enhancing consumer servicing: An interpersonal emotion management approach. Information & Management, 60(5), 103794. https://doi.org/10.1016/j.im.2023.103794[47] Mele, V., Belardinelli, P., & Bellé, N. (2023). Telework in public organizations: A systematic review and research agenda. Public Administration Review, 83(6), 1649–1666. https://doi. org/10.1111/puar.13734[48] Narayan, D., & Shestakofsky, B. (2024). Relationships that matter: Four perspectives on AI, work, and organizations. The Journal of Applied Behavioral Science, 60(4), 356–368. https:// doi.org/10.1177/00218863241285456[49] Nguyen, V. A., Serwe, W., Mateescu, R., & Jenn, E. (2019). Hunting superfluous locks with model checking. In M. ter. Beek, A. Fantechi, & L. Semini (Eds.), From software engineering to formal methods and tools, and back: Essays dedicated to Stefania Gnesi on the occasion of her 65th birthday(pp. 416–432). Switzerland: Springer. https://doi.org/10.1007/ 978-3-030-30985-5_24[50] O’Connor, S., & Liu, H. (2024). Gender bias perpetuation and mitigation in AI technologies: Challenges and opportunities.AI&SOCIETY,39(4),2045–2057.https://doi.org/10.1007/ s00146-023-01675-4[51] Olawade, D. B., Wada, O. J., David-Olawade, A. C., Kunonga, E., Abaire, O., & Ling, J. (2023). Using artificial intelligence to improve public health: A narrative review. Frontiers in Public Health, 11, 1196397. https://doi.org/10.3389/fpubh.2023. 1196397[52] Park, S. (2024). The work of art in the age of generative AI: Aura, liberation, and democratization. AI & Society, 40, 1–10. https://doi.org/10.1007/s00146-024-01948-6[53] Qi, W., Pan, J., Lyu, H., & Luo, J. (2024). Excitements and concerns in the post-ChatGPT era: Deciphering public perception ofAIthroughsocialmediaanalysis.TelematicsandInformatics, 92, 102158. https://doi.org/10.1016/j.tele.2024.102158[54] Roemmich, K., Schaub, F., & Andalibi, N. (2023). Emotion AI at work: Implications for workplace surveillance, emotional labor, and emotional privacy. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 1–20. https://doi.org/10.1145/3544548.358095[55] Samuelson, P. (2023). Generative AI meets copyright. Science, 381(6654), 158–161. https://doi.org/10.1126/science.adi0656[56] Schlosberg, D., Rickards, L., & Tschakert, P. (2023). Climate justice in a more-than-human world. The Sociological Review, 71(6), 1299–1321. https://doi.org/10.1177/ 00380261231184357[57] Simon, F. (2024). Artificial intelligence in the news: How AI retools, rationalizes, and reshapes journalism and the public arena. Retrieved from: https://academiccommons. columbia.edu/doi/10.7916/ncm5-3v06[58] Singh, D. (2025). Dalits’ encounters with casteism on social media: A thematic analysis. Information,Communication & Society, 28(2), 335–353. https://doi.org/10. 1080/1369118X.2025.246224416[59] Singh, S. (2024). Digital surveillance and valuation in datafile societies. In A. Krüger, T. Peetz, & H. Schaefer (Eds.), The Routledge international handbook of valuation and society (pp. 257–266). Routledge.[60] Smith, R. C., Winschiers-Theophilus, H., de Paula, R. A., Zaman, T., & Loi, D. (2024). Towards pluriversality: Decolonising design research and practices. CoDesign, 20(1), 1–13. https://doi.org/10.1080/15710882.2024.2379704[61] Stark, D., & Vanden Broeck, P. (2024). Principles of algorithmic management. Organization Theory, 5(2), 26317877241257213. https://doi.org/10.1177/26317877241257213[62] Suárez-Roldan, C., & Méndez-Giraldo, G. (2024). Peasant displacement and food sustainability: The Colombian case. SN Computer Science, 5(5), 634. https://doi.org/10.1007/ s42979-024-02877-3[63] Tao, Y., Viberg, O., Baker, R. S., & Kizilcec, R. F. (2024). Cultural bias and cultural alignment of large language models. PNAS Nexus, 3(9), 346. https://doi.org/10.1093/pnasnexus/ pgae346[64] Williams, P., & Khan, M. H. (2025). Framing algorithmic management: Constructed antagonism on HR technology websites. New Technology, Work and Employment, 40(1), 102–123. https://doi.org/10.1111/ntwe.12305[65] Yan, L., Sha, L., Zhao, L., Li, Y., Martinez-Maldonad, R., Chen, G., ..., & Ga?evi?, D. (2024). Practical and ethical challenges of large language models in education: A systematic scoping review. British Journal of Educational Technology, 55(1), 90– 112. https://doi.org/10.1111/bjet.13370[66] Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. Qualitative Research in Psychology, 3, 77–101. https://doi.org/10.1191/1478088706qp063oa[67] Sánchez Lira, P. (2024). Sisyphean labor instantiation device (SLIDE): Using critical design to make the precarious human labor in AI tangible. Master’s Thesis, KTH School of Electrical Engineering and Computer Science.[68] Stutzer, A., & Zehnder, M. (2012). Is camera surveillance an effective measure of counterterrorism? Defence and Peace Economics, 24(1), 1–14. https://doi.org/10.1080/10242694.2011. 650481[69] Cornelissen, J. P., Akemu, O., Jonkman, J. G., & Werner, M. D. (2021). Building character: The formation of a hybrid organizational identity in a social enterprise. Journal of Management Studies, 58(5), 1294–1330. https://doi.org/10.1111/joms.12640[70] Zuboff, S. (2023). The age of surveillance capitalism. In W. Longhofer & D. Winchester (Eds.), Social theory re-wired (pp. 203–213). Routledge.[71] van Dijck, J. (2018). Datafication, dataism and dataveillance: Big data between scientific paradigm and ideology. Surveillance & Society, 12(2), 197–208. https://doi.org/10.24908/ss. v12i2.4776How	to	Cite:	Selgas-Cors,	M.	(2025).	SociotechnicalTransforma-tion:	A	Systematic	Review	on	the	Impact	of	ArtificialIntelligenceon	Society	and	Organizations.	FinTech	and	Sustainablehttps://doi.org/10.47852/bonviewFSI52026076Innovation.    1 AI adoption in organizational contexts has significantly altered managerial practices, introducing new layers of control and performance monitoring. This is often referred to as algorithmic management, a system whereby human work is directed, evaluated, and optimized by automated decision-making tools and data-driven rules [49].2 Surveillance capitalism is an economic system centered on the unilateral extraction and commodification of human behavioral data (often collected without informed consent) to predict and influence behavior at scale. It transforms private human experiences into free raw material for commercial practices, creating markets for behavioral predictions and ultimately enabling new forms of economic control and profit generation [68].3 Datafication, in turn, describes the process by which qualitative aspects of human activity are translated into quantifiable digital data, enabling performance tracking and predictive analytics [71].---------------    ------------------------------------------------------------        ---------------        ------------------------------------------------------------    FinTech and Sustainable Innovation	Vol. 00	Iss. 00	2025    FinTech and Sustainable Innovation	Vol. 00	Iss. 00	2025    Pdf_Folio:1    Pdf_Folio:1        Pdf_Folio:1    FinTech and Sustainable Innovation	Vol. 00	Iss. 00	2025Table 1(Continued)    FinTech and Sustainable Innovation	Vol. 00	Iss. 00	2025Table 1(Continued)    Pdf_Folio:1    Pdf_Folio:1    FinTech and Sustainable Innovation	Vol. 00	Iss. 00	2025Table 1(Continued)    Pdf_Folio:1    FinTech and Sustainable Innovation	Vol. 00	Iss. 00	2025    FinTech and Sustainable Innovation	Vol. 00	Iss. 00	2025    Pdf_Folio:1    Pdf_Folio:1    FinTech and Sustainable Innovation	Vol. 00	Iss. 00	2025    Pdf_Folio:1    