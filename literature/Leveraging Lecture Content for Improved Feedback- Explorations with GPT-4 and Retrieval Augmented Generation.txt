Leveraging Lecture Content for ImprovedFeedback: Explorations with GPT-4 and Retrieval Augmented Generation1st Sven JacobsComputer Science EducationUniversity of Siegen    Siegen, Germany sven.jacobs@uni-siegen.de2nd Steffen JaschkeComputer Science EducationUniversity of Siegen     Siegen, Germany steffen.jaschke@uni-siegen.de  
Abstract—This paper presents the use of Retrieval AugmentedGeneration (RAG) to improve the feedback generated by Large Language Models for programming tasks. For this purpose, corresponding lecture recordings were transcribed and made available to the Large Language Model GPT-4 as external knowledge source together with timestamps as metainformation by using RAG. The purpose of this is to prevent hallucinations and to enforce the use of the technical terms and phrases from the lecture. In an exercise platform developed to solve programming problems for an introductory programming lecture, students can request feedback on their solutions generated by GPT-4. For this task GPT-4 receives the students’ code solution, the compiler output, the result of unit tests and the relevant passages from the lecture notes available through the use of RAG as additional context. The feedback generated by GPT-4 should guide students to solve problems independently and link to the lecture content, using the time stamps of the transcript as meta-information. In this way, the corresponding lecture videos can be viewed immediately at the corresponding positions. For the evaluation, students worked with the tool in a workshop and decided for each feedback whether it should be extended by RAG or not. First results based on a questionnaire and the collected usage data show that the use of RAG can improve feedback generation and is preferred by students in some situations. Due to the slower speed of feedback generation, the benefits are situation dependent.  Index Terms—Programming Education, Feedback, Large Language Models, GPT-4, Retrieval Augmented GenerationI. INTRODUCTION  Individual support in teaching and learning contexts with heterogeneous learning groups is desirable in both school and university educational settings, but usually cannot be fully implemented in reality due to the limited availability of teaching staff.  The topic of individual support by generative AI such as GPT-4 [1] is particularly promising in computer science education due to its good programming capabilities [2]. Commercially available applications such as ChatGPT, Bard, GitHub Copilot and others are not explicitly designed for skill development or knowledge acquisition, so they directly solve the given programming tasks instead of guiding the learner to solve the problem. This would only be possible with specific prompts. In addition, the external LLM application, such as ChatGPT, must be provided with the current code or error messages each time. For this reason, we have developed the Tutor Kai programming exercise environment with integrated LLM support.  In order to support the student in exercises in the context of an associated lecture with teaching material with feedback, it seems useful if the feedback also refers to the corresponding content and is verifiably linked. This can be achieved by using Retrieval Augmented Generation (RAG), which can also reduce hallucinations because it is based more on real, verifiable facts [3]. The question then becomes how to design a feedback system that references and links to lecture information and how students perceive it.II. RELATED WORK  Even before LLMs were available, there were a variety of feedback systems for programming tasks [4]. While checking for correct syntax and semantics can be easily automated with unit tests, for example, more specific feedback requires static code checks, which can be time-consuming to set up for each task [5].  The new possibilities of large language models are manifesting themselves in computer science education in several areas, such as the generation of teaching materials [6] and the analysis of student work [7]. The CodeAid system, for example, provides students with various programming aids such as Inline Code Exploration, Question from Code, Help Fix Code, Explain Code, and Help Write Code [8]. There are also solutions for automated feedback using LLMs [9] [10] [11] [12], although these do not include specific lecture information in the generated feedback.  While there are still no publications on knowledge-based feedback in programming education, videos are already widely used as a knowledge base for question answering [13] and chatbots [14]. Asthana et al. describe a system that uses lecture videos and transcripts as a knowledge base and extracts metadata from them using large language models, e.g. to generate questions about concepts. Feedback is also to be generated on this basis, but has not yet been evaluated [15].  
©2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.Fig. 1. System Design for Enhanced Programming Feedback using Retrieval Augmented Generation  
III. DESIGN AND IMPLEMENTATION  For lectures, the question arises as to which knowledge elements are suitable as a basis. For example, there are texts such as lecture slides, books, and worksheets combined with illustrations. However, the oral explanations of the teacher, which put the content into context, seem to be particularly suitable, since they are available in video form as a lecture recording. By linking the lecture recording to the corresponding timestamp, students are able to perceive the associated visual elements.A. Indexing  The lecture recordings were first transcribed into .SRT (SubRip Text) format using the OpenAI speech recognition model Whisper [16]. This format not only provides the transcribed text, but also includes timestamps for each segment, allowing subsequent linking to specific points in the video. The texts assigned to a segment in the transcript can vary greatly in length, making them unsuitable for further processing. Therefore, a simple chunking strategy has been implemented, which reduces the text assigned to a segment to a uniform size of 512 characters with an overlap of 64 characters to the previous text. The associated timestamp is updated accordingly. A vector representation of the text is stored in a vector database using an embedding model, together with the original text and the start of the associated timestamp, as well as the name of the original video file. A Postgres database with the pgvector extension and text-embedding-ada-002 from OpenAI is used for Tutor Kai.B. Retrieval  In naive RAG, the question is put into a vector representation as a query with the same embedding model for Question Answering, so that similarity scores between the query vector and the vectorized chunks within the indexed corpus can be computed [17]. The most similar chunks (top K) are then made available to the LLM for answer generation. However, in order to generate feedback on the solutions to programming tasks, there is no specific question or other text that would be suitable as a query for retrieval in the application. Therefore, a system (Fig. 1) was implemented that first creates a suitable query for retrieval.C. Generation  To implement the logic described, a prompt chain was implemented:  1) First Run: Similarly to the ReAct logic [18], the first run identifies X missing concepts for a correct solution based on the available student context information such as task description, student code solution, unit tests and compiler out. For each concept (e.g. recursion), the LLM formulates a simple question (e.g. ”How does recursion work in Python?”), which is then used as a query in the described retrieval.  For Tutor Kai, this is implemented with GPT-4 (1106preview and temperature = 0) and function calling. A maximum of 2 queries are generated for each feedback, for each of which the top 4 relevant chunks with associated meta information are retrieved. In total, a maximum of 8 relevant chunks with a length of 512 characters each are provided for the second run. In addition, the timestamps and video file names in the metainformation of the retrieved chunks are converted into a Markdown footnote format. In this way, a listFig. 2. User Interface of Tutor Kai (translated from German)Feedback questionnaireFig. 3. Questionnaire result: Feedback (Questions translated from German)of sources is automatically created in the final feedback by a markdown parser.  2) Second Run: In the second run, the final feedback is generated based on the retrieved chunks and the student context information (Fig. 2: A, B, D) using GPT-4 (1106preview and temperature = 0). The prompt used includes the following elements:1) Role description: The LLM is put in the role of a helpful professor.2) Definition of rules: Outputting the solution and formulating the code is prohibited. Feedback should be no more than six sentences in no more than three paragraphs.3) Description of the JSON format of the retrieved chunks with associated meta information.4) Few Shot Examples on how to cite the retrieved chunks in the final feedback (using the provided markdown footnote link).5) Student context information (task description, programming language, student code solution, compiler output, unit test result).6) Retrieved lecture chunks in the described JSON format.Using a markdown parser, linked lecture chunks appear as footnotes in feedback (Fig. 2: C). Based on the filename and timestamp, the videos can now be opened in a modal at the linked timestamp (Fig. 2: E).IV. EVALUATION  As part of a voluntary exam preparation workshop, 15 students participated and used Tutor Kai for two to four hours in person and more online over the following three weeks. In addition to the familiar tasks from the previous semester, 10 new tasks were provided. Each time, students could choose between feedback with lecture information and feedback without lecture information. The feedback with lecture information was generated as described. The feedback without lecture information does not use a prompt chain or retrieval, but only the 2nd run (Fig. 1) without the lecture information (same prompt without elements 3, 4 and 6).  After the workshop, the opinions of the students (n = 15) about Tutor Kai and the generated feedback were surveyed using a questionnaire. It should be noted that not all students provided responses to every question posed. During and after the workshops, there were 2192 code submissions for which a total of 574 feedbacks were generated. Of these, 478 were feedback without lecture information and 96 were feedback with lecture information generated by the described system.A. General Evaluation  An important goal of Tutor Kai is that the students do not receive knowledge of correct result in the feedback, but solve the problem independently, which is ensured by the prompt. This goal is achieved from the students’ point of view (Fig. 3: Q1). Overall, the students are satisfied with both the simplicity and the length of the feedback and were able to understand it well (Fig. 3: Q3, Q4, Q5).B. Comparing Feedback Types  In the 96 feedbacks with lecture information, 160 videosegments were linked (average = 1.67). These are spread across 57 different videosegments, with 3 specific segments being linked more than 10 times (maximum = 16).  Lecture information feedback is slower (time to the first streamed token) because the LLM response stream cannot begin until Run 1 (Fig. 1) is fully completed. The time to the first streamed token depends on the use of the OpenAI API, the length of the task description and the solution of the student’s code. Therefore, it cannot be accurately predicted. In our tests, feedback with lecture content took about 18 seconds                                  Q10: I think that I would like to use Tutor-Kai frequently. Q11: I found Tutor-Kai unnecessarily complex. Q12: I thought Tutor-Kai was easy to use. Q13: I think that I would need the support of a technical person to be able to use Tutor-Kai. Q14: I found the various functions in Tutor-Kai were well integrated. Q15: I thought there was too much inconsistency in Tutor-Kai. Q16: I would imagine that most people would learn to use Tutor-Kai very quickly.                            Q17: I found Tutor-Kai very cumbersome to use. Q18: I felt very confident usingTutor-Kai. Q19: I needed to learn a lot of things before I could get going with Tutor-Kai.Q20: I am sceptical about the reliability/accuracy of the information provided by Tutor-Kai.Q21: I'm worried about becoming too dependent on a technology like Tutor-Kai.Q22: I was concerned about the potential privacy risks that may be associated with the use of TutorKai.	15	0	15CountFig. 4. Questionnaire result: System Usability and Concerns (Questions translated from German)to stream the first token to the student, while feedback without lecture content took about 1 to 2 seconds.  Students found the additional linking of lecture segments in the feedback helpful and it helped them find the necessary concepts to solve the problem (Fig. 3: Q6, Q7). However, some students found the generated feedback with lecture information too slow (Fig. 3: Q9). This perception is consistent with the additional responses collected in open-ended questions in the questionnaire about why they preferred which type of feedback. In this questionnaire, students mainly referred to the mentioned speed and several times described an approach where feedback without lecture content is generated first, which would be sufficient for ”easy” cases, and feedback with lecture information is used for ”more difficult” problems. In the open-ended questions it was mentioned multiple times that the feedback with lecture information helped to remember the lecture.C. Concerns  There are concerns that students will become too dependent on LLM-based support systems for programming [7]. In order to obtain preliminary results, three questions (Q20-Q22) from the TAME-ChatGPT [19] were included in the questionnaire. For the most part, students do not share this concern (Fig. 4: Q21). A possible reason for this is that, unlike ChatGPT and similar applications, Tutor Kai does not provide knowledge of the correct result.  The survey also showed that many students did not question the feedback even though they knew it was generated by generative AI (Fig. 3: Q2 and Fig. 4: Q20). When using such systems, a warning should therefore be displayed at all times.D. System Usability Scale  The system used was also evaluated for system usability using the System Usability Scale (Fig. 4) [20]. The final usability score was 74.8. Since the user interface consists of only a few clear elements, it should be straightforward to use. It should be noted that students rated their overall experience with Tutor Kai beyond the feedback, which also depends on the tasks to be completed. For example, Q19 (Fig. 4) may have been rated negatively by the students because they feel that they still have a lot to learn in order to complete the tasks successfully.V. LIMITATIONS AND FUTURE WORK  Due to the sample size of n=15, only trends are observable. Further research is required to validate these findings. Knowledge and skill acquisition were not the subject of this evaluation. To investigate the extent to which the linked videos were used, future studies should record how long each video was watched per feedback.  There is great potential for indexing and retrieval improvements for future work. For example, specially created short explanatory videos could be better suited than lecture recordings. Additional lecture content could also be linked. The current chunking strategy, which is simply based on the number of characters, could be improved by semantic chunking strategies or the use of a knowledge graph [21]. When generating the necessary concepts in the first run, this could be done several times and the most frequently selected ones could be used by majority voting. Solutions such as the feedback validation [12] will significantly improve the results in the future or enable completely new applications.In this context, Nori et al. have shown that more advanced prompting techniques can lead to higher performance gains than the development of an improved foundation model [22].VI. CONCLUSION  This work investigates how to design a feedback system for programming tasks that refers to lecture content such as videos and can provide concrete content from this information. A two-run prompt chain is used, in which a query for retrieval augmented generation is generated in a first run with GPT-4. In a second run with GPT-4, the retrieved chunks from the transcribed lecture video are used together with a markdown footnote link containing the timestamp and name of the video. Together with the student’s context information, the final feedback is generated, linking the corresponding lecture videos at the respective timestamp.  The system has been evaluated with 15 students. Most of them stated in a questionnaire that they found the feedback with linked lecture information helpful and that it helped them to find relevant concepts more quickly. The feedback without lecture information was preferred by students for quick feedback on what they considered a ”simple” problem. The feedback with lecture information takes multiple times longer before the first token is streamed, because the query generation has to be completed first. This is another reason why feedback without lecture information was used about four times more often. The speed to the first character is therefore a trade-off. The students were also satisfied with the length and simplicity of the feedback.REFERENCES[1] OpenAI,	“GPT-4	Technical	Report,”	2023,	doi:10.48550/ARXIV.2303.08774.[2] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang, “Sparks of Artificial General Intelligence: Early experiments with GPT-4,” 2023, doi: 10.48550/arXiv.2303.12712.[3] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W.-t. Yih, T. Rockt¨ aschel, S. Riedel, and D. Kiela,¨ “Retrieval-augmented generation for knowledge-intensive NLP tasks,” in Proceedings of the 34th International Conference on Neural Information Processing Systems, ser. NIPS ’20, NY, USA, 2020, doi: 10.48550/arXiv.2005.11401.[4] H. Keuning, J. Jeuring, and B. Heeren, “A Systematic Literature Review of Automated Feedback Generation for Programming Exercises,” ACM Transactions on Computing Education, vol. 19, no. 1, pp. 1–43, 2019, doi: 10.1145/3231711.[5] J. Jeuring, H. Keuning, S. Marwan, D. Bouvier, C. Izu, N. Kiesler, T. Lehtinen, D. Lohr, A. Peterson, and S. Sarsa, “Towards Giving Timely Formative Feedback and Hints to Novice Programmers,” in Proceedings of the 2022 Working Group Reports on Innovation and Technology in Computer Science Education, ser. ITiCSE-WGR ’22. New York: ACM, 2022, pp. 95–115, doi: 10.1145/3571785.3574124.[6] S. Sarsa, P. Denny, A. Hellas, and J. Leinonen, “Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models,” in Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 1. Lugano and Virtual Event Switzerland: ACM, 2022, pp. 27–43, doi: 10.1145/3501385.3543957.[7] J. Prather, P. Denny, J. Leinonen, B. A. Becker, I. Albluwi, M. Craig, H. Keuning, N. Kiesler, T. Kohn, A. Luxton-Reilly, S. MacNeil, A. Petersen, R. Pettit, B. N. Reeves, and J. Savelka, “The Robots Are Here: Navigating the Generative AI Revolution in Computing Education,” in Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education, ser. ITiCSE-WGR ’23.New York: ACM, 2023, pp. 108–159, doi: 10.1145/3623762.3633499.[8] M. Kazemitabaar, R. Ye, X. Wang, A. Z. Henley, P. Denny, M. Craig, and T. Grossman, “CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs,” 2024, doi: 10.48550/arXiv.2401.11314.[9] N. Kiesler, D. Lohr, and H. Keuning, “Exploring the Potential of Large Language Models to Generate Formative Programming Feedback,” in 2023 IEEE Frontiers in Education Conference (FIE). College Station, USA: IEEE, 2023, pp. 1–5, doi: 10.1109/FIE58773.2023.10343457.[10] A. Hellas, J. Leinonen, S. Sarsa, C. Koutcheme, L. Kujanpa¨a, and¨ J. Sorva, “Exploring the Responses of Large Language Models to Beginner Programmers’ Help Requests,” in Proceedings of the 2023 ACM Conference on International Computing Education Research Volume 1, ser. ICER ’23, vol. 1. New York: ACM, 2023, pp. 93–105, doi: 10.1145/3568813.3600139.[11] S. Jacobs and S. Jaschke, “Evaluating the Application of Large Language Models to Generate Feedback in Programming Education,” in 2024 IEEE Global Engineering Education Conference (EDUCON). Kos, Greek: IEEE, 2024, doi: 10.48550/arXiv.2403.09744.[12] T. Phung, V.-A. Padurean, A. Singh, C. Brooks, J. Cambronero,˘ S. Gulwani, A. Singla, and G. Soares, “Automating Human TutorStyle Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation,” in Proceedings of the 14th Learning Analytics and Knowledge Conference. Kyoto, Japan: ACM, 2024, pp. 12–23, doi: 10.1145/3636555.3636846.[13] A. Madasu, J. Oliva, and G. Bertasius, “Learning to Retrieve Videos by Asking Questions,” in Proceedings of the 30th ACM International Conference on Multimedia. Lisboa Portugal: ACM, 2022, pp. 356– 365, doi: 10.1145/3503161.3548361.[14] M. Wolfel, B. S. Mehrnoush, A. Reich, and K. Anderer, “Knowledge-¨ based and generative-ai-driven pedagogical conversational agents: A comparative study of grice’s cooperative principles and trust,” Big Data and Cognitive Computing, vol. 8, no. 1, pp. 1–20, 2024, doi: 10.3390/bdcc8010002.[15] S. Asthana, T. Arif, and K. C. Thompson, “Field experiences and reflections on using LLMs to generate comprehensive lecture metadata,” NeurIPS’23 Workshop on Generative AI for Education (GAIED), 2023.[16] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust Speech Recognition via Large-Scale Weak Supervision,” in Proceedings of the 40 Th International	Conference on Machine Learning,	Honolulu,	Hawaii,	2023,	doi:10.48550/arXiv.2212.04356.[17] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, Q. Guo, M. Wang, and H. Wang, “Retrieval-Augmented Generation for Large Language Models: A Survey,” Jan. 2024, doi:10.48550/arXiv.2312.10997.[18] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, “ReAct: Synergizing Reasoning and Acting in Language Models,” in International Conference on Learning Representations (ICLR), 2023, doi: 10.48550/arXiv.2210.03629.[19] M. Sallam, N. A. Salim, M. Barakat, K. Al-Mahzoum, A. B. AlTammemi, D. Malaeb, R. Hallit, and S. Hallit, “Assessing Health Students’ Attitudes and Usage of ChatGPT,” JMIR Medical Education, vol. 9, no. 1, pp. 1–15, 2023, doi: 10.2196/48254.[20] J. Sauro and J. R. Lewis, “Chapter 8 - Standardized usability questionnaires,” in Quantifying the User Experience (Second Edition), J. Sauro and J. R. Lewis, Eds.	Boston: Morgan Kaufmann, 2016, pp. 185–248, doi: 10.1016/B978-0-12-802308-2.00008-4.[21] H. Abu-Rasheed, M. H. Abdulsalam, C. Weber, and M. Fathi, “Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring,” in Joint Proceedings of the 14th International Learning Analytics and Knowledge Conference (LAK24), Kyoto, Japan, 2024, doi: 10.48550/arXiv.2401.08517.[22] H. Nori, Y. T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King,J. Larson, Y. Li, W. Liu, R. Luo, S. M. McKinney, R. O. Ness,H. Poon, T. Qin, N. Usuyama, C. White, and E. Horvitz, “Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine,” 2023, doi: 10.48550/arXiv.2311.16452.