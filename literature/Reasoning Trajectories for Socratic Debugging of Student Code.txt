Reasoning Trajectories for Socratic Debugging of Student Code:From Misconceptions to Contradictions and Updated BeliefsErfan Al-Hossami and Razvan BunescuUniversity of North Carolina at CharlotteCharlotte, NC, USA{ealhossa, rbunescu}@charlotte.edu
AbstractIn Socratic debugging, instructors guide students towards identifying and fixing a bug on their own, instead of providing the bug fix directly. Most novice programmer bugs are caused by programming misconceptions, namely false beliefs about a programming concept. In this context, Socratic debugging can be formulated as a guided Reasoning Trajectory (RT) leading to a statement about the program behavior that contradicts the bug-causing misconception. Upon reaching this statement, the ensuing cognitive dissonance leads the student to first identify and then update their false belief. In this paper, we introduce the task of reasoning trajectory generation, together with a dataset of debugging problems manually annotated with RTs. We then describe LLM-based solutions for generating RTs and Socratic conversations that are anchored on them. A large scale LLMas-judge evaluation shows that frontier models can generate up to 91% correct reasoning trajectories and 98.7% valid conversation turns.á https://github.com/taisazero/debugging_rts1	Introduction and MotivationOne of the most effective ways of improving students’ learning is through Socratic dialogue. In a Socratic dialogue, a teacher guides a learner within their zone of proximal development (Vygotsky, 2012) by asking questions and providing feedback, with the purpose of directing them towards solving a problem on their own rather than providing solutions directly. The instructor’s questions may probe a student’s existing knowledge or assumptions; guide attention to relevant aspects of a complex problem; or encourage discovery of general principles through the consideration of alternative solutions or counterexamples (Elder and Paul, 1998). Through its emphasis on active inquiry, in-context reasoning about evidence, and repeated retrieval of relevant concepts from memory, Socratic dialogue engages students in deep thinking and meaningful integration of new knowledge, which can greatly improve their acquisition of generalizable skills and ultimately their learning outcomes (Brown et al., 2023).  Socratic questioning is often used in instructional scaffolding (Quintana et al., 2004) and is effective in enhancing learning gains in code comprehension tasks (Tamang et al., 2021). While Socratic questioning can substantially improve learning outcomes, it is time-consuming and cognitively demanding, requiring human instructors to continuously assess a student’s understanding and to tailor questions to be most effective at each turn. In this paper, we introduce tools that support instructors to first plan and then articulate Socratic conversations, in the context of helping students fix buggy code. A significant part of the curriculum in beginner programming classes is allocated to programming exercises, where students are asked to solve coding problems with increasing levels of difficulty. However, when students learn to code they often develop false beliefs about various programming concepts, i.e., misconceptions, which can lead to buggy code. We assume an ideal scenario where the student has access to an Instructor, such that when the student cannot fix the bug on his own, he seeks help from her. The Instructor is assumed to be a proficient programmer, with experience in teaching novice programmers. When contacted by the Student for help, she aims to optimize learning by following a Socratic approach, where over one or more dialogue turns, she guides the student towards figuring out on his own the misconception causing the bug. Henceforth, we use the term SOCRATIC DEBUGGING (Al-Hossami et al., 2023, 2024) to refer to the ensuing dyadic conversation between a Student and an Instructor.  In a Socratic debugging approach, it is essential that the student himself realizes which of his programming beliefs are false, i.e., misconceptions.Figure 1: Socratic debugging example: (a) the input specifies the problem, the buggy code, the failed test case, and the student misconception that caused the bug; (b) a reasoning trajectory ending with a statement that contradicts the misconception; (c) a Socratic conversation that follows the reasoning trajectory and ends with a belief update.By guiding the student to discover and fix a misconception on his own, the instructor also maximizes the likelihood that the fixed belief will endure over time and not revert to the initial false belief. To achieve this aim, we propose that the Instructor guide the student along a sequence of inferences about the code behavior for a failed test case. The reasoning trajectory is designed such that the last inference step proves a statement that is in direct contradiction with the student’s misconception. This overt contradiction between the false belief and the actual code behavior is expected to create a strong cognitive dissonance (Festinger, 1959) for the student, who consequently not only realizes which of his beliefs is false, but also corrects it on his own, as shown in the example in Figure 1. In general, the psychological discomfort associated with cognitive dissonance has been found empirically to be extremely motivating in terms of triggering learning processes that seek to resolve the dissonance (Zanna and Cooper, 1976; Elliot and Devine, 1994). As described in (Adcock, 2012), placing learners in a state of cognitive dissonance is ideal for learning in problem-solving scenarios, where the intrinsic human need for consistency and equilibrium leads to a constant process of examining new information and updating existing knowledge structures (Piaget, 1975).  The rest of the paper is structured as follows: in Section 2 we provide a definition of reasoning trajectories and Socratic conversations that are anchored in them; in Section 3 we describe the benchmark dataset created to support the development and evaluation of LLM-based approaches for articulating RTs and Socratic turns, which we introduce in Sections 4.1 and 4.2, respectively; in Section 5 we present and discuss experimental results, whereas in Section 6 we summarize related work. The paper ends with conclusion and thoughts on future work in Section 7.2	Task DefinitionAs shown in Figure 1(a), the input to the Socratic debugging task consists of a problem description, the student’s buggy code, a failed test case, and the student’s misconception that caused the bug. Consistent with the aforementioned aim of guiding the student towards discovering his own misconception, we approach the task of Socratic debugging as a pipeline of two main subtasks:1. Reasoning Trajectory (RT): In the first step, a reasoning trajectory is generated as a sequence of inference steps such that the statement proven in the last step contradicts the misconception or provides a counterexample to the misconception, as shown in Figure 1(b).2. Socratic Conversation (SC): In the second step, a Socratic conversation is generated step by step, such that each RT step is associated with an Instructor turn followed by a Student turn, where the instructor’s question aims to elicit from the student the statement proven at that step, as illustrated in Figure 1(c).The reasoning trajectory shown in Figure 1(b) is structured in two parts. In the first part, the reasoning steps lead to showing a statement of the student’s misconception for the failed test case, namely that the expressions 1 + 3 / 2 evaluatesFigure 2: Alternative reasoning trajectory for the input from Figure 1(a).as 2. In the second part, the reasoning proceeds backwards from the returned value in order to infer a statements that contradicts the misconception statement, namely that 1 + 3 / 2 evaluates to ±2.5. Note that this is not the only way of articulating an RT that ends with a statement contradicting the misconception. Figure 2 shows an alternative RT where the reasoning steps end with a statement that is the opposite of the misconception statement. Thus, while the RT from Figure 1 (b) can be seen as providing a counterexample to the misconception statement by instantiating it for a particular failed test case, the RT in Figure 2 does not instantiate the misconception statement and instead proves a statement that contradicts the misconception statement in the general case. Given that the first type of RTs are generally shorter, in this paper we focus on generating RTs that derive counterexamples to the student misconception. The LLM-based approach for generating reasoning trajectories is described in Section 4.1.  Once a reasoning trajectory is generated, it is used step by step to generate a corresponding Socratic conversation. As shown in Figure 1(c), the Socratic conversation is structured in three parts. The first part contains a generic, initial statement from the instructor, while the student’s turn describes the failed test case. The turns in the second and third parts map to the steps in the first and second parts of the RT, respectively. For each RT step, the instructor asks a question that aims to guide the student towards articulating the statement proven at that step. Note that although we generate a Socratic turn for each RT step, it is possible for the instructor to skip one or more steps if she determines that the student is capable of making one or more inferences on his own, without guidance. For example, right after turn 3, the instructor can choose to skip turn 4 and go directly to turn 5. The LLM-based approach for generating reasoning trajectories is described in Section 4.2.2.1	SimplificationAs shown in Figure 1(b), the reasoning trajectory is structured in two parts: the first part leads to an instance of the misconception, whereas the second part leads to a statement that contradicts it. It is important for both parts in this reasoning process to be short, otherwise a long and complicated reasoning trajectory can place a significant cognitive burden on the student, which will defeat the aim of Socratic guidance. Therefore, to keep the complexity of the reasoning traces at a feasible level, we envision a simplification process where the original problem description, code, and failed test case are simplified such that (a) they focus on the code behavior that instantiates the misconception, while (b) they stay as close to the original as possible. In Figure 3 we show an example input, where formulating a reasoning trajectory would be overly complicated due to the many calls to the function about which the student has a misconception, and the length of the input string. Furthermore, the student has two misconceptions, whereas by definition a reasoning trace corresponds to just one misconception. While it is possible to merge the two misconceptions into a general misconception that subsumes both, e.g., "string methods can modify the string object", it is easier for the student to address concrete misconceptions, one at a time. Correspondingly, the original task is simplified as shown in Figure 4, whereas the corresponding RT is shown in in Figure 5.3	DatasetWe leverage the problem-solution and misconception dataset introduced in the MCMINING dataset from (Al-Hossami and Bunescu, 2025) containing 501 problems, 558 solutions, and a subset of 40 bug-inducing misconceptions. Given the 558 coding solutions and an input misconception, we developed a construct-based pairing algorithm that identifies the most suitable solutions that rely on the programming concept referenced by the misconception, ending with 250 solutions connected with misconceptions through a programming con-Figure 3: The original input specification.Figure4: ThesimplifiedinputfortheoriginalinFigure3.Figure 5: The RT for the simplified input in Figure 4.struct. The pseudocode is detailed in Appendix B.  The ?problem, solution, misconception? triplets are then used as input to the MCINJECT tool introduced in (Al-Hossami and Bunescu, 2025), which generates buggy code samples by injecting the misconception in the correct solution. To ensure the misconception is correctly exhibited in the buggy code, we use MCINJECT with up to 3 refinementComponentCountProblems501Solutions558Misconceptions40?Problem, Solution, Misconception? triplets227HandwrittenReasoning Trajectories10Total RT steps57LLM-GeneratedLLM configurations14Total RT steps22,506Table 1: Overall dataset statistics. Each of the 14 LLM configurations generated reasoning trajectories for all 227 problems, with the number of steps varying by model, as shown in Table 2.iterations. The refinement process uses an LLM-asjudge to determine whether the buggy code exhibits a misconception or not, providing feedback to MCINJECT if the code does not yet exhibit the misconception (Al-Hossami and Bunescu, 2025). When used in this way, MCINJECT generated 250 corrupted code samples. Of these, 43 samples were filtered out: 17 due to not being buggy (i.e., they passed all the unit tests), and 26 due to not exhibiting the misconception. To the remaining 207 samples we added 20 handwritten samples, yielding a final dataset of 227 buggy code samples exhibiting an associated misconception. For each buggy code sample, we use an LLM connected to a code execution tool to identify and describe the simplest test case that the buggy code fails. This procedure is described in more detail in Appendix D.  The problem description, buggy code sample, failed test case description, and misconception description were then used as input for the Socratic debugging pipeline, where first a reasoning trajectory is generated (Section 4.1), and then as input for generating a Socratic conversation (Section 4.2). We use 14 different LLM configurations to generate reasoning trajectories and Socratic conversations, as described in detail in Appendix C. The overall statistics of the dataset are summarized in Table 1. 4 Socratic Debugging PipelineThe generation of Socratic debugging conversations is implemented as a pipeline of two steps. First, a reasoning trajectory is generated that starts from the failed test case and ends with a correct statement about the buggy code behavior that contradicts the student’s misconception. Then, the RT is used as a plan for generating a Socratic conversation, where each reasoning step is associated a Socratic turn composed of an Instructor utterance followed by a Student utterance.4.1	Reasoning TrajectoriesGiven a problem description, the buggy code, a failed test case, and a misconception, an LLM is instructed to generate a sequence of deductive reasoning steps that culminate in a statement contradicting the student’s false belief.  Figure 6 shows the prompt template used for RT generation.1 We employ a 2-shot prompting approach, which includes two worked examples and structured input and output formats. The prompt emphasizes five core principles that guide the generation process: (1) strict deductive reasoning with no logical leaps or abductive inferences; (2) consistency with the student’s misconception, avoiding the use of programming knowledge that would contradict their false belief, e.g., if the student believes that range(n) starts at 1, the RT should not use that fact that range(n) starts at 0; (3) exclusive focus on contradicting the misconception rather than providing fixes; (4) starting from observable facts in the failed test case; and (5) sequential reasoning steps with explicit citation of premises.  These principles ensure that generated RTs maintain logical rigor and focus on deducing a statement that contradicts the misconception. Each inference step must follow necessarily from previously established facts and correct knowledge of Python programming that does not contradict the student’s misconception. By requiring consistency with the misconception at every intermediate step, we ensure the reasoning steps can be achieved by students who hold that false belief, making the eventual contradiction at the last step more impactful in terms of the cognitive dissonance that it produces.4.2	Socratic TurnsBuilding on the generated reasoning trajectories, we approach Socratic conversation generation as a sequential dialogue turn generation where each RT step anchors an instructor-student exchange. The instructor’s utterances are intended to guide the student to make the inferences described at each RT step, instead of providing the inference step directly to the student. For example, if the RT step proves that range(1) must have produced the value 0, the teacher should ask a question like "Where did theYour TaskYou will be given a problem description, buggy code, a failed test case, and a student misconception. Your task is to write a reasoning trajectory: a sequence of rigorous, deductivereasoning steps thatprovea statement contradicting the misconception.Core Principles1. Strictly deductive: Each step must be a necessary logical consequence of previous steps, correct programming language knowledge, and observable facts.2. Consistent with misconception: Do not assume programming knowledge that contradicts the student’s false belief.3. Focus on disproving misconception: End whenreaching a statement that contradicts the misconception. Do not show the correct fix.4. Start from failed test: Begin with observable facts from the failed test case and trace program state throughout execution.5. Sequential labeling: Label steps as A.1, A.2, ..., A.n. Reference non-adjacent prior steps when used.Input Format<problem>[problem_description]</problem><bug_code>[buggy_code]</bug_code><failed_test>[failed_test]</failed_test><misconception>[misconception]</misconception>Output FormatStepA.1:[Observablefact(s)fromfailedtest] ...StepA.k:[Deducedfact(s)usingprevioussteps] ...StepA.n:[Statementcontradictingmisconception]Figure 6: Reasoning trajectories prompt template. The full template includes worked examples demonstrating codetracingandprooftechniquessuchasloopinvariants.value 0 come from?" rather than "Isn’t it true then that range(1) must have produced the value 0?".  Figure 7 shows the prompt template for Socratic conversation generation. We employ a 1-shot prompting approach that includes a worked example showing the complete conversation associated with a reasoning trajectory. The prompt takes as input the complete reasoning trajectory along with the problem specification. The generated conversations follow a natural dialogue structure where the teacher begins by inquiring about the encountered issue, and subsequent turns systematically work through each RT step. Each teacher utterance corresponds to one RT step, aiming to elicit from the student the statement proven at that step. This one-to-one correspondence with the underlying reasoning trajectory ensures that the dialoguemaintains logical coherence while preserving the pedagogical value of instructor-guided discovery.Your TaskYou will be given a Reasoning Trajectory (RT), which is a sequence of reasoning steps ending with a statement that disproves a student’s misconception. Your task is to write a Socratic conversation between a Teacher and a Student that guides the student to articulate, at each turn, the statement proven at that RT step. The Teacher should not provide statements directly but ask questions that prompt the student to infer them independently.Guidelines• Natural conversation: Teacher utterances should be direct, clear, and concise. Avoid phrases like “That’s an interesting point” or “Good question.”• Socratic approach: Ask open-ended questions that require reasoning. Do not state the inference and ask for confirmation.• RT correspondence: Each Teacher utterance prompts step A.X, and each Student response corresponds to A.X.Formatting and Structure• Use Teacher: and Student: as speaker labels• Conversation begins with Teacher inquiring about the issueInput Format<problem>[problem_description]</problem><buggy_code>[buggy_code]</buggy_code><failed_test>[failed_test]</failed_test><misconception>[misconception]</misconception><rt>[reasoning_trajectory]</rt>Figure 7: Prompt template for Socratic conversation generation. The full template includes a worked example demonstrating the correspondence between RT steps and dialogue turns.5	Experimental EvaluationWe benchmark six state-of-the-art LLMs on their ability to generate valid reasoning trajectories and Socratic conversations: GPT-5, GPT-5-mini, ClaudeSonnet-4.5, ClaudeHaiku-4.5, Gemini 2.5-flash, and Gemini 2.5-pro. The 6 LLMs are evaluated in 14 total configurations with varying levels of reasoning and different hyperparameters, as described in detail in Appendix C. All experiments leverage the API from the respective LLM providers.5.1	LLM-as-Judge MethodologyThe sheer number of generated RT steps, over 22K as indicated in Table 1, prohibits manual evaluation. Consequently, for both RT and Socratic conversation evaluation, we turn to using an LLM-as-judge approach, where:1. A suitably instructed LLM is first shown to be a reliable evaluator by manually verifying its decision on a small subset of examples(Section 5.1.1).2. The LLM is then deployed to automatically evaluate all generated trajectories and conversations (Section 5.1.2).A priori, using the LLM-as-judge for LLM-based generations is sensible considering that, in general, verification is much easier than generation, e.g., determining whether a sequence of reasoning steps is sound is much easier than generating a sequence of reasoning steps that disproves a misconception.5.1.1	Evaluating the LLM-as-JudgeTo evaluate the reliability of the LLM-as-judge, we conducted a manual evaluation of the LLMas-judge output on a subset of 30 RT samples, 10 from each of three models: Claude Sonnet-4.5 with reasoning, Gemini 2.5-pro with reasoning, and GPT-5 with medium reasoning effort. For each sample, we generated reasoning trajectories and Socratic conversations using the model configurations specified in Appendix C. We then evaluated these outputs using Claude Sonnet-4.5 with extended thinking as the LLM judge, by applying the evaluation criteria described in Appendices E.1 and E.2. One of the authors then independently evaluated the same 30 samples for RT validity. Correspondingly, we observed a 76.7% agreement between the LLM judge and the human on reasoning trace evaluation. We also use a subset of 88 teacher utterances to manually evaluate the Socratic turn quality using the same criteria as the LLM-as-judge. Correspondingly, we observed a96.6% agreement on Socratic turn evaluation.  In RT validation, the LLM judge occasionally struggles with detecting when RTs rely on programming knowledge that contradicts misconceptions and misses technical inaccuracies in terminology (e.g., using “conditional expression” to mean a boolean condition when it actually refers to Python’s ternary operator x if C else y). InSocratic turn validation, the judge demonstrates strong reliability on clear-cut cases and consistently detects and penalizes teacher utterances consisting of rhetorical questions seeking confirmation. The judge occasionally makes evaluation mistakes whereby it penalizes useful conversational framing, e.g., "Let’s trace through the code", by motivatingTable 2: Performance of language models on reasoning trajectory generation and Socratic conversation generation. RT Steps shows total steps across all 227 samples. Valid Convs measures whether all teacher turns in a conversation are grounded in the RT; Grounded Turns measures the percentage of individual teacher turns that are properly grounded.that it is not relevant to eliciting the target reasoning step from the student. For detailed failure patterns in both RT and Socratic turn evaluation, see Appendices H.1 and H.2, respectively.5.1.2	Using the LLM-as-JudgeTo evaluate the RT generation step, we employ the LLM-as-judge approach with structured criteria across four major categories: logical soundness, step construction, precision, and focus, where a correct RT must satisfy all criteria. We then compute the percentage of correct RTs for each model. For more details on the RT evaluation setup, including the evaluation of the LLM-as-judge itself, see Appendix E.1.  To evaluate the quality of generated Socratic turns, we employ an LLM-as-judge approach as well, with two key criteria: whether the teacher utterance elicits the correct inference from the corresponding RT step, and whether it avoids stating that inference directly. For a teacher Socratic utterances to be deemed correct, it must satisfy both criteria. We then compute the percentage of valid Socratic turns for each model. Lastly, we compute the percentage of valid Socratic conversations for each model, where a valid conversation must have all valid teacher utterances grounded in the corresponding RT step. For more details on the methodology for Socratic conversation evaluation, see Appendix E.2.5.2	Results and DiscussionThe results from all 14 LLM configurations are summarized in Table 2 and reveal several key findings. First, reasoning trajectory quality varies considerably, with GPT-5 achieving the highest validity rates between 85?91%, while generating relatively concise trajectories. Notably, extended reasoning capabilities do not uniformly improve performance. Claude models benefit substantially from reasoning mode: Claude Sonnet-4.5 with +6.6% in RT validity and Claude Haiku-4.5 with +16.3% in RT validity. Similarly, GPT-5 models perform better with increased reasoning effort. In contrast, the results from Gemini models are mixed, with 2.5-flash performing slightly worse when reasoning is enabled, with ?0.8% in RT validity, while generating more reasoning steps.  We also observe a slight inverse relationship between trajectory length and validity: GPT-5 medium-effort produces the fewest total steps (1,271) and achieves the highest RT validity (91.1%), while Claude Haiku-4.5 without reasoning generates the most steps (1,962) but has the lowest validity (62.6%). This is somewhat to be expected, given that the more reasoning steps are contained in an RT, the more chances for one of them to be invalid, which then, according to our evaluation methodology, invalidates the entire RT.  Generally, once a reasoning trajectory is generated, the Socratic conversation generation process is relatively straightforward and consistent across different models. Most LLMs are able to generate valid Socratic utterances grounded in the input reasoning trajectory, and they consistently do so throughout an entire conversation.  Qualitative analysis of generated reasoning trajectories reveals that successful RTs exhaustively eliminate alternative possibilities, use concrete execution tracing, and end with clear contradictions. Failure modes include relying on knowledge that contradicts misconceptions and employing abductive rather than reasoning. These patterns are detailed in Appendix F.  Qualitative analysis of generated Socratic utterances reveals that LLMs demonstrate accurate RT step alignment, with no observed cases of questions eliciting entirely different reasoning steps than intended. Models successfully integrate facts from prior reasoning steps into coherent questions and employ implicit elicitation, where questions prime students to provide complete logical steps beyond what is explicitly requested. For instance, asking "What expression is evaluated on line 5?" implicitly elicits both the abstract expression (e.g. x = 1 + 5) and its concrete evaluation (x = 6), without requiring separate prompts for each component. A notable failure pattern includes teacher utterances occasionally stating conclusions directly rather than prompting the student to derive them. These patterns are detailed in Appendix G.6	Related WorkScaffolding enables learners to achieve goals through guided efforts (Wood et al., 1976), and Socratic Questioning (SQ) represents a conversational form of scaffolding where a knowledgeable person helps learners solve problems beyond their current abilities (Wood et al., 1976; Quintana et al., 2004; Vygotsky, 2012). Wood (1994) identified two key questioning types: funneling, which guides learners toward solutions through sequential questions, and focusing, which directs attention to important problem aspects and encourages reflection (Wood, 1994; National Council of Teachers of Mathematics, 2014; Alic et al., 2022). While students can complete programming exercises yet struggle to explain their code (Lehtinen et al., 2021), Tamang et al. (2021) demonstrated that Socratic methods effectively improve code comprehension. However, the impact of Socratic questioning on debugging learning outcomes remains unexplored.  Prior AI work in programming education includes intelligent tutoring systems (ITS) and learning support systems that provide automated feedback, generate exercises, and create code explanations (Sarsa et al., 2022). Most ITS models use pre-LLM methods like action-rules and Bayesian networks (Crow et al., 2018; Mousavinasab et al., 2021; Costello, 2012; Butz et al., 2006). Recent work has shown that computer-based scaffolding techniques have a moderate impact on STEM learning (Kim et al., 2018), with approaches like automatically generating Socratic questions for math problems using fine-tuned language models (Shridhar et al., 2022; Macina et al., 2023, 2025). Furthermore, several open source LLMs have been fine-tuned on a large amount of synthetic tutoring conversations in mathematics (Liu et al., 2025) and over 100,000 hours of real tutoring conversations in multiple subjects (Perczel et al., 2025).  Automated hint generation systems aim to assist programming students through instant feedback using techniques like extracting common bugs (Lee et al., 2018), analyzing peer data patterns (Iii et al., 2014; Lazar et al., 2017), and generating custom solution paths (Rivers and Koedinger, 2017; McBroom et al., 2021). AI tutoring for formal proving in mathematics such as the LeanTutor (Patel et al., 2025), rely on generating three types of hints: an identification of the error, a single guiding question, an explicit suggestion for the next step, and does not engage in a complete Socratic conversation. Lu and Krishnamurthi (2024) present an approach to identifying and correcting student misconceptions about programming language behavior through "misinterpreters", pre-programmed interpreters that can deterministically detect misconceptions about programming language semantics. Their SMoL Tutor uses refutation texts to explicitly address these misconceptions during MCQ quizzes.  Our approach focuses on the diagnosis and correction of misconceptions in buggy code through complete Socratic dialogue. Unlike prior work, we plan Socratic conversations such that they engage the student in a particular type of reasoning about the buggy code behavior, where they are guided towards inferring a correct statement about the actual code execution that conflicts with their misconception. As argued in Section 1, reaching this moment of cognitive dissonance is important in that it is expected to trigger a Eureka moment for the student, where they suddenly realize which of their programming beliefs is false, followed by an enduring belief update that fixes the misconception.7	Conclusion and Future WorkWe introduced a novel formulation of Socratic debugging, where the teacher utterances aim to follow a reasoning trajectory that starts from a failed test case, and upon a sequence of inference steps reaches a correct statement about the program that is in contradiction with the student misconception that caused the bug. Upon reaching this statement, the student is expected to experience a strong cognitive dissonance, which then entails an enduring belief update. To support development and evaluation, we created a dataset of 227 problems paired with buggy solutions and the corresponding bug-causing misconception. A large scale LLM-asjudge evaluation of over 22K reasoning trajectory steps and their associated Socratic utterances shows that frontier models can achieve up to 91% trajectory validity and 98.7% conversation validity. Overall, through carefully orchestrated moments of cognitive dissonance, the proposed automated Socratic guidance approach can be of significant benefit to instructors seeking to help students durably fix their programming misconceptions.  In future work, we plan to develop approaches for simplifying the input to Socratic debugging in order to lessen the cognitive demands on the student by focusing on the code behavior that contains the misconception while staying as close to the original buggy code as possible.AcknowledgmentsWe are grateful to Srijan Das for providing access to GPT models through the NAIRR Pilot initiative from the NSF (NAIRR240338). The project has also benefited from the Microsoft Accelerating Foundation Models Research (AFMR) grant program. This research was partly supported by the United States Air Force (USAF) under Contract No. FA8750-21-C-0075. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the USAF.ReferencesAmy Adcock. 2012. Cognitive Dissonance in the Learning Processes, pages 588–590. Springer US, Boston, MA.Erfan Al-Hossami and Razvan Bunescu. 2025. Mcmining: Automated discovery of misconceptions in student code. arXiv preprint arXiv:2510.08827.Erfan Al-Hossami, Razvan Bunescu, Justin Smith, and Ryan Teehan. 2024. Can language models employ the socratic method? experiments with code debugging. InProceedingsofthe55thACMTechnicalSymposium on Computer Science Education V. 1, SIGCSE 2024, page 53–59, New York, NY, USA. Association for Computing Machinery.Erfan Al-Hossami, Razvan Bunescu, Ryan Teehan,Laurel Powell, Khyati Mahajan, and Mohsen Dorodchi. 2023. Socratic questioning of novice debuggers: A benchmark dataset and preliminary evaluations. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 709–726, Toronto, Canada. Association for Computational Linguistics.Sterling Alic, Dorottya Demszky, Zid Mancenido, Jing Liu, Heather Hill, and Dan Jurafsky. 2022. Computationally identifying funneling and focusing questions in classroom discourse. BEA 2022, page 224.Jacob Austin, Augustus Odena, Maxwell Nye, MaartenBosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and 1 others. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.Neil C. C. Brown, Felienne F. J. Hermans, and Lauren E. Margulieux. 2023. 10 things software developers should learn about learning. Commun. ACM,67(1):78–87.Cory J Butz, Shan Hua, and R Brien Maguire. 2006. A web-based bayesian intelligent tutoring system for computer programming. Web Intelligence and Agent Systems: An International Journal, 4(1):77–97.Robert Costello. 2012. Adaptive intelligent personalised learning (aipl) environment. Ph.D. thesis.Tyne Crow, Andrew Luxton-Reilly, and BurkhardWuensche. 2018. Intelligent tutoring systems for programming education: a systematic review. In Proceedings of the 20th Australasian Computing Education Conference, pages 53–62.Adrian de Freitas, Joel Coffman, Michelle de Freitas,Justin Wilson, and Troy Weingart. 2023. Falconcode: A multiyear dataset of python code samples from an introductory computer science course. In Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1, SIGCSE 2023, page 938–944, New York, NY, USA. Association for Computing Machinery.Linda Elder and Richard Paul. 1998. The Role of Socratic Questioning in Thinking, Teaching, and Learning. The Clearing House: A Journal of Educational Strategies, Issues and Ideas, 71(5):297–301.Andrew J Elliot and Patricia G Devine. 1994. On the motivational nature of cognitive dissonance: Dissonance as psychological discomfort. Journal of personality and social psychology, 67(3):382.Andrew Ettles, Andrew Luxton-Reilly, and Paul Denny. 2018. Common logic errors made by novice programmers. In Proceedings of the 20th Australasian Computing Education Conference, ACE’18, page 83–89, New York, NY, USA. Association for Computing Machinery.L Festinger. 1959. Cognitive dissonance. New York.Barry Peddycord Iii, Andrew Hicks, and Tiffany Barnes. 2014. Generating hints for programming problems using intermediate output. In Educational Data Mining 2014. Citeseer.Nam Ju Kim, Brian R Belland, and Andrew E Walker. 2018. Effectiveness of computer-based scaffolding in the context of problem-based learning for STEM education: Bayesian meta-analysis. Educational Psychology Review, 30:397–429.Timotej Lazar, Martin Mo?ina, and Ivan Bratko. 2017. Automatic extraction of ast patterns for debugging student programs. In Artificial Intelligence in Education: 18th International Conference, AIED 2017,Wuhan, China, June 28–July 1, 2017, Proceedings 18, pages 162–174. Springer.Victor CS Lee, Yuen-Tak Yu, Chung Man Tang,Tak-Lam Wong, and Chung Keung Poon. 2018. Vida: A virtual debugging advisor for supporting learning in computer programming courses. Journal of Computer Assisted Learning, 34(3):243–258.Teemu Lehtinen, Aleksi Lukkarinen, and Lassi Haaranen. 2021. Students struggle to explain their own program code. In Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1, pages 206–212.Jiayu Liu, Zhenya Huang, Tong Xiao, Jing Sha, Jinze Wu, Qi Liu, Shijin Wang, and Enhong Chen. 2025.Socraticlm: exploring socratic personalized teaching with large language models. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS ’24, Red Hook, NY, USA. Curran Associates Inc.Kuang-Chen Lu and Shriram Krishnamurthi. 2024. Identifying and correcting programming language behavior misconceptions. Proc. ACM Program. Lang., 8(OOPSLA1).Jakub Macina, Nico Daheim, Sankalan Chowdhury,Tanmay Sinha, Manu Kapur, Iryna Gurevych, and Mrinmaya Sachan. 2023. MathDial: A dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems. In Findings of the Association for Computational Linguistics: EMNLP2023, pages 5602–5621, Singapore. Association for Computational Linguistics.Jakub Macina, Nico Daheim, Ido Hakimi, Manu Kapur, Iryna Gurevych, and Mrinmaya Sachan. 2025. Mathtutorbench: A benchmark for measuring open-ended pedagogical capabilities of llm tutors. arXiv preprint arXiv:2502.18940.Jessica McBroom, Irena Koprinska, and Kalina Yacef. 2021. A survey of automated programming hint generation: The hints framework. ACM Computing Surveys (CSUR), 54(8):1–27.Elham Mousavinasab, Nahid Zarifsanaiey, Sharareh R. Niakan Kalhori, Mahnaz Rakhshan, Leila Keikha, and Marjan Ghazi Saeedi. 2021. Intelligent tutoring systems: a systematic review of characteristics, applications, and evaluation methods. Interactive Learning Environments, 29(1):142–163.National Council of Teachers of Mathematics. 2014. Principles to actions: Ensuring mathematical success for all. NCTM, National Council of Teachers of Mathematics, Reston, VA.Manooshree Patel, Rayna Bhattacharyya, Thomas Lu, Arnav Mehta, Niels Voss, Narges Norouzi, and GireejaRanade.2025. Leantutor: Aformally-verified ai tutor for mathematical proofs. arXiv preprint arXiv:2506.08321.Janos Perczel, Jin Chow, and Dorottya Demszky. 2025. Teachlm: Post-training llms for education using authentic learning data. arXiv preprint arXiv:2510.05087.Jean Piaget. 1975. The equilibration of cognitive structures: The central problem of intellectual development. University of Chicago Press.Chris Quintana, Brian J. Reiser, Elizabeth A. Davis,Joseph Krajcik, Eric Fretz, Ravit Golan Duncan, Eleni Kyza, Daniel Edelson, and Elliot Soloway. 2004. A scaffolding design framework for software to support science inquiry. Journal of the Learning Sciences, 13(3):337–386.Kelly Rivers and Kenneth R Koedinger. 2017. Datadriven hint generation in vast solution spaces: a self-improving python programming tutor. International Journal of Artificial Intelligence in Education, 27:37–64.Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. 2022. Automatic generation of programming exercises and code explanations using large language models. In Proceedings of the 2022 ACM Conference on International Computing Education Research Volume 1, ICER ’22, page 27–43, New York, NY, USA. Association for Computing Machinery.Kumar Shridhar, Jakub Macina, Mennatallah El-Assady, Tanmay Sinha, Manu Kapur, and Mrinmaya Sachan. 2022. Automatic generation of socratic subquestions for teaching math word problems. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4136–4149,Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.Lasang Jimba Tamang, Zeyad Alshaikh, Nisrine Ait Khayi, Priti Oli, and Vasile Rus. 2021. A comparative studyoffreeself-explanationsandsocratictutoringexplanationsforsourcecodecomprehension. NewYork, NY, USA. Association for Computing Machinery.Lev S Vygotsky. 2012. Thought and language. MIT press.DavidWood, JeromeSBruner, andGailRoss.1976. The role of tutoring in problem solving. Child Psychology & Psychiatry & Allied Disciplines.Terry Wood. 1994. Patterns of interaction and the culture of mathematics classrooms. In Cultural perspectives on the mathematics classroom, pages 149–168. Springer.Mark P Zanna and Joel Cooper. 1976. Dissonance and the attribution process. New directions in attribution research, 1:199–217.
A	Problem and Misconception Dataset SourcesWe use a problem-solution set containing 558 solutions across 501 problems from the MCMINING dataset (Al-Hossami and Bunescu, 2025). This dataset contains 438 problems from MBPP (Austin et al., 2021), 27 from Socratic Debugging (Al-Hossami et al., 2023, 2024), 8 from Auckland (Ettles et al., 2018), 6 from FalconCode (de Freitas et al., 2023), and 19 handwritten solutions. The MCMINING dataset also contains 67 misconceptions, with 40 of them being bug-inducing misconceptions. We select all 40 bug-inducing misconceptions from the dataset and use them for our experiments.B	Pairing Misconceptions with Program SolutionsTo ensure plausibility in LLM-generated corrupted code, we developed an automated construct-based pairing algorithm that generates 250 high-quality (misconception, solution) pairs. These pairs serve as input to MCINJECT, ensuring each misconception is matched with solutions containing the necessary programming constructs. Without proper construct matching, two failure modes occur: (1) inapplicable misconceptions, where the pattern cannot be applied (e.g., a class-related misconception on code without classes), and (2) implausible code, where the LLM seems to exhibit the misconceptions but lacks logical coherence. A student with that misconception would not have written such code and genuinely believed that it would solve the programming problem.Phase 1: Programming Construct Extraction We extract fine-grained programming constructs from each solution using a combination of abstract syntax tree (AST) parsing and pattern matching. Our implementation uses Python’s ast module to parse the syntax tree and identify control flow structures (loops, conditionals), function definitions, operators, and data structures. Additionally, we employ regular expressions (re module) to detect patterns not easily captured by AST analysis, such as method chaining (e.g., x.a().b()), specific string methods (e.g., .upper(), .split()), and particular operator combinations relevant to precedence misconceptions.  This process analyzes all 558 solutions across 501 problems from our problem-solution set described in Appendix A. The extraction produces solution-level constructs, identifying over 80 distinct construct types, including specific language features like list.append, str.split, recursion patterns, and class initialization.Phase 2: Construct-Overlap Pairing Given extracted constructs, we generate 250 (misconception, solution) pairs through semantic alignment based on construct overlap. For each misconception m hand-annotated with related_constructs (e.g., “for loops”, “indexing”, “range function”), we compute an overlap score with each solution s as:	score(m,s)=|constructs[m]?constructs[s]|	(1)  For regular misconceptions, only pairs with score(m,s)≥ 1 are considered, ensuring compatibility. However, 16 misconceptions require special case handling where code pattern matching can override the overlap requirement: for instance, recursion misconceptions require actual recursive function calls verified through code analysis, class misconceptions require __init__ methods, and operator precedence misconceptions require specific operator combinations like + and / in the same expression. For these special cases, solutions are accepted if they either have construct overlap or satisfy the required code pattern.  The algorithm employs round-robin allocation, cycling through misconceptions while selecting the highest-scoring unused solution for each. This ensures diversity (each solution used at most once)while handling varying construct availability. Common constructs like loops yield 13-14 pairs per misconception, while rare constructs like method chaining yield only 2-4 pairs out of the 250 pairs.  Algorithm 1 presents the complete procedure. The algorithm uses an auxiliary function IsSpecialCase(m,s) that returns true if misconception m and solution s match one of 16 special case patterns requiring code verification (e.g., recursion misconceptions paired with solutions containing actual recursive function calls, class misconceptions with solutions containing __init__ methods, or operator precedence misconceptions with specific operator combinations like + and /). The output is 250 validated Algorithm 1 Construct-Based Misconception-Solution PairingRequire: Solutions S with extracted constructs, Misconceptions M with constructs, Target count NEnsure: Pairings1: Phase 1: Extract Constructs2: for each solution s?S do3:	constructs[s]?ExtractConstructs(s) {AST + regex}4: end for 5: Phase 2: Generate Pairs6: used?? {Track used solutions}7: D?? {Result pairings}8: i?0 {Round-robin index}9: while |D|<N do10:	m?M[i mod |M|] {Current misconception}11:	candidates??12:	for each solution s?S\used do13:	score?|constructs[m]?constructs[s]|14:	if score≥1 or IsSpecialCase(m,s) then 15:	candidates?candidates?{(s,score)}16:	end if 17:	end for18:	if candidates?=? then19:	s??argmax(s,score)?candidatesscore20:	D?D?{(m,s?)}21:	used?used?{s?}22:	end if23:	i?i+124: end while 25: return Dpairs stored with metadata, including overlap scores and matching constructs for reproducibility and analysis. Our implementation uses standard Python libraries: ast for abstract syntax tree parsing, re for regular expression pattern matching, and json for data serialization. The entire process is fully deterministic.C	LLM Hyper-parametersOur experiments evaluate three state-of-the-art LLMs via their respective APIs, each configured with model-specific parameters:• OpenAI GPT-5: We employ the gpt-5 and gpt-5-mini models using the Responses API, which provides built-in reasoning capabilities. The model supports four reasoning effort levels: minimal, low, medium. We avoid using the high reasoning effort level since we observed issues with the batching API. We set max_output_tokens to 4000 and configure text verbosity to medium. Unlike traditional models, the GPT-5 family does not support temperature configuration, as reasoning effort directly controls the model’s deliberation intensity.• Anthropic Claude: We utilize the claude-sonnet-4-5 and claude-haiku-4-5 models with two operational modes. For standard generation, we apply a temperature of 0.1 and max_tokens 4000. When extended thinking is enabled, we increase temperature to 1.0 (as mandated by Anthropic’sAPI), allocate an additional 2000 tokens for the thinking budget (yielding 6000 total tokens), and activate the thinking mode with budget_tokens set to 2000.• Google Gemini: We use the gemini-2.5-flash and gemini-2.5-pro models with temperature 0.1 and max_output_tokens 4000 for baseline experiments. For reasoning-enabled experiments, we augment max_output_tokens by an additional 2000 tokens (totaling 6000), and configure thinking_configwith include_thoughts enabled and thinking_budget set to 2000.  All experiments utilize the LLM provider APIs. The complete evaluation prompt specifications are provided in Appendix E while the reasoning trajectory generation prompt template is provided in §4.1 and the Socratic turn generation is provided in §4.2.D	Failed Test Case GenerationFor each buggy code sample in our dataset, we generate a concise description of the simplest failing test case to serve as input for reasoning trajectory generation. To do so, we execute all the unit tests associated with each problem using a Python interpreter and capture detailed execution results including passed tests, failed tests with output mismatches, runtime errors, and syntax errors.  Figure 8 shows the prompt template for failed test case description generation. Given the problem description, buggy code, and execution results, the LLM selects the simplest failing test according to a priority order: syntax errors first (affecting all tests), then runtime errors, then logical errors with simplest inputs. The output is a one-sentence description with description writing conventions based on the error type, ensuring consistency across the dataset.Your TaskGiven a Python problem, buggy code, and execution results, select the simplest failing test case and write a concise description of how it fails.Selection Strategy1. Syntax errors: If present, all tests fail the same way2. Runtime errors: Select the test with simplest inputs that raises the error3. Logical errors: Choose test with most basic arguments (single values, small numbers, edge cases) 4. First failing test: If multiple tests fail similarly, choose the first oneOutput Format Conventions• Logical errors: “When called as [function_call], the function returns [actual]; whereas the expected result is [expected].”• Runtime errors: “When called as [function_call], the function raises [ErrorType] on line [N].”• Syntax errors: “When called as [function_call], the function produces a SyntaxError on line [N].”Input FormatProblem Description: [problem_description]Buggy Code: [buggy_code]Execution Results: [execution_results]Figure 8: Prompt template for failed test case description generation. The full template includes detailed execution result formats and worked examples for each error type.  This automated approach ensures that each buggy code sample is paired with a clear, consistent description of its simplest failure, providing a focused starting point for reasoning trajectory generation. We task the LLM to select the simplest failing test and describe it in a concise manner. This reduces unnecessary complexity while debugging and code tracing in the reasoning trajectory. This ensures that the Socratic conversation focuses on the core misconception and avoids unnecessary complexity. The LLM used to generate the failed test case description is Claude Sonnet-4.5 with temperature 0.1, disabled reasoning, and max_tokens 4000.E	Language Model Evaluation PromptsTo ensure the quality of generated reasoning trajectories and Socratic conversations, we employ an LLM-asjudge evaluation approach. We use Claude Sonnet-4.5 with extended thinking enabled as the evaluator model, configured with temperature 1.0 and max_tokens 8000. The evaluation uses prompting with structured criteria that enable systematic assessment of both logical rigor and pedagogical appropriateness.  This automated evaluation approach enables scalable assessment across larger datasets, maintains consistency in applying complex evaluation criteria, and produces structured feedback that can guide LLMs through iterative refinement. The evaluation prompts are carefully designed to operationalize abstract quality requirements into concrete and verifiable criteria.E.1	Reasoning Trajectory EvaluationTo validate that generated reasoning trajectories serve as rigorous logical proofs by counterexample, we evaluate them across three hierarchical categories. Figure 9 shows a simplified evaluation prompt template since the full template is too long to fit in the page. The evaluation prompt is a zero-shot prompting approach which does not include any worked examples.Your TaskEvaluate whether a reasoning trajectory (RT) serves as a rigorous, logical proof by counterexample that contradicts a student misconception. An RT is VALID only if it passes all criteria in all three categories below.Category 1: Logical Soundness• Valid Starting Point: Begins with verifiable fact from failed test• Deductively Valid: Each step follows necessarily from prior steps and Python semantics. No abduction or logical leaps. Does not assume programming knowledge that directly contradicts the misconception.• Sound Contradiction: Establishes facts incompatible with misconception.• Complete Causal Chain: Unbroken chain from observation to contradiction• Execution Tracing: Traces program execution to deduce concrete factsCategory 2: Step Construction & Precision• Clear Boundaries: Each step is a distinct logical unit• Precision: Uses specific line numbers, variable names, values• Proper Citation: Non-adjacent dependencies explicitly cited• Technical Accuracy: All claims about Python constructs are correctCategory 3: Formatting & Focus• Sequential Labeling: All steps labeled sequentially (A.1, A.2, ...)• Focus on Misconception: Exclusively focused on disproving target misconceptionOutput Format{"valid": true/false,"categories": {"logical_soundness": true/false,"step_construction_and_precision": true/false,"formatting_and_focus": true/false},"comments": "[Evaluation rationale]",  "feedback": "[Actionable suggestions or NONE]" }Figure 9: Prompt template for LLM-as-judge evaluation of reasoning trajectories. An RT is valid only if all three categories pass. The full template includes detailed criterion descriptions and scoring instructions.  The evaluation framework assesses Logical Soundness through five criteria: valid starting point from the failed test, deductively valid inferences without abduction or logical leaps, sound contradiction of the target misconception, complete causal chain from observation to contradiction, and proper execution tracing to deduce concrete facts. Although reasoning steps may use general knowledge about Python, mathematics, or other domains to deduce facts about program execution, they must not assume programming knowledge that directly contradicts the misconception. For example, if the student believes range(n) starts at 1, the reasoning cannot assume it starts at 0, since the student holding the misconception would disagree with that assumption in a Socratic conversation. The Step Construction & Precision category evaluates clear step boundaries, precision in referencing code elements, proper citation of dependencies, and technical accuracy of all claims. Finally, Formatting & Focus ensures sequential step labeling and exclusive focus on contradicting the target misconception.  A reasoning trajectory is considered valid only if all criteria in all three categories pass. This hierarchical scoring mechanism ensures that RTs meet both the logical rigor required for sound proofs and the pedagogical clarity needed for effective student guidance. The judge outputs structured JSON with binary scores for each category, detailed comments explaining the evaluation rationale, and actionable feedback for invalid RTs.E.2	Socratic Turn EvaluationTo ensure that generated Socratic conversations effectively guide students through the reasoning trajectory, we evaluate each teacher utterance using a two-criterion framework. Figure 10 shows a simplified evaluation prompt template. The full template includes 5 fully worked examples.Your TaskEvaluate whether a Teacher utterance in a Socratic conversation effectively guides a student to articulate the inference from a specific RT step. A teacher utterance is VALID only if it satisfies both criteria below.Criterion 1: Prompts the Correct InferenceThe teacher’s question must guide the student to articulate the key inference from the specific RT step it claims to prompt. The student’s response should contain the statement proven in that step, and only that step. Questions may state facts from previous steps but must prompt the new inference at the target step.Criterion 2: Does Not State the Inference DirectlyThe teacher must ask a question requiring reasoning. The teacher should not provide the answer or state the conclusion. Questions can be general (“What’s the issue?”) or specific, as long as they require the student to think and derive the answer rather than merely confirm a stated fact.Evaluation Process1. Read RT step A.X to understand the target inference2. Read RT steps A.1 through A.X-1 for established facts3. Read the teacher utterance and student response4. Evaluate against both criteria5. Valid only if both criteria passOutput Format{"valid": true/false,"criteria_scores": {"prompts_correct_inference": true/false,"does_not_state_inference": true/false},"comments": "[Evaluation explanation]",  "feedback": "[Suggestions or NONE]" }Figure 10: Prompt template for LLM-as-judge evaluation of Socratic teacher utterances. A teacher utterance is valid only if it prompts the correct RT step without stating the inference directly. The full template includes worked examples demonstrating both valid and invalid utterances.  The first criterion, Prompts the Correct Inference, ensures that the teacher’s question guides the student to articulate the specific inference from the target RT step. The student’s response should contain the statement proven in that step, and only that step. Questions may reference facts established in previous steps but must prompt the new inference at the target step. The second criterion, Does Not State the Inference Directly, ensures the teacher asks a genuine question requiring reasoning rather than stating the conclusion and requesting confirmation.  A teacher utterance is considered valid only if it satisfies both criteria. The evaluation process follows a systematic procedure: read the target RT step to understand the required inference, review previous steps for context, examine the teacher utterance and student response, and evaluate against both criteria.The judge outputs structured JSON with binary scores for each criterion, explanatory comments, and actionable feedback for invalid utterances.F	Reasoning Trajectory Generation Qualitative AnalysisTo complement our quantitative evaluation, we conduct a qualitative analysis of reasoning trajectory generation. Using the RT evaluation criteria established in Appendix E.1, we perform manual evaluation on 30 reasoning trajectories created from 10 samples from each of three models. This section presents both successful and unsuccessful patterns observed.F.1	Success PatternsWe identified three key patterns that characterize successful reasoning trajectory generation from the manually evaluated samples, demonstrating rigorous logical proof by counterexample.? Exhausting Alternative Possibilities. Effective reasoning trajectories enumerate all possible scenarios and systematically eliminate scenarios that contradict observed behavior. Consider an example where a student has written:def calculate_average(x, y): return x + y / 2  The student incorrectly believes that + has higher precedence than /. When called as calculate_average(1, 3), the function returns 2.5 instead of the expected 2.0. The reasoning trajectory establishes:  Step A.1: The failed test states that calculate_average(1, 3) returns 2.5. So with x = 1 and y = 3, the expression on line 2, x + y / 2, evaluates to 2.5.  Step A.2: There are no parentheses in line 2, so the only two possible groupings of x + y / 2 are: (1) (x + y) / 2, or (2) x + (y / 2).Step A.3: Compute (x + y) / 2 with x = 1 and y = 3: (1 + 3) / 2 = 4 / 2 = 2.0.  Step A.4: If + had higher precedence than /, then line 2 would be evaluated as (x + y) / 2, which we computed to be 2.0 (A.3). But the actual result is 2.5 (A.1). Therefore, + is not evaluated before / in this expression.This approach demonstrates systematic elimination: the RT enumerates all possible interpretations(Step A.2), computes the result under each interpretation (Step A.3), and rules out the interpretation matching the misconception by contrasting it with observed behavior (Step A.4). The exhaustive enumeration ensures no logical gaps remain.? Concrete Execution Tracing. Successful reasoning trajectories ground abstract reasoning in concrete test values, tracing program execution with specific inputs throughout the logical chain. In the same example above, the RT uses the specific values x = 1 and y = 3 from the failed test consistently across all steps. Step A.1 establishes these concrete values from the failed test. Step A.3 computes the concrete result (1 + 3) / 2 = 4 / 2 = 2.0 for the first grouping. Step A.5 (not shown above) verifies the alternative grouping 1 + (3/2) = 2.5 matches the observed output. This concrete tracing ensures every deductive step is verifiable against observable program behavior rather than relying on abstract reasoning about Python semantics. The specificity eliminates ambiguity and makes each logical inference checkable. ? Clear Contradiction. Effective reasoning trajectories explicitly structure the contradiction between the misconception and observed behavior. Step A.4 in the example above demonstrates this structure: it first states the implication of the misconception (“If + had higher precedence than /, then line 2 would be evaluated as (x + y) / 2"), then computes the result under that assumption (“which we computed to be 2.0"), and finally contrasts this with actual observed behavior (“But the actual result is 2.5"). The explicit “If...then...But" structure makes the logical contradiction transparent. This pattern ensures the reasoning trajectory achieves its primary purpose: proving the misconception leads to predictions incompatible with observed program behavior.F.2	Failure PatternsSeveral failure modes emerged in manual evaluation, each revealing different challenges in constructing logically sound reasoning trajectories.? Using Knowledge that Contradicts the Misconception. The most pedagogically damaging failure pattern occurs when reasoning trajectories rely on programming knowledge that directly contradicts the target misconception. Consider an example where a student has written:def top_k(lst, k): result = [] for i in range(k):result.append(max(lst)) lst.pop(max(lst)) # Line 5return result  The student incorrectly believes that the .pop() method takes a value to be deleted from the list. When called as top_k([1, 2, 3, 4, 5], 1), the function raises an IndexError on line 5. The reasoning trajectory contains:  Step A.5: In Python, list.pop(i) removes and returns the item at index i; if i is outside the valid index range for the list, it raises IndexError. Therefore, the observed IndexError on lst.pop(5) means the argument 5 was used as an index, not as a value.  This step explicitly states how list.pop() actually works, directly contradicting the student’s belief. A student holding the misconception would reject this premise in a Socratic conversation. The RT evaluation criterion states that reasoning steps must not assume programming knowledge that directly contradicts the misconception. The correct approach would prove that interpreting the argument as a value leads to a contradiction, without stating how pop() actually works.? Abductive Reasoning and Logical Leaps. Reasoning trajectories sometimes employ abductive reasoning (inference to the best plausible explanation) rather than deductive proof, leaving logical gaps. Consider an example where a student has written:def count_words(sentence):words = 0 space_mode = Truefor i in range(1, len(sentence)): # Line 4 if sentence[i] == ' ': if not space_mode: words += 1space_mode = Trueelse:space_mode = Falseif not space_mode:words += 1return words  The student incorrectly believes string indexing starts at 1. When called as count_words("I love Python"), the function returns 2 instead of 3. After establishing that the loop executes from i = 1 to i = 12 and that words is incremented exactly once during the loop, the reasoning trajectory contains:  Step A.9: At the start of the loop when i = 1, space_mode is True (initialized on line 3). If sentence[1] is a space, line 6’s condition not space_mode would be False, so line 7 would not execute.For the algorithm to eventually count only 2 words while “I love Python" has 3 words, and for line 7 to execute exactly once, sentence[1] must be a space.  This reasoning works backward from the observed output to infer that sentence[1] must be a space. However, it does not prove this is the only possibility that produces words = 2, leaving the logical chain incomplete. The RT must explain: (1) what happens if sentence[1] is NOT a space (e.g., if it’s ’I’)? and (2) why would that scenario fail to produce the observed output of ‘words‘ = 2?  Another example demonstrates this pattern more concisely. In the top_k example above, a different reasoning trajectory contains:  Step A.7: Since calling lst.pop(5) raises an IndexError, and the number 5 is an invalid index for lst, the .pop() method must be interpreting the argument 5 as an index, not as a value.  The phrase “must be interpreting" reveals abduction. The step infers the most likely explanation but does not prove it deductively. It is theoretically possible for pop() to use the argument in a different way while still raising IndexError. A valid approach would prove that if 5 were interpreted as a value to remove, no error would occur (since 5 exists in the list), establishing contradiction.? TechnicalInaccuracy. Reasoning trajectories sometimes contain technically incorrect claims about programming constructs, undermining their logical validity. Consider an example where a student has written:def is_palindrome(string):rev_string = '' for i in string:rev_string = i + rev_stringif rev_string = string: # Line 5 return Trueelse: return False  The student incorrectly believes the = operator is used for equality comparison. When called as is_palindrome(“racecar"), the function produces a SyntaxError on line 5. The reasoning trajectory contains:  Step A.3: In Python, an if statement requires a conditional expression following the if keyword. A conditional expression is something that can be evaluated to determine if it is true or false.  This statement uses the term “conditional expression" incorrectly. According to Python documentation2, a conditional expression refers specifically to the ternary operator (xifCelsey), which is an expression that evaluates to a value. The RT appears to mean “boolean expression" or “condition," but the misuse of technical terminology contradicts authoritative documentation. Although the RT later defines the term differently (“something that can be evaluated to determine if it is true or false"), this redefinition itself violates technical accuracy. Such inaccuracies undermine the RT’s credibility and may confuse students learning precise programming terminology.? Incomplete Logical Chains. Reasoning trajectories sometimes skip necessary intermediate steps, leaving gaps in the causal chain from observation to contradiction. In the same is_palindrome example above, the reasoning trajectory establishes:  Step A.4: The statement on line 5 uses the single equals sign (=). In Python, the = operator is the assignment operator. Its function is to assign the value on its right to the variable on its left.  Step A.5: Because the = operator is for assignment and not for evaluation, the expression rev_string = string is an assignment statement, not a conditional expression.  The logical leap occurs between these steps. Step A.4 establishes that = is used for assignment. Step A.5 concludes that = cannot be used for comparison. However, the RT does not prove that an operator cannot serve both purposes. A student might reasonably ask: “Why can’t = be used for both assignment and comparison in different contexts?" The missing step must establish that Python operators are unambiguous and cannot serve dual purposes in the same statement context. Without this intermediate link, the causal chain from “= is for assignment" to “= is not for comparison" remains incomplete.G	Socratic Utterance Generation Qualitative AnalysisTo complement our quantitative evaluation, we conduct qualitative analysis of Socratic utterance generation. We manually evaluated 88 teacher utterances evaluated by both an LLM judge and a human expert created from 15 reasoning trajectories, 5 from each of three models: Claude Sonnet-4.5 with reasoning, Gemini-2.5-pro with reasoning, and GPT-5 with medium reasoning effort. This section presents both successful and unsuccessful patterns observed, while evaluating against the Socratic turn evaluation criteria established in Appendix E.2.G.1	Success PatternsWe identified four key patterns that characterize successful Socratic conversation generation across the manually evaluated samples.? Accurate RT Step Alignment. Across all manually evaluated teacher utterances, we did not observe any cases where a teacher question prompted content from a completely different RT step than requested. For instance, we found no instances where a question marked for prompting Step A.5 actually prompted for Step A.7’s conclusion. This indicates alignment between generated questions and their intended reasoning steps, demonstrating that LLMs reliably understand the step-by-step structure of reasoning trajectories and can write Socratic utterances specific to a logical inference.? Step Reference Integration. Effective Socratic questions occasionally need to explicitly incorporate facts established in prior RT steps while prompting a new inference. LLMs successfully synthesize these references into a single, coherent question. Consider the following example where the student has written buggy code that incorrectly assumes str.replace() modifies strings in place:def toxNGLXSH(sen): vowels = ["a"] for i in vowels:if i.islower():sen.replace(i, "x") # Line 5else:sen.replace(i, "X")return senThe reasoning trajectory establishes:• Step A.2: The loop executes exactly once with i = ‘a’• StepA.3: Since i=‘a’ (A.2) and ‘a’.islower() evaluates to True, the condition on line 4 is satisfied, so line 5 is executed  The teacher question targeting Step A.3 is: “Given that i = ‘a’, which branch of the if-else statement executes?”  This question effectively integrates the fact from Step A.2 (i = ‘a’) as a given, then prompts the student to determine which branch executes. The question requires the student to evaluate the condition i.islower() and conclude that line 5 executes, which is precisely the inference in Step A.3. The prior step’s conclusion becomes established context for the new question.? Implicit Prompting. Socratic questions prime students to provide information beyond what is explicitly requested, demonstrating understanding of what constitutes a complete logical step. Consider another example with the same buggy code. The reasoning trajectory step is:  Step A.4: On line 5, the expression sen.replace(i, “x") is evaluated. Given that sen = ‘a’ and i = ‘a’ (A.2), this evaluates to ‘a’.replace(‘a’, ‘x’).The teacher question is simply: “What expression is evaluated on line 5?”  The student responds: “On line 5, the expression sen.replace(i, “x") is evaluated. Given that sen = ‘a’ and i = ‘a’, this evaluates to ‘a’.replace(‘a’, ‘x’).”  Although the question asks only “what expression is evaluated,” it implicitly expects the student to provide both the abstract expression (sen.replace(i, “x")) and its concrete evaluation with values substituted (‘a’.replace(‘a’, ‘x’)). The student correctly provides this complete inference without requiring explicit prompting for each component.? Generic Opening Questions. All of the conversations in our manual evaluation begin with broad, open-ended questions such as “What issue are you encountering?” or “What seems to be the problem?” These effectively elicit students’ initial observations about failed tests without leading them. Such opening moves are straightforward and natural components of Socratic dialogue, and all models generate them successfully.G.2	Failure PatternWe identify a primary failure mode in generated Socratic utterances, representing a failure in asking a Socratic question with good pedagogical quality.? Stating the Conclusion Directly. Teacher utterances sometimes reveal the inference students should derive, reducing the question to mere confirmation rather than genuine reasoning. Consider an examplewhere the student has written:def calculate_average(x, y): return x + y / 2  The student incorrectly believes that + has higher precedence than /.	The reasoning trajectory establishes:  Step A.1: When called as calculate_average(1, 3), the function returns 2.5, meaning x = 1, y = 3, and x + y / 2 evaluates to 2.5....Step A.4: Let’s assume the misconception is true: + has higher precedence than /.Step A.5: Applying this assumption, 1 + 3 would be evaluated first, resulting in 4.Step A.6: After evaluating the addition, the expression would simplify to 4 / 2, which equals 2.0.  Step A.7: Therefore, the assumption that + has higher precedence than / leads to the conclusion that the expression 1 + 3 / 2 evaluates to 2.0 (Step A.6).  The generated teacher question for Step A.7 is: “So, your assumption that addition comes first leads to a final result of 2.0. How does that compare to what the program actually calculated?”  The first sentence explicitly states the conclusion from Step A.7 before asking the question. The student only needs to compare the values, not derive that the assumption leads to 2.0. A correct alternative would omit the statement of the conclusion and ask only: “How does that compare to what the program actually calculated?”H	LLM-as-Judge Qualitative AnalysisTo complement the quantitative agreement rates reported in the main paper, we conduct qualitative analysis of LLM-as-judge evaluation patterns. We analyze disagreement cases between the LLM judge and human expert across both reasoning trajectory validation and Socratic turn validation to identify failure modes and better understand the judge’s capabilities and limitations.H.1	LLM-as-Judge Failures for Reasoning Trajectory EvaluationRT validation achieved 76.66% agreement between the LLM judge and human expert, notably lower than Socratic turn validation (96.59%), suggesting more nuanced challenges in evaluating the logical rigor of reasoning trajectories. We analyze disagreement cases to identify patterns where the judge fails. ? Fails to Detect Knowledge Contradicting Misconceptions. The judge incorrectly marked reasoning trajectories as valid when they explicitly assume programming knowledge that contradicts the target misconception. In the top_k example from the failure patterns analysis, the reasoning trajectory contains:  Step A.5: In Python, list.pop(i) removes and returns the item at index i; if i is outside the valid index range for the list, it raises IndexError. Therefore, the observed IndexError on lst.pop(5) means the argument 5 was used as an index, not as a value.  The human expert marked this RT as invalid because Step A.5 explicitly states how list.pop() works, directly contradicting the misconception that pop() takes a value to delete. However, the judge marked the RT as valid. The judge appears to miss that this assumption violates the RT evaluation criterion: “Does not assume programming knowledge that directly contradicts the misconception." This represents a critical oversight since this failure pattern is the most pedagogically damaging, rendering the RT unusable in Socratic dialogue where students holding the misconception would reject the contradicting premise.  Another example, in the count_words example from the failure patterns analysis, the reasoning trajectory contains:  Step A.9: At the start of the loop when i = 1, space_mode is True (initialized on line 3). If sentence[1] is a space, line 6’s condition not space_mode would be False, so line 7 would not execute.For the algorithm to eventually count only 2 words while “I love Python" has 3 words, and for line 7 to execute exactly once, sentence[1] must be a space.  The human expert and the LLM marked this RT as invalid because Step A.9 uses abduction rather than proving deductively that sentence[1] is a space. However, the judge’s feedback suggested fixing the RT by “directly observing that in the input string ’I love Python’, the character at index 1 is verifiably a space." This suggestion itself violates RT evaluation rules by assuming that indexing starts at 0 (since it claims index 1 contains a space, which is the second character). The judge was suggesting assuming knowledge that contradicts the misconception to address the abduction issue of this step.? Fails to Catch Technical Inaccuracies. The judge does not verify technical claims about programming constructs against authoritative documentation. In the is_palindrome example from the failure patterns analysis, the reasoning trajectory contains:  Step A.3: In Python, an if statement requires a conditional expression following the if keyword. A conditional expression is something that can be evaluated to determine if it is true or false.  The human expert marked this RT as invalid because “conditional expression" has a specific technical meaning in Python (referring to the ternary operator x if C else y) that contradicts how the RT uses the term. The judge marked the RT as valid, missing this technical inaccuracy. The judge appears to lack capability to fact-check domain-specific terminology against Python documentation, accepting the RT’s redefinition of the term as sufficient. This suggests the judge evaluates logical structure without verifying technical accuracy of claims about programming language semantics.? Over-Accepts Incomplete Logical Chains. The judge sometimes accepts reasoning trajectories with missing intermediate steps that leave gaps in the causal chain. In the same is_palindrome example, the reasoning trajectory establishes that = is used for assignment (Step A.4) and then concludes that = cannot be used for comparison (Step A.5), without proving that Python operators cannot serve dual purposes. The human expert identified this as an incomplete logical chain, but the judge marked the RT as valid.The judge appears too lenient on what constitutes a “complete causal chain," accepting direct leaps from A to C without requiring explicit proof of the intermediate link B. This suggests the judge may assess whether conclusions follow plausibly rather than whether they follow necessarily from stated premises.  Despite these systematic failures, the judge correctly identifies valid reasoning trajectories that employ sound deductive reasoning, proper execution tracing, and clear contradiction structure. The challenge lies in detecting subtle logical flaws (abduction, missing steps, contradictory assumptions, technical errors) rather than recognizing rigorous proofs. The overall agreement rate with a human expert is high, scoring76.66%, highlighting the reliability of the LLM-as-judge for this task.H.2	LLM-as-Judge Failures for Socratic Turn EvaluationWe conducted manual evaluation of 88 teacher utterances, comparing the LLM judge (Claude Sonnet-4.5 with extended thinking) against human expert judgments. The overall agreement rate was 96.59%. Erroranalysis on the disagreement samples reveals some patterns.H.2.1	False Negative: Judge Too StrictIn one case, the judge incorrectly marked a valid teacher utterance as invalid, revealing a systematic issue with how the judge handles conversational framing.? Conversational Framing Penalized. The sample involves a student who has written:class Node:def __init__(self, data=None, next=None):       self.data = data self.next = next def insert_after(head, prev_data, new_data): curr_head = head while curr_head:if curr_head.data == prev_data: # Line 9 new_node = Node(data = new_data) curr_head.next = new_node # Line 11return new_nodeelse: curr_head = curr_head.next  The student incorrectly believes that assigning a new node Y to the next field of a node X in a linked list will automatically ensure that Y will be followed by the rest of the list that originally came after X.The reasoning trajectory establishes:  Step A.2: From Test Case 1, before any modifications, the linked list is constructed as: head (data=1) ? node (data=2) ? node (data=3) ? node (data=4). Therefore, initially head.next.next.next should refer to the node with data=4.  Step A.3: Test Case 2 calls insert_after(head, 2, 5), which is intended to insert a new node with data=5 after the node with data=2.  Step A.4: Test Case 5 confirms that after the insertion, head.next.next.data == 5, meaning the new node with data=5 is successfully positioned at head.next.next.  Step A.5: Since the node with data=2 is at head.next (Test Case 4), and the new node with data=5 is now at head.next.next (A.4), we can confirm that in the code execution, when curr_head points to the node with data=2 on line 9, the new node is created and assigned.  The generated teacher question for Step A.5 is: “Let’s trace through the code execution. When the condition on line 9 becomes true, which node does curr_head point to?”  The LLM judge marked this as invalid, while the human expert marked it as valid. The question effectively prompts the key inference from Step A.5 (identifying which node curr_head points to when the condition becomes true). The judge appears to have penalized the conversational preamble “Let’s trace through the code execution” as not directly prompting the step. However, the utterance still elicits the correct inference.H.2.2	False Positives: Judge Too LenientThe judge also incorrectly marked invalid teacher utterances as valid, revealing patterns where the judge aligned too closely with RT structure at the expense of not stating the inference directly in the utterance. ? Misattributing the Utterance to the Wrong Step. This case also involves the same turn_clockwise(compass_point) buggy code. The reasoning trajectory establishes:  Step A.5: If compass_point = “N" were a valid comparison expression (A.4), then placing it in an if statement, as in ifcompass_point=“N":, would be grammatically correct Python code. The interpreterwould be able to understand and execute it without raising a SyntaxError.  Step A.6: However, the interpreter does raise a SyntaxError for the code on line 2 (A.1, A.2). This directly contradicts the expectation from Step A.5.  The generated teacher question for Step A.6 is: “But what did the interpreter actually do when it saw that line?”  The LLM judge marked this as invalid, claiming it prompts the wrong RT step, while the human expert marked it as valid. The question appropriately contrasts the student’s expectation (from A.5: if = were valid, the code would parse) with actual interpreter behavior (A.6: interpreter raises SyntaxError). The judge appears confused by questions that reference expectations from prior steps, misidentifying this as prompting the wrong step.H.2.3	Judge StrengthsThe judge’s 96.59% overall agreement rate demonstrates strong alignment with human judgment. The judge is particularly reliable on clear-cut cases, including generic opening questions, direct requests for values or expressions, and questions with obvious logical errors. The judge also reliably catches rhetorical questions ending with “right?” that seek confirmation rather than reasoning.  Additionally, the judge appropriately handles implicit information, generally accepting questions where some information is implied. For instance, it recognizes that asking “What does this expression evaluate to?” can expect both the expression and its result, acknowledging that complete pedagogical steps may require multi-part answers.I	Web InterfaceTo support practical use of our approach, we developed an interactive web application using Streamlit3that implements an end-to-end pipeline: from a problem specification and buggy student code to a complete Socratic debugging conversation. The interface provides educators and researchers with immediate access to our methodology without requiring technical expertise or substantial setup.Figure 11: Interactive web interface for generating reasoning trajectories and Socratic conversations from student code. The interface accepts problem descriptions, buggy implementations, and misconception descriptions as input, and produces structured Socratic intervention.Figure 12: Interactive web interface generates a reasoning trajectory concluding with a statement that contradicts the misconception of addition having a higher precedence than division.Figure 13: The tool generates a complete Socratic conversation between a student and a teacher based on the generated reasoning trajectory.  The workflow begins when users input a problem description along with student code that fails one or more test cases, a description of the failed test case, and a misconception description. The application generates a reasoning trajectory that maps a path from the student’s flawed mental model to a contradictory statement about program behavior. This trajectory subsequently guides the generation of a Socratic conversation demonstrating how an instructor might guide the student toward self-discovery of their error.  Users can select from multiple state-of-the-art LLMs (Claude, GPT, Gemini), with the interface automatically applying the same prompt templates and model configurations used in our experimental evaluation. Reasoning capabilities are enabled by default for all compatible models. The interface presents results in a structured format featuring: the complete reasoning trajectory with intermediate steps, an example Socratic conversation, and expandable reasoning traces showing the model’s internal deliberation process.  For classroom applications, instructors can use this tool during office hours or help sessions to rapidly prepare targeted interventions for individual students. When a student presents buggy code, the instructor can input it into the system and within seconds receive a principled Socratic questioning strategy tailored to a specific misconception. This enables more effective one-on-one debugging sessions by providing the instructor with a structured pedagogical plan rather than ad-hoc questioning. The application runs locally and securely loads API credentials from environment variables.1 Complete versions can be seen in the GitHub repository.2 https://docs.python.org/3/reference/expressions.html#conditional-expressions3 https://streamlit.io------------------------------------------------------------------------------------------------------------------------------------------------------111